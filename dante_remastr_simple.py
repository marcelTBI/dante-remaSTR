from __future__ import annotations

from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter, ArgumentTypeError
from datetime import datetime
from typing import Iterator, TypeAlias, Any
from collections import Counter
# from pprint import pprint

import os
import re
import json
import textwrap
import enum
import math
import functools
import itertools

import numpy as np
import pandas as pd

from scipy.stats import binom  # type: ignore

VERSION = "0.9.0"
DANTE_DESCRIPTION = '''
            DANTE = Da Amazing NucleoTide Exposer (Remastered)
            --------------------------------------------------
Genotyping and reporting from annotated reads generated by the remaSTR program.
The reads are filtered, clustered, and a genotype is inferred for each motif.
If a verbose option is switched on, DANTE creates a directory for each of
the annotated motifs and a summary report in HTML (report.html). Otherwise only
a table with all genotypes, confidences, and supporting information.
'''
MOTIF_COLUMN_NAME = 'motif'
MIN_FLANK_LEN = 3
MIN_REP_LEN = 3
MIN_REP_CNT = 1
MAX_ABS_ERROR = None
MAX_REL_ERROR = 1.0
MAX_REPETITIONS = 40

BASE_MAPPING = {
    'A': 'A', 'C': 'C', 'G': 'G', 'T': 'T',
    'M': '[AC]', 'R': '[AG]', 'W': '[AT]', 'S': '[CG]', 'Y': '[CT]', 'K': '[GT]',
    'V': '[ACG]', 'H': '[ACT]', 'D': '[AGT]', 'B': '[CGT]',
    'N': '[ACGT]'
}


def main() -> None:
    start_time = datetime.now()
    args = load_arguments()

    print(f'DANTE_remaSTR Starting : {start_time:%Y-%m-%d %H:%M:%S}')
    sample = os.path.basename(os.path.normpath(args.output_dir))

    data_json = analyse_motif(args.input_tsv, args.male, sample)

    os.makedirs(args.output_dir, exist_ok=True)
    json_example = json.dumps(data_json, indent=2, sort_keys=True)
    with open(f"{args.output_dir}/data.json", "w") as f:
        f.write(json_example)

    end_time = datetime.now()
    print(f'DANTE_remaSTR Stopping : {end_time:%Y-%m-%d %H:%M:%S}')
    print(f'Total time of run      : {end_time - start_time}')


def analyse_motif(input_tsv: str, male: bool, sample: str):
    main_json: dict[str, Any] = {}
    main_json["sample"] = sample
    main_json["dante_version"] = VERSION

    pf = PostFilter()
    pf_error = f'{pf.max_rel_error * 100:.0f}%'
    if pf.max_abs_error is not None:
        pf_error += f' (abs={pf.max_abs_error})'
    main_json["postfilter_params"] = (pf.min_rep_len, pf.min_rep_cnt, pf_error)

    main_json["motifs"] = generate_motifs(input_tsv, male, pf)
    return main_json


def generate_motifs(input_tsv: str, male: bool, pf: PostFilter) -> list:
    motif_json: dict[str, Any] = {}

    motif_table = pd.read_csv(input_tsv, sep='\t')

    motif_str = motif_table[MOTIF_COLUMN_NAME].iloc[0]
    motif_name = motif_table['name'].iloc[0]
    motif = Motif(motif_str, motif_name, male)

    motif_json["motif_id"] = motif.name
    motif_json["motif_stats"] = {
        "chrom": motif.chrom,
        "start": motif.start,
        "end": motif.end,
        "modules": motif.modules
    }

    annotations: list[Annotation] = []
    for _, mt_row in motif_table.iterrows():
        annotations.append(Annotation(
            mt_row['read_id'], mt_row['mate_order'], mt_row['read'], mt_row['reference'],
            mt_row['modules'], mt_row['log_likelihood'], motif
        ))

    genotypes = []

    read_distribution = np.bincount([len(ann.read_seq) for ann in annotations], minlength=100)
    for __3__curr_module_num, _, _ in motif.get_repeating_modules():

        __3__anns_spanning, __3__rest = pf.get_filtered(motif, annotations, __3__curr_module_num, both_primers=True)
        __3__anns_flanking, __3__anns_filtered = pf.get_filtered(motif, __3__rest, __3__curr_module_num, both_primers=False)
        del __3__rest

        __3__model = Inference(read_distribution, None)
        __3__lh_array, __3__predicted, __3__confidence = __3__model.genotype(__3__anns_spanning, __3__anns_flanking, __3__curr_module_num, motif.monoallelic)
        genotypes.append((
            __3__curr_module_num,
            __3__anns_spanning,
            __3__anns_flanking,
            __3__anns_filtered,
            __3__predicted,
            __3__confidence,
            __3__lh_array,
            __3__model
        ))

    __9__seq = motif.modules_str(include_flanks=True)
    __9__m_list1 = []
    for __9__gt in genotypes:
        __9__locus_data = generate_locus_data(__9__gt, motif, 5, motif.name, pf, __9__seq)
        __9__m_list1.append(__9__locus_data)

    haplotypes: list[None | tuple] = [None]

    __5__repeating_modules = motif.get_repeating_modules()
    for __5__i, (__5__curr_module_num, _, _) in enumerate(__5__repeating_modules[1:], start=1):
        __5__prev_module_num = __5__repeating_modules[__5__i - 1][0]
        __5__mod_nums = [__5__prev_module_num, __5__curr_module_num]

        __5__anns_2good, __5___filtered = pf.get_filtered_list(motif, annotations, __5__mod_nums, both_primers=[True, True])
        __5___left_good, __5___left_bad = pf.get_filtered_list(motif, __5___filtered, __5__mod_nums, both_primers=[False, True])
        __5___right_good, __5__anns_0good = pf.get_filtered_list(motif, __5___left_bad, __5__mod_nums, both_primers=[True, False])
        __5__anns_1good = __5___left_good + __5___right_good
        del __5___filtered, __5___left_good, __5___left_bad, __5___right_good

        __5__phasing, __5__supp_reads = phase(__5__anns_2good, __5__prev_module_num, __5__curr_module_num)

        haplotypes.append(
            (__5__curr_module_num, __5__anns_2good, __5__anns_1good, __5__anns_0good, __5__phasing, __5__supp_reads, __5__prev_module_num)
        )

    # construct motif_nomenclatures
    rep_mods = motif.get_repeating_modules()
    for module_number, _, _ in rep_mods:
        annotations, _ = pf.get_filtered(motif, annotations, module_number, both_primers=True)

    __9__m_list2 = []
    for __9__ph in haplotypes[1:]:
        __9__locus_data2 = generate_locus_data2(__9__ph, motif, __9__seq, pf, motif.name, 5)
        __9__m_list2.append(__9__locus_data2)

    motif_json["phased_seqs"] = get_phased_sequence(motif, genotypes, haplotypes)

    motif_nomenclatures = []
    for (count, location, part) in generate_nomenclatures(annotations, None, None, motif, 5):
        motif_nomenclature: dict[str, Any] = {}
        motif_nomenclature["count"] = count
        motif_nomenclature["location"] = location
        motif_nomenclature["noms"] = part
        motif_nomenclatures.append(motif_nomenclature)
    motif_json["nomenclatures"] = motif_nomenclatures

    __1__modules = []
    for __1__old_module in __9__m_list1:
        __1__module = {}
        __1__module["module_id"] = __1__old_module[0]
        __1__module["id"] = __1__old_module[1]
        __1__module["sequence"] = __1__old_module[2]

        module_nomenclatures = []
        for __1__old_nomenclature in __1__old_module[3]:
            __1__nomenclature = {}
            __1__nomenclature["count"] = __1__old_nomenclature[0]
            __1__nomenclature["location"] = __1__old_nomenclature[1]
            __1__nomenclature["noms"] = __1__old_nomenclature[2]
            module_nomenclatures.append(__1__nomenclature)
        __1__module["nomenclatures"] = module_nomenclatures

        __1__module["allele_1"] = __1__old_module[4]
        __1__module["allele_2"] = __1__old_module[5]
        __1__module["stats"] = __1__old_module[6]
        __1__module["raw_conf"] = __1__old_module[7]
        __1__module["reads_spanning"] = __1__old_module[8]
        __1__module["reads_flanking"] = __1__old_module[9]
        __1__module["graph_data"] = __1__old_module[10]
        __1__modules.append(__1__module)

    motif_json["modules"] = __1__modules

    __1__modules = []
    for __1__old_phasing in __9__m_list2:
        __1__module = {}
        __1__module["module_id"] = __1__old_phasing[0]
        __1__module["ids"] = __1__old_phasing[1]
        __1__module["sequence"] = __1__old_phasing[2]

        module_nomenclatures = []
        for __1__old_nomenclature in __1__old_phasing[3]:
            __1__nomenclature = {}
            __1__nomenclature["count"] = __1__old_nomenclature[0]
            __1__nomenclature["location"] = __1__old_nomenclature[1]
            __1__nomenclature["noms"] = __1__old_nomenclature[2]
            module_nomenclatures.append(__1__nomenclature)
        __1__module["nomenclatures"] = module_nomenclatures

        __1__module["allele_1"] = __1__old_phasing[4]
        __1__module["allele_2"] = __1__old_phasing[5]
        __1__module["stats"] = __1__old_phasing[6]
        __1__module["raw_conf"] = __1__old_phasing[7]
        __1__module["reads_spanning"] = __1__old_phasing[8]
        __1__module["reads_flanking"] = __1__old_phasing[9]
        __1__module["graph_data"] = __1__old_phasing[10]
        __1__modules.append(__1__module)

    motif_json["phasings"] = __1__modules
    return [motif_json]


def get_phased_sequence(motif: Motif, genotype: list[GenotypeInfo], phasing: list[None | tuple]) -> dict:
    h1 = []
    h2 = []
    nomenclatures = []
    for gt in genotype:
        # (mod_num, spanning, flanking, filtered, predicted, conf, lh_array, model)
        (mod_num, spanning, _, _, predicted, _, _, _) = gt
        h1.append(str(predicted[0]))
        h2.append(str(predicted[1]))

        nomenclature = list(map(nom_count_to_triple, generate_nomenclatures(spanning, mod_num, None, motif)))
        nomenclatures.append(nomenclature)

    hp1 = []
    hp2 = []
    for ph in phasing:
        if ph is None:
            continue

        # (mod_num, 2good, 1good, 0good, phase, supp_reads, prev_mod_num)
        (_, _, _, _, phase, _, _) = ph
        hp1.append(phase[0])
        hp2.append(phase[1])

    h1_full, h2_full, err1, err2 = phase_full_locus(h1, h2, hp1, hp2)
    aug_nom1, nom1_len, nomenclatures, errs1 = augment_nomenclature(motif, h1_full, nomenclatures, 0.8)
    aug_nom2, nom2_len, nomenclatures, errs2 = augment_nomenclature(motif, h2_full, nomenclatures, 0.8)
    result = {
        "motif_name": motif.name,
        "nomenclature1": aug_nom1,
        "nomenclature1_len": nom1_len,
        "errors1": err1 + errs1,
        "nomenclature2": aug_nom2,
        "nomenclature2_len": nom2_len,
        "errors2": err2 + errs2
    }
    return result


def augment_nomenclature(
    motif: Motif, hapl: list[str], nomenclatures: list[list[tuple[int, int, str]]], assignment_factor: float
) -> tuple[list[str], int, list[list[tuple[int, int, str]]], list[str]]:

    errors = set()
    result = []
    for i, x in enumerate(hapl):
        try:
            count = int(x)
        except ValueError:
            errors.add("contains B/E/X")
            result.append(f"err[{x}]")
            continue

        assigned = False
        for j, nom in enumerate(nomenclatures[i]):
            length, c, representation = nom
            if length == count:
                result.append(representation)
                c = int(c * (1.0 - assignment_factor))
                nomenclatures[i][j] = (length, c, representation)
                assigned = True
                nomenclatures[i] = sorted(nomenclatures[i], key=lambda x: -x[1])
                break

        if not assigned:
            errors.add("nomenclature mismatch")
            result.append(f"err[{count}]")

    aug_nom = motif.augmented_nomenclature(result)
    nom_len = sum((hgvs_to_len(aug_nom[i]) for i in range(len(aug_nom))))
    return aug_nom, nom_len, nomenclatures, list(errors)


def hgvs_to_len(hgvs: str) -> int:
    "Used for sorting based on nomenclature length."
    length = 0
    for seq, num in re.findall(r'([A-Z]+)\[([0-9BE]+)\]', hgvs):
        if num == "B":
            length += -1000
        elif num == "E":
            length += 1000
        else:
            length += len(seq) * int(num)

    return length


def nom_count_to_triple(nomenclature: tuple[int, str, list[str]]) -> tuple[int, int, str]:
    count, _, repr_list = nomenclature
    assert len(repr_list) == 1, "This function converts only one representation"
    representation = repr_list[0]
    length = sum(int(num) for _, num in re.findall(r'([A-Z]+)\[(\d+)', representation))
    return length, count, representation


def phase_full_locus(
    h1_full: list[str], h2_full: list[str], hp1: list[str], hp2: list[str]
) -> tuple[list[str], list[str], list[str], list[str]]:
    # ['9', '15'] ['12', '16'] ['9|16'] ['12|15'] -> ['9', '16'] ['12', '15']

    n_diff: int = sum(map(lambda x: x[0] != x[1], zip(h1_full, h2_full)))
    if n_diff <= 1:
        return (h1_full, h2_full, [], [])  # there is nothing to phase

    errors1 = set()
    errors2 = set()
    # print()
    # print(h1_full, h2_full, hp1, hp2)
    different_prefix = False
    for i in range(len(h1_full) - 1):
        if h1_full[i] == h2_full[i]:
            if different_prefix:
                errors1.add("homozygous link")
                errors2.add("homozygous link")
                print(f"Warning: Cannot phase prefix and suffix at position {i}.\n{h1_full} {h2_full}\n{hp1} {hp2}")
            continue
        different_prefix = True

        h1_cur, _ = hp1[i].split("|")
        if h1_cur != h1_full[i]:
            hp1[i], hp2[i] = hp2[i], hp1[i]

        h1_cur, h1_next = hp1[i].split("|")
        _, h2_next = hp2[i].split("|")

        if h1_full[i] == h1_cur and h1_full[i + 1] == h1_next:
            # if h2_full[i] != h2_cur or h2_full[i + 1] != h2_next: WARN?
            pass
        elif h1_full[i] == h1_cur and h1_full[i + 1] == h2_next:
            # if h2_full[i] != h2_cur or h2_full[i + 1] != h1_next: WARN?
            h1_full[i + 1], h2_full[i + 1] = h2_full[i + 1], h1_full[i + 1]
        else:
            print(f"Warning: {h1_full[i:i + 2]} {h2_full[i:i + 2]} {hp1[i]} {hp2[i]} is inconsistent.")

    for i in range(len(h1_full) - 1):
        new_p1 = f"{h1_full[i]}|{h1_full[i + 1]}"
        new_p2 = f"{h2_full[i]}|{h2_full[i + 1]}"
        from_genotyping = sorted([new_p1, new_p2])
        from_phasing = sorted([hp1[i], hp2[i]])
        if from_genotyping != from_phasing:
            errors1.add("genotyping-phasing inconsistence")
            errors2.add("genotyping-phasing inconsistence")

    # print(f"-> {h1_full} {h2_full}\n")
    return (h1_full, h2_full, list(errors1), list(errors2))


def generate_locus_data(gt: GenotypeInfo, motif, nomenclature_limit, motif_id, post_filter, seq):
    graph_data: GraphData
    (module_number, anns_spanning, anns_flanking, anns_filtered, predicted, raw_confidence, lh_array, model) = gt

    locus_nomenclatures = generate_nomenclatures(anns_spanning, module_number, None, motif, nomenclature_limit)

    read_counts = None
    if len(anns_spanning) != 0 or len(anns_flanking) != 0:
        read_counts = write_histogram_image(anns_spanning, anns_flanking, module_number)
    else:
        print(f"Zero reads in {motif_id}")

    heatmap_data = None
    if lh_array is not None:
        heatmap_data = draw_pcolor(model, lh_array, motif.nomenclature)
    else:
        print(f"Likelihood array is None for {motif_id}.")

    row = generate_result_line(motif, predicted, raw_confidence, len(anns_spanning), len(anns_flanking), len(anns_filtered), module_number, qual_annot=anns_spanning)
    row_tuple = generate_row(seq, row, post_filter)

    tmp = generate_motifb64(seq, row)
    (locus_id, highlight, _, _, _, _, sequence) = tmp
    del row

    graph_data = (read_counts, heatmap_data, None)

    _, _, a1_prediction, a1_confidence, a1_reads, a1_indels, a1_mismatches, a2_prediction, a2_confidence, a2_reads, a2_indels, a2_mismatches, confidence, indels, mismatches, spanning_reads, flanking_reads = row_tuple
    locus_data = (
        locus_id, highlight, sequence, locus_nomenclatures,
        (a1_prediction, a1_confidence, a1_indels, a1_mismatches, a1_reads),
        (a2_prediction, a2_confidence, a2_indels, a2_mismatches, a2_reads),
        (confidence, indels, mismatches),
        raw_confidence,
        spanning_reads, flanking_reads,
        graph_data,
    )
    return locus_data


def generate_locus_data2(ph, motif, seq, post_filter, motif_id, nomenclature_limit):
    pass
    if ph is None:
        raise ValueError

    (module_number, anns_2good, anns_1good, anns_0good, phasing1, supp_reads, prev_module_num) = ph
    second_module_number = module_number
    suffix = f'{prev_module_num}_{second_module_number}'

    row = generate_result_line(motif, phasing1, supp_reads, len(anns_2good), len(anns_1good), len(anns_0good), prev_module_num, second_module_number=module_number)
    row_tuple = generate_row(seq, row, post_filter)

    tmp = generate_motifb64(seq, row)
    (locus_id, highlight, _, _, _, _, sequence) = tmp

    annotations = anns_2good + anns_1good
    if len(annotations) == 0:
        print(f"{motif_id} {suffix} is empty")

    locus_nomenclatures = generate_nomenclatures(anns_2good, prev_module_num, second_module_number, motif, nomenclature_limit)
    hist2d_data = write_histogram_image2d(annotations, prev_module_num, second_module_number, motif.module_str(prev_module_num), motif.module_str(second_module_number))

    graph_data = (None, None, hist2d_data)

    _, _, a1_prediction, a1_confidence, a1_reads, a1_indels, a1_mismatches, a2_prediction, a2_confidence, a2_reads, a2_indels, a2_mismatches, confidence, indels, mismatches, spanning_reads, flanking_reads = row_tuple
    raw_confidence = "tmp"
    locus_data2 = (
        locus_id, highlight, sequence, locus_nomenclatures,
        (a1_prediction, a1_confidence, a1_indels, a1_mismatches, a1_reads),
        (a2_prediction, a2_confidence, a2_indels, a2_mismatches, a2_reads),
        (confidence, indels, mismatches),
        raw_confidence,
        spanning_reads, flanking_reads,
        graph_data,
    )
    return locus_data2


def generate_nomenclatures(
    annotations: list[Annotation], module_num: int | None, next_module_num: int | None,
    motif: Motif, nomenclature_limit: int | None = None
) -> list[tuple[int, str, list[str]]]:

    count_dict = Counter(annot.get_nomenclature(motif, module_num, next_module_num, False) for annot in annotations)
    count_dict2 = sorted(count_dict.items(), key=lambda k: (-k[1], k[0]))

    lines = []
    for nomenclature, count in count_dict2:
        count2 = count
        ref = f'{motif.chrom}:g.{motif.start}_{motif.end}'
        parts = nomenclature.rstrip().split('\t')

        lines.append((count2, ref, parts))

        if nomenclature_limit is not None and len(lines) >= nomenclature_limit:
            break

    return lines


def load_arguments() -> Namespace:
    """
    Loads and parses the arguments.
    :return: args - parsed arguments
    """
    parser = ArgumentParser(
        formatter_class=RawDescriptionHelpFormatter,
        description=textwrap.dedent(DANTE_DESCRIPTION)
    )

    options = parser.add_argument_group('Options')
    options.add_argument(
        '--input-tsv', '-i', default="",
        help='Input annotation table as obtained by remaSTR. Default=stdin'
    )
    options.add_argument(
        '--output-dir', '-o', type=str, default="dante_out",
        help='Output destination (directory). Default=./dante_out/'
    )
    options.add_argument(
        '--verbose', '-v', action='store_true',
        help='Print all the outputs. Default is to print only the result table to stdout.'
    )
    options.add_argument(
        '--male', action='store_true',
        help='Indicate that the sample is male. Process motifs from chrX/chrY as mono-allelic.'
    )
    options.add_argument(
        '--nomenclatures', '-n', type=positive_int, default=5,
        help='Number of nomenclature strings to add to reports. Default=5'
    )
    options.add_argument(
        '--cutoff-alignments', type=positive_int, default=20,
        help='How many bases to keep beyond annotated part. Default=20'
    )

    args = parser.parse_args()

    return args


def positive_int(value: str) -> int:
    try:
        int_value = int(value)
    except ValueError:
        raise ArgumentTypeError(f'Value {value} is not integer') from None
    if int_value < 0:
        raise ArgumentTypeError(f'Value {value} is negative')
    return int_value


class Motif:
    """
    Class to represent DNA motifs.

    :ivar chrom: Chromosome name.
    :ivar start: Start position of the motif.
    :ivar end: End position of the motif.
    :ivar modules: A list of tuples containing sequence and repetition count.
    :ivar name: name of the Motif
    :ivar motif: motif nomenclature
    """

    def __init__(self, motif: str, name: str | None, male: bool) -> None:
        """
        Initialize a Motif object.
        :param motif: The motif string in the format "chrom:start_end[A][B]..."
        :param name: optional name of the motif
        """
        # remove whitespace
        nomenclature = motif.strip().replace(' ', '')
        name = (name if name is not None else nomenclature).replace(':', '-').replace('.', '_').replace('/', '_')

        # extract prefix, first number, second number
        tmp = re.match(r'([^:]+):g\.(\d+)_(\d+)(.*)', nomenclature)
        if tmp is None:
            raise ValueError(f"{nomenclature} has incorrect format")
        chrom, start, end, remainder = tmp.groups()

        # extract sequence and repetition count
        modules = [(str(seq), int(num)) for seq, num in re.findall(r'([A-Z]+)\[(\d+)', remainder)]
        modules = [('left_flank', 1)] + modules + [('right_flank', 1)]

        motif_monoallelic = male and chrom_from_string(chrom) in [ChromEnum.X, ChromEnum.Y]

        # store members
        self.nomenclature: str = nomenclature
        self.name: str = name
        self.chrom: str = chrom
        self.start: int = int(start)
        self.end: int = int(end)
        self.modules: list[tuple[str, int]] = modules
        self.monoallelic: bool = motif_monoallelic

    def __getitem__(self, index: int) -> tuple[str, int]:
        """
        Returns module at given index.
        :param index: The index of the module to fetch.
        :return: The module at the given index.
        """
        return self.modules[index]

    def __str__(self) -> str:
        """
        Returns string representation of the Motif object.
        :return: String representation in the format "chrom:start_end[A][B]..."
        """
        return f'{self.chrom}:g.{self.start}_{self.end}' + self.modules_str(include_flanks=False)

    def __lt__(self, obj):
        """
        Less than for sorting purposes.
        :return: bool - if this object comes before the other
        """
        return self.name < obj.name

    def __eq__(self, obj):
        """
        Equal to for sorting purposes.
        :return: bool - if this object is equal to the other
        """
        return self.name == obj.name

    def augmented_nomenclature(self, rep_counts: list[str]) -> list[str]:
        modules = []
        i = 0
        for seq, num in self.modules[1:-1]:
            if num == 1:
                modules.append(f"{seq}[{num}]")
            else:
                if rep_counts[i].startswith("err"):
                    x = rep_counts[i][4:-1]
                    modules.append(f"{seq}[{x}]")
                else:
                    modules.append(rep_counts[i])
                i += 1
        assert i == len(rep_counts), "Invalid augmentation."
        return modules

    def modules_str(self, include_flanks: bool = False) -> str:
        """
        Returns string representation of modules
        :param include_flanks: bool - include flank modules?
        :return: String representation of modules
        """
        if include_flanks:
            return ''.join([f'{seq}[{num}]' for seq, num in self.modules])
        return ''.join([f'{seq}[{num}]' for seq, num in self.modules[1:-1]])

    def module_str(self, module_number: int) -> str:
        """
        Returns string representation of modules
        :param module_number: int - module number
        :return: String representation of modules
        """
        seq, num = self.modules[module_number]
        return f'{seq}[{num}]'

    def get_repeating_modules(self) -> list[tuple[int, str, int]]:
        """
        Returns list of modules with more than one repetition.
        :return: List of tuples containing index, sequence, and repetition count.
        """
        return [(int(i), str(seq), int(num)) for i, (seq, num) in enumerate(self.modules) if num > 1]

    def get_location_subpart(self, index: int) -> tuple[int, int]:
        """
        Returns the chromosome location of a subpart of a motive
        :param index: int - index of a module
        :return: start and end location of the subpart
        """
        start = self.start
        for module in self.modules[1: index]:
            seq, rep = module
            start += len(seq) * rep

        return start, start + len(self.modules[index][0]) * self.modules[index][1]


class Annotation:
    """
    Encapsulate sequence of states from HMM and provide its readable representation and filters
    """

    def __init__(
        self, read_id: str, mate_order: int, read_seq: str, expected_seq: str,
        states: str, probability: float, motif: Motif
    ):
        """
        :param read_id: str - read ID
        :param read_seq: str - read sequence
        :param mate_order: int - mate order (0 - unpaired, 1 - left pair, 2 - right pair)
        :param expected_seq: str - expected sequence as in motif
        :param states: str - sequence of states (numbers of modules)
        :param probability: Probability of generating sequence by the most likely sequence of HMM states
        :param motif: Sequence of tuples (sequence, repeats) as specified by user
        """

        # Store arguments into instance variables
        self.read_id = read_id
        self.mate_order = mate_order
        self.read_seq = read_seq
        self.expected_seq = expected_seq
        self.states = states
        self.probability = probability
        self.n_modules = len(motif.modules)

        # Calculate insertion/deletion/mismatch string
        self.mismatches_string = self.__get_errors()

        # Calculate number of insertions, deletions and normal bases
        self.n_insertions = self.mismatches_string.count('I')
        self.n_deletions = self.mismatches_string.count('D')
        self.n_mismatches = self.mismatches_string.count('M')

        # Number of STR motif repetitions and sequences of modules
        self.module_bases = self.__get_bases_per_module()
        self.module_repetitions = self.__get_module_repetitions(motif)
        self.module_sequences = self.__get_module_sequences()

    def __str__(self) -> str:
        """
        Return the annotation.
        :return: str - annotation
        """
        return '\n'.join([f'{self.read_id} {str(self.module_bases)} {str(self.module_repetitions)}', self.read_seq,
                          self.expected_seq, self.states, self.mismatches_string])

    def __get_errors(self) -> str:
        """
        Count errors in annotation and the error line.
        :return: str - error line
        """
        err_line = []
        for exp, read in zip(self.expected_seq.upper(), self.read_seq.upper()):
            if exp == '-' or read in BASE_MAPPING.get(exp, ''):
                err_line.append('_')
            elif read == '_':
                err_line.append('D')
            elif exp == '_':
                err_line.append('I')
            else:
                err_line.append('M')

        return ''.join(err_line)

    def __get_bases_per_module(self) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of bases of input sequence that were generated by the module
        :return: Number of bases generated by each module
        """
        # Count the module states
        return tuple(self.states.count(chr(ord('0') + i)) for i in range(self.n_modules))

    def __get_module_repetitions(self, motif: Motif) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of repetitions of module in annotation
        :return: Number of repetitions generated by each module
        """
        # Count the module states
        repetitions = self.__get_bases_per_module()

        # Divide by the module length where applicable
        # TODO: this is not right for grey ones, where only closed ones should be counted, so round is not right.
        return tuple(
            1 if reps == 1 and cnt > 0 else round(cnt / len(seq))
            for (seq, reps), cnt in zip(motif.modules, repetitions)
        )

    def __get_module_sequences(self) -> tuple[str, ...]:
        """
        List of sequences, each per module
        :return: list(str)
        """
        sequences = [''] * self.n_modules
        for i in range(self.n_modules):
            state_char = chr(ord('0') + i)
            first = self.states.find(state_char)
            if first > -1:
                last = self.states.rfind(state_char)
                sequences[i] = self.read_seq[first:last + 1]
        return tuple(sequences)

    def get_module_errors(self, motif: Motif, module_num: int, overhang: int | None = None) -> tuple[int, int, int]:
        """
        Get the number of insertions and deletions or mismatches in a certain module.
        If overhang is specified, look at specified number of bases around the module as well.
        :param module_num: int - 0-based module number to count errors
        :param overhang: int - how long to look beyond module, if None, one length of STR module
        :return: int, int, int - number of insertions and deletions, mismatches, length of the interval
        """
        # get overhang as module length
        if overhang is None:
            seq, _ = motif.modules[module_num]
            overhang = len(seq)

        # define module character
        char_to_search = chr(ord('0') + module_num)

        # if the annotation does not have this module, return 0
        if char_to_search not in self.states:
            return 0, 0, 0

        # search for the annotation of the module
        start = max(0, self.states.find(char_to_search) - overhang)
        end = min(self.states.rfind(char_to_search) + overhang + 1, len(self.states))

        # count errors
        indels = self.mismatches_string[start:end].count('I') + self.mismatches_string[start:end].count('D')
        mismatches = self.mismatches_string[start:end].count('M')

        # return indels, mismatches, and length
        return indels, mismatches, end - start

    def has_less_errors(self, max_errors: float | int | None, relative=False) -> bool:
        """
        Check if this annotation has fewer errors than max_errors.
        Make it relative to the annotated length if relative is set.
        :param max_errors: int/float - number of max_errors (relative if relative is set)
        :param relative: bool - if the errors are relative to the annotated length
        :return: bool - True if the number of errors is less than allowed
        """
        errors = self.n_deletions + self.n_insertions + self.n_mismatches

        if max_errors is None or errors == 0:
            return True

        if relative:
            return errors / float(sum(self.module_bases)) <= max_errors
        return errors <= max_errors

    def primers(self, index_rep: int) -> int:
        """
        Count how any primers it has on repetition index.
        :param index_rep: int - index of the repetition, that we are looking at
        :return: int - number of primers (0-2)
        """
        primers = 0
        if index_rep > 0 and self.module_repetitions[index_rep - 1] > 0:
            primers += 1
        if index_rep + 1 < len(self.module_repetitions) and self.module_repetitions[index_rep + 1] > 0:
            primers += 1
        return primers

    def is_annotated_right(self) -> bool:
        """
        Is it annotated in a way that it is interesting?
        More than one module annotated + modules are not missing in the middle.
        :return: bool - annotated right?
        """

        # remove those that starts/ends in background but don't have a neighbour module
        starts_background = self.states[0] in '_-'
        ends_background = self.states[-1] in '_-'
        if starts_background and self.module_repetitions[0] == 0:
            return False
        if ends_background and self.module_repetitions[-1] == 0:
            return False

        # remove those with jumping modules
        started = False
        ended = False
        for repetition in self.module_repetitions:
            if repetition > 0:
                started = True
                if ended:
                    return False
            if repetition == 0 and started:
                ended = True

        # pass?
        return True

    def get_str_repetitions(self, index_str: int) -> tuple[bool, int] | None:
        """
        Get the number of str repetitions for a particular index.
        :param index_str: int - index of a str
        :return: (bool, int) - closed?, number of str repetitions
        """
        if self.is_annotated_right():
            primer1 = index_str > 0 and self.module_repetitions[index_str - 1] > 0
            primer2 = index_str + 1 < len(self.module_repetitions) and self.module_repetitions[index_str + 1] > 0
            if primer1 or primer2:
                return primer1 and primer2, self.module_repetitions[index_str]
        return None

    @staticmethod
    def find_with_regex(read_sequence: str, motif_sequence: str, search_pos: int = 0) -> int:
        """
        Find the first occurrence of a motif sequence in the read sequence using regular expressions.
        :param read_sequence: The sequence to search in.
        :param motif_sequence: The motif sequence (as a regex) to search for.
        :param search_pos: The position to start the search from.
        :return: int - The start position of the first occurrence of the motif sequence. Returns -1 if not found.
        """
        # convert motif sequence to regex
        motif_regex = ''.join(BASE_MAPPING[char] for char in motif_sequence)

        # compile the regular expression pattern
        pattern = re.compile(motif_regex)

        # search for the pattern in the read sequence starting from search_pos
        match = pattern.search(read_sequence, search_pos)

        # return the start position if a match is found, else return -1
        return match.start() if match else -1

    def get_nomenclature(
            self, motif: Motif, index_rep: int | None = None, index_rep2: int | None = None, include_flanking: bool = True
    ) -> str:
        """
        Get HGVS nomenclature.
        :param index_rep: int - index of the first repetition (None if include all)
        :param index_rep2: int - index of the second repetition (None if include all)
        :param include_flanking: boolean - include flanking regions (i.e. first and last module)
        :return: str - HGVS nomenclature string
        """
        # prepare data
        if index_rep is not None:
            if index_rep2 is not None:
                data = zip(
                    [self.module_repetitions[index_rep], self.module_repetitions[index_rep2]],
                    [motif[index_rep], motif[index_rep2]],
                    [self.module_sequences[index_rep], self.module_sequences[index_rep2]]
                )
            else:
                data = zip(
                    [self.module_repetitions[index_rep]],
                    [motif[index_rep]],
                    [self.module_sequences[index_rep]]
                )
        elif include_flanking:
            data = zip(self.module_repetitions, motif.modules, self.module_sequences)
        else:
            data = zip(self.module_repetitions[1:-1], motif.modules[1:-1], self.module_sequences[1:-1])

        # iterate and build the nomenclature string
        nomenclatures = []
        for repetitions, (motif_sequence, _), read_sequence in data:
            nomenclature = self.build_nomenclature_string(repetitions, motif_sequence, read_sequence)
            nomenclatures.append(nomenclature)

        return '\t'.join(nomenclatures)

    def build_nomenclature_string(self, repetitions, motif_sequence, read_sequence) -> str:
        nomenclature = ''
        if repetitions == 1:
            if len(read_sequence) > 0:
                nomenclature += f'{read_sequence}[1]'
            return nomenclature

        reps = 0
        search_pos = 0
        found_rep_seq = ''
        while True:
            search_found = self.find_with_regex(read_sequence, motif_sequence, search_pos)
            if search_found == search_pos:
                # setup current rep. sequence
                if reps == 0:
                    found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]

                if read_sequence[search_found:search_found + len(motif_sequence)] == found_rep_seq:
                    # regular continuation
                    reps += 1
                else:
                    # interruption, but in line with searched motif
                    nomenclature += f'{found_rep_seq}[{reps}]'
                    found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                    reps = 1
            elif search_found == -1:
                # the end, we did not find any other STRs
                if reps > 0:
                    nomenclature += f'{found_rep_seq}[{reps}]'
                if len(read_sequence[search_pos:]) > 0:
                    nomenclature += f'{read_sequence[search_pos:]}[1]'
                break
            else:
                # some interruption
                if reps > 0:
                    nomenclature += f'{found_rep_seq}[{reps}]'
                if len(read_sequence[search_pos:search_found]) > 0:
                    nomenclature += f'{read_sequence[search_pos:search_found]}[1]'
                found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                reps = 1
            # update search pos and iterate
            search_pos = search_found + len(motif_sequence)
        return nomenclature

def errors_per_read(
    errors: list[tuple[int, int, int]], relative: bool = False
) -> tuple[float | str, float | str]:
    """
    Count number of errors per read. Relative per length or absolute number.
    :param errors: list[tuple[int, int, int]] - indels, mismatches and length of module
    :param relative: bool - relative?
    :return: tuple[float, float] - number of indels, mismatches per hundred reads
    """
    # if we have no reads, return '---'
    if len(errors) == 0:
        return '---', '---'

    if relative:
        mean_length = np.mean([length for _, _, length in errors])
        return (
            float(np.mean([indels / float(length) for indels, _, length in errors]) * mean_length),
            float(np.mean([mismatches / float(length) for _, mismatches, length in errors]) * mean_length)
        )
    return (
        float(np.mean([indels for indels, _, _ in errors])),
        float(np.mean([mismatches for _, mismatches, _ in errors]))
    )


def generate_result_line(
    motif: Motif, predicted: tuple[str | int, str | int], confidence: tuple[float | str, ...],
    qual_num: int, primer_num: int, filt_num: int, module_number: int,
    qual_annot: list[Annotation] | None = None,
    second_module_number: int | None = None
) -> dict:
    """
    Generate result line from the template string.
    :param motif_class: Motif - motif class
    :param predicted: tuple[str, str] - predicted alleles (number or 'B'/'E')
    :param confidence: tuple[7x float/str] - confidences of prediction
    :param qual_num: int - number of reads with both primers
    :param primer_num: int - number of reads with exactly one primer
    :param filt_num: int - number of filtered out reads (no primers, many errors, ...)
    :param module_number: int - module number in motif
    :param qual_annot: list[Annotation] - list of quality annotations for error and number of reads
    :param second_module_number: int/None - second module number in motif
    :return: dict - result dictionary
    """
    # setup motif info
    start, end = motif.get_location_subpart(module_number)
    motif_seq = motif.module_str(module_number)
    repetition_index: int | str = module_number
    if second_module_number is not None:
        _, end = motif.get_location_subpart(second_module_number)
        motif_seq = ','.join([motif.module_str(i) for i in range(module_number, second_module_number + 1)])
        repetition_index = f'{module_number}_{second_module_number}'

    reads_a1: int | str
    reads_a2: int | str
    indels_rel: float | str
    indels_rel1: float | str
    indels_rel2: float | str
    mismatches_rel: float | str
    mismatches_rel1: float | str
    mismatches_rel2: float | str
    # get info about errors and number of reads from quality annotations if provided
    reads_a1 = reads_a2 = '---'
    indels_rel = mismatches_rel = '---'
    indels_rel1 = mismatches_rel1 = '---'
    indels_rel2 = mismatches_rel2 = '---'
    if qual_annot is not None:
        # get info about number of reads
        a1 = int(predicted[0]) if isinstance(predicted[0], int) else None
        a2 = int(predicted[1]) if isinstance(predicted[1], int) else None
        reads_a1 = 0 if a1 is None else len([a for a in qual_annot if a.module_repetitions[module_number] == a1])
        reads_a2 = 0 if a2 is None else len([a for a in qual_annot if a.module_repetitions[module_number] == a2])

        # get info about errors
        errors = [a.get_module_errors(motif, module_number) for a in qual_annot]
        errors_a1 = [a.get_module_errors(motif, module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a1]
        errors_a2 = [a.get_module_errors(motif, module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a2]
        assert len([l for i, _, l in errors if l == 0]) == 0

        # extract error metrics
        indels_rel, mismatches_rel = errors_per_read(errors, relative=True)
        indels_rel1, mismatches_rel1 = errors_per_read(errors_a1, relative=True)
        indels_rel2, mismatches_rel2 = errors_per_read(errors_a2, relative=True)

    return {
        'motif_name': motif.name, 'motif_nomenclature': motif.nomenclature, 'motif_sequence': motif_seq,
        'chromosome': motif.chrom, 'start': start, 'end': end,
        'allele1': predicted[0], 'allele2': predicted[1],
        'confidence': confidence[0], 'conf_allele1': confidence[1], 'conf_allele2': confidence[2],
        'reads_a1': reads_a1, 'reads_a2': reads_a2,
        'indels': indels_rel, 'indels_a1': indels_rel1, 'indels_a2': indels_rel2,
        'mismatches': mismatches_rel, 'mismatches_a1': mismatches_rel1, 'mismatches_a2': mismatches_rel2,
        'quality_reads': qual_num, 'one_primer_reads': primer_num, 'filtered_reads': filt_num,
        'conf_background': confidence[3] if len(confidence) > 3 else '---',
        'conf_background_all': confidence[4] if len(confidence) > 4 else '---',
        'conf_extended': confidence[5] if len(confidence) > 5 else '---',
        'conf_extended_all': confidence[6] if len(confidence) > 6 else '---',
        'repetition_index': repetition_index
    }


def phase(
    annotations: list[Annotation], module_number1: int, module_number2: int
) -> tuple[tuple[str, str], tuple[str, str, str]]:
    """
    Infer phasing based on the Annotations.
    :param annotations: list(Annotation) - good (blue) annotations
    :param module_number1: int - index of a repetition
    :param module_number2: int - index of the second repetition
    :return: tuple - predicted symbols and confidences
    """
    # resolve trivial case
    if len(annotations) == 0:
        return ('-|-', '-|-'), ('-/0', '-/0', '-/0')

    # gather module repetitions from annotations and count them
    repetitions = Counter([
        (ann.module_repetitions[module_number1], ann.module_repetitions[module_number2]) for ann in annotations
    ])

    # pick the highest two
    most_common = repetitions.most_common(2)
    rep1, cnt1 = most_common[0]
    rep2, cnt2 = most_common[1] if len(most_common) >= 2 else (('-', '-'), 0)

    # output phasing with number of supported reads
    phasing = (f'{rep1[0]}|{rep1[1]}', f'{rep2[0]}|{rep2[1]}')
    supported_reads = (f'{cnt1 + cnt2}/{len(annotations)}', f'{cnt1}/{len(annotations)}', f'{cnt2}/{len(annotations)}')
    return phasing, supported_reads


class PostFilter:
    """
    Class that encapsulates post-filtering.
    """

    def __init__(self):
        self.min_flank_len = MIN_FLANK_LEN
        self.min_rep_len = MIN_REP_LEN
        self.min_rep_cnt = MIN_REP_CNT
        self.max_rel_error = MAX_REL_ERROR
        self.max_abs_error = MAX_ABS_ERROR

    def quality_annotation(self, motif: Motif, ann: Annotation, module_number: int, both_primers: bool = True) -> bool:
        """
        Is this annotation good?
        :param ann: Annotation - annotation to be evaluated
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: bool - quality annotation?
        """
        is_right = ann.is_annotated_right()

        primers = ann.primers(module_number)
        has_primers = primers == 2 if both_primers else primers >= 1

        has_less_errors = (
            ann.has_less_errors(self.max_rel_error, relative=True)
            and ann.has_less_errors(self.max_abs_error, relative=False)
        )

        left_flank = sum(ann.module_bases[module_number + 1:]) >= self.min_flank_len
        right_flank = sum(ann.module_bases[:module_number]) >= self.min_flank_len
        has_flanks = left_flank and right_flank if both_primers else left_flank or right_flank

        has_repetitions = (
            ann.module_bases[module_number] >= self.min_rep_len
            and ann.module_repetitions[module_number] >= self.min_rep_cnt
        )

        _seq, reps = motif.modules[module_number]

        return is_right and has_primers and has_less_errors and has_flanks and (has_repetitions or reps == 1)

    def get_filtered_list(
        self, motif: Motif, annotations: list[Annotation], module_number: list[int], both_primers: list[bool] | None = None
    ) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations (list of modules).
        :param annotations: list(Annotation) - annotations
        :param module_number: list(int) - module numbers
        :param both_primers: list(bool) or None - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # adjust input if needed
        if both_primers is None:
            both_primers = [True] * len(module_number)
        assert len(both_primers) == len(module_number)

        # filter annotations
        quality_annotations = [
            an for an in annotations
            if all((
                self.quality_annotation(motif, an, mn, both_primers=bp) for mn, bp in zip(module_number, both_primers)
            ))
        ]
        filtered_annotations = [an for an in annotations if an not in quality_annotations]

        return quality_annotations, filtered_annotations

    # TODO: is this just a specialized version of the previous method? Do we need this?
    def get_filtered(
        self, motif: Motif, annotations: list[Annotation], module_number: int, both_primers: bool = True
    ) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations.
        :param annotations: list(Annotation) - annotations
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # pick quality annotations
        quality_annotations = []
        filtered_annotations = []
        for an in annotations:
            if self.quality_annotation(motif, an, module_number, both_primers):
                quality_annotations.append(an)
            else:
                filtered_annotations.append(an)

        return quality_annotations, filtered_annotations


class ChromEnum(enum.Enum):
    X = 'X'
    Y = 'Y'
    NORM = 'NORM'


def chrom_from_string(chrom_str: str) -> ChromEnum:
    """
    Converts a string to a ChromEnum object.
    :param chrom_str: str - the string to convert to a ChromEnum object
    :return ChromEnum - enum object representing the chromosome
    """
    return (
        ChromEnum.X if chrom_str in ['chrX', 'NC_000023'] else
        ChromEnum.Y if chrom_str in ['chrY', 'NC_000024'] else
        ChromEnum.NORM
    )


def sorted_repetitions(annotations: list[Annotation]) -> list[tuple[tuple[int, ...], int]]:
    """
    Aggregate same repetition counts for annotations and sort them according to quantity of repetitions of each module
    :param annotations: Annotated reads
    :return: list of (repetitions, count), sorted by repetitions
    """
    count_dict = Counter(tuple(annot.module_repetitions) for annot in annotations)
    return sorted(count_dict.items(), key=lambda k: k[0])


def write_histogram_image2d(
    deduplicated: list[Annotation], index_rep: int, index_rep2: int, seq: str, seq2: str
) -> Hist2DGraph | None:
    if deduplicated is None or len(deduplicated) == 0:
        return None

    dedup_reps: list[tuple[tuple[bool, int], tuple[bool, int]]] = []
    for x in deduplicated:
        r_1 = x.get_str_repetitions(index_rep)
        r_2 = x.get_str_repetitions(index_rep2)
        if r_1 is not None and r_2 is not None:
            dedup_reps.append((r_1, r_2))

    if len(dedup_reps) == 0:
        return None

    # assign maximals
    xm = max(r for (_, r), _ in dedup_reps)
    ym = max(r for _, (_, r) in dedup_reps)
    max_ticks = max(ym, xm) + 2
    xm = max(MAX_REPETITIONS, xm)
    ym = max(MAX_REPETITIONS, ym)

    # create data containers
    data = np.zeros((xm + 1, ym + 1), dtype=int)
    data_primer = np.zeros((xm + 1, ym + 1), dtype=int)
    for ((c1, r1), (c2, r2)) in dedup_reps:
        if c1 and c2:
            data[r1, r2] += 1
        if c1 and not c2:
            data_primer[r1, r2:] += 1
        if not c1 and c2:
            data_primer[r1:, r2] += 1

    str1 = 'STR %d [%s]' % (index_rep + 1, seq.split('-')[-1])
    str2 = 'STR %d [%s]' % (index_rep2 + 1, seq2.split('-')[-1])

    def parse_labels(num, num_primer):
        if num == 0 and num_primer == 0:
            return ''
        if num == 0 and num_primer != 0:
            return '0/%s' % str(num_primer)
        if num != 0 and num_primer == 0:
            return '%s/0' % str(num)
        return '%s/%s' % (str(num), str(num_primer))

    z_partial = data_primer[:max_ticks, :max_ticks]
    z_full = data[:max_ticks, :max_ticks]
    text = [[parse_labels(z_full[i, j], z_partial[i, j]) for j in range(z_full.shape[1])] for i in range(z_full.shape[0])]

    z_partial_out: list[list[int]] = z_partial.tolist()
    z_full_out: list[list[int]] = z_full.tolist()
    return (z_partial_out, z_full_out, text, str1, str2)


def write_histogram_image(
    annotations: list[Annotation], filt_annot: list[Annotation], index_rep: int
) -> HistReadCounts:
    """
    Stores quantity of different combinations of module repetitions, generates separate graph image for each module
    :param out_prefix: Output file prefix
    :param annotations: Annotated reads.
    :param filt_annot: Annotated reads (filtered)
    :param index_rep: int - index of repetition module of a motif
    """
    repetitions = sorted_repetitions(annotations)
    repetitions_filt = sorted_repetitions(filt_annot)

    spanning_counts = [(r[index_rep], c) for r, c in repetitions]
    filtered_counts = [(r[index_rep], c) for r, c in repetitions_filt]
    inread_counts: list[tuple] = []

    xm = max(
        [r for r, c in spanning_counts]
        + [r for r, c in filtered_counts]
        + [r for r, c in inread_counts]
        + [MAX_REPETITIONS]
    )

    # set data
    spanning = [0] * (xm + 1)
    for r, c in spanning_counts:
        spanning[r] += c

    flanking = spanning.copy()
    for r, c in filtered_counts:
        flanking[r] += c

    inread = flanking.copy()
    for r, c in inread_counts:
        inread[r] += c

    # plot_histogram_image_plotly(out_prefix, spanning, flanking, inread)

    only_flanking = [df - d for df, d in zip(flanking, spanning)]
    only_inread = [di - df for di, df in zip(inread, flanking)]
    return spanning, only_flanking, only_inread


def combine_distribs(deletes, inserts):
    """
    Combine insert and delete models/distributions
    :param deletes: ndarray - delete distribution
    :param inserts: ndarray - insert distribution
    :return: ndarray - combined array of the same length
    """
    # how much to fill?
    to_fill = sum(deletes == 0.0) + 1
    while to_fill < len(inserts) and inserts[to_fill] > 0.0001:
        to_fill += 1

    # create the end array
    end_distr = np.zeros_like(deletes, dtype=float)

    # fill it!
    for i, a in enumerate(inserts[:to_fill]):
        end_distr[i:] += (deletes * a)[:len(deletes) - i]

    return end_distr


def const_rate(_n, p1=0.0, _p2=1.0, _p3=1.0):
    return p1


def linear_rate(n, p1=0.0, p2=1.0, _p3=1.0):
    return p1 + p2 * n


def quadratic_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * n + p3 * n * n


def exp_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * math.exp(p3 * n)


def clip(value, minimal, maximal):
    """
    Clips value to range <minimal, maximal>
    :param value: ? - value
    :param minimal: ? - minimal value
    :param maximal: ? - maximal value
    :return: ? - clipped value
    """
    return min(max(minimal, value), maximal)


def model_full(rng, model_params, n, rate_func=linear_rate):
    """
    Create binomial model for both deletes and inserts of STRs
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param n: int - target allele number
    :param rate_func: function - rate function for deletes
    :return: ndarray - combined distribution
    """
    p1, p2, p3, q = model_params
    deletes = binom.pmf(np.arange(rng), n, clip(1 - rate_func(n, p1, p2, p3), 0.0, 1.0))
    inserts = binom.pmf(np.arange(rng), n, q)
    return combine_distribs(deletes, inserts)


def model_template(rng, model_params, rate_func=linear_rate):
    """
    Partial function for model creation.
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param rate_func: function - rate function for deletes
    :return: partial function with only 1 parameter - n - target allele number
    """
    return functools.partial(model_full, rng, model_params, rate_func=rate_func)


def generate_models(
    min_rep: int, max_rep: int, multiple_backgrounds: bool = True
) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :param multiple_backgrounds: bool - whether to generate all background states
    :return: generator of allele pairs (numbers or 'E' or 'B')
    """
    for model_index1 in range(min_rep, max_rep):
        for model_index2 in range(model_index1, max_rep):
            yield model_index1, model_index2
        yield model_index1, 'E'
        if multiple_backgrounds:
            yield 'B', model_index1

    yield 'B', 'B'
    yield 'E', 'E'


def generate_models_one_allele(min_rep: int, max_rep: int) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :return: generator of allele pairs (numbers or 'E' or 'B'), 'X' for non-existing allele
    """
    for model_index1 in range(min_rep, max_rep):
        yield model_index1, 'X'

    yield 'B', 'X'
    yield 'E', 'X'


class Inference:
    """ Class for inference of alleles. """
    MIN_REPETITIONS = 1
    # default parameters for inference (miSeq default)
    # DEFAULT_MODEL_PARAMS = (0.00716322, 0.000105087, 0.0210812, 0.0001648)
    DEFAULT_MODEL_PARAMS = (0.00716322, 0.000105087, 0.0210812, 0.00716322)  # it actually changes things
    DEFAULT_FIT_FUNCTION = 'linear'

    # TODO: type the members
    def __init__(
        self, read_distribution, params_file,
        str_rep=MIN_REP_CNT, minl_primer1=MIN_FLANK_LEN, minl_primer2=MIN_FLANK_LEN, minl_str=MIN_REP_LEN,
        p_bckg_closed=None, p_bckg_open=None, p_expanded=None
    ):
        """
        Initialization of the Inference class + setup of all models and their probabilities.
        :param read_distribution: ndarray(int) - read distribution
        :param params_file: str - filename of parameters, None for defaults
        :param str_rep: int - length of the STR
        :param minl_primer1: int - minimal length of the left primer
        :param minl_primer2: int - minimal length of the right primer
        :param minl_str: int - minimal length of the STR
        :param p_bckg_closed: float - probability of the background model for closed observation
        :param p_bckg_open: float - probability of the background model for open observation
        :param p_expanded: float - probability of the expanded model (if None it is equal to other models)
        """
        # assign variables
        self.str_rep = str_rep
        self.minl_primer1 = minl_primer1
        self.minl_primer2 = minl_primer2
        self.minl_str = minl_str
        self.read_distribution = read_distribution
        self.sum_reads = np.sum(read_distribution)
        self.params_file = params_file
        self.p_expanded = p_expanded
        self.p_bckg_closed = p_bckg_closed
        self.p_bckg_open = p_bckg_open

    def construct_models(self, min_rep, max_rep, e_model):
        """
        Construct all models needed for current inference.
        :param min_rep: int - minimal allele to model
        :param max_rep: int - maximal allele to model
        :param e_model: int - model for expanded alleles
        :return: None
        """
        # extract params
        model_params = self.DEFAULT_MODEL_PARAMS
        rate_func_str = self.DEFAULT_FIT_FUNCTION
        str_to_func = {'linear': linear_rate, 'const': const_rate, 'exponential': exp_rate, 'square': quadratic_rate}
        rate_func = const_rate
        if rate_func_str in str_to_func.keys():
            rate_func = str_to_func[rate_func_str]

        # save min_rep and max_rep
        self.min_rep = min_rep
        self.max_rep = max_rep  # non-inclusive
        self.max_with_e = e_model + 1  # non-inclusive

        # get models
        mt = model_template(self.max_with_e, model_params, rate_func)
        self.background_model = np.concatenate([
            np.zeros(self.min_rep, dtype=float),
            np.ones(self.max_with_e - self.min_rep, dtype=float) / float(self.max_with_e - self.min_rep)
        ])
        self.expanded_model = mt(self.max_with_e - 1)
        self.allele_models = {i: mt(i) for i in range(min_rep, max_rep)}
        self.models = {'E': self.expanded_model, 'B': self.background_model}
        self.models.update(self.allele_models)

        # get model likelihoods
        open_to_closed = 10.0

        l_others = 1.0
        l_bckg_open = 0.01
        l_exp = 1.01

        l_bckg_model_open = 1.0

        if self.p_expanded is None:
            self.p_expanded = l_exp
        if self.p_bckg_open is None and self.p_bckg_closed is None:
            self.p_bckg_open = l_bckg_open
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_closed is None:
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_open is None:
            self.p_bckg_open = self.p_bckg_closed * open_to_closed

        self.model_probabilities = {'E': self.p_expanded, 'B': l_bckg_model_open}
        self.model_probabilities.update({i: l_others for i in self.allele_models.keys()})

    def likelihood_rl(self, rl):
        """
        Likelihood of a read with this length.
        :param rl: int - read length
        :return: float - likelihood of a read this long
        """
        return self.read_distribution[rl] / float(self.sum_reads)

    @staticmethod
    def likelihood_model(model, g):
        """
        Likelihood of a generated allele al from a model of
        :param model: ndarray - model that we evaluate
        :param g: int - observed read count
        :return: float - likelihood of a read coming from this model
        """
        return model[g]

    def likelihood_coverage(self, true_length, rl, closed=True):
        """
        Likelihood of generating a read with this length and this allele.
        :param true_length: int - true number of repetitions of an STR
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of a read being generated with this attributes
        """
        whole_inside_str = max(0, true_length * self.str_rep + self.minl_primer1 + self.minl_primer2 - rl + 1)
        # closed_overlapping = max(0, rl - self.minl_primer1 - self.minl_primer2 - true_length * self.str_rep + 1)
        open_overlapping = max(0, rl + true_length * self.str_rep - 2 * self.minl_str + 1)

        assert open_overlapping > whole_inside_str, '%d open %d whole inside %d %d %d' % (
            open_overlapping, whole_inside_str, true_length, rl, self.minl_str)

        return 1.0 / float(open_overlapping - whole_inside_str)

    def likelihood_read_allele(self, model, observed, rl, closed=True):
        """
        Likelihood of generation of read with observed allele count and rl.
        :param model: ndarray - model for the allele
        :param observed: int - observed allele count
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return:
        """
        if closed:
            return (self.likelihood_rl(rl)
                    * self.likelihood_model(model, observed)
                    * self.likelihood_coverage(observed, rl, True))
        number_of_options = 0
        partial_likelihood = 0
        for true_length in itertools.chain(range(observed, self.max_rep), [self.max_with_e - 1]):
            partial_likelihood += (self.likelihood_model(model, true_length)
                                   * self.likelihood_coverage(true_length, rl, False))
            number_of_options += 1

        return self.likelihood_rl(rl) * partial_likelihood / float(number_of_options)

    @functools.lru_cache()
    def likelihood_read(
        self, observed: int, rl: int, model_index1: int, model_index2: int | None = None, closed: bool = True
    ) -> float:
        """
        Compute likelihood of generation of a read from either of those models.
        :param observed: int - observed allele count
        :param rl: int - read length
        :param model_index1: char/int - model index for left allele
        :param model_index2: char/int - model index for right allele or None if mono-allelic
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of this read generation
        """
        # TODO: tuto podla mna nemoze byt len tak +, chyba tam korelacia modelov, ale v ramci zjednodusenia asi ok
        allele1_likelihood = (
            self.model_probabilities[model_index1] * self.likelihood_read_allele(self.models[model_index1], observed, rl, closed)
        )
        allele2_likelihood = 0.0 if model_index2 is None else (
            self.model_probabilities[model_index2] * self.likelihood_read_allele(self.models[model_index2], observed, rl, closed)
        )

        p_bckg = self.p_bckg_closed if closed else self.p_bckg_open
        bckgrnd_likelihood = p_bckg * self.likelihood_read_allele(self.models['B'], observed, rl, closed)

        assert not np.isnan(allele2_likelihood)
        assert not np.isnan(allele1_likelihood)
        assert not np.isnan(bckgrnd_likelihood)

        # "tuto" refers to the next line
        return allele1_likelihood + allele2_likelihood + bckgrnd_likelihood

    def infer(
        self, annotations: list[Annotation], filt_annotations: list[Annotation],
        index_rep: int, monoallelic: bool = False
    ) -> dict[tuple[int | str, int | str], float]:
        """
        Does all the inference,
        computes for which 2 combination of alleles are these annotations and parameters the best.
        argmax_{G1, G2} P(G1, G2 | AL, COV, RL)
            ~ P(AL, COV, RL | G1, G2) * P(G1, G2)
            = prod_{read_i} P(al_i, cov_i, rl_i | G1, G2) * P(G1, G2)
            = independent G1 G2
            = prod_{read_i} P(al_i, cov_i, rl_i | G1) * P(al_i, cov_i, rl_i | G2) * P(G1) * P(G2)
            {here G1, G2 is from possible alleles, background, and expanded, priors are from params}

         P(al_i, cov_i, rl_i | G1) - 2 options:
             1. closed evidence (al_i = X), we know X;
             2. open evidence (al_i >= X), cl_i == True if i is closed

         1.: P(al_i, cov_i, rl_i, cl_i | G1)
            = P(rl_i from read distrib.) * p(allele is al_i | G1) * P(read generated closed evidence | rl_i, al_i)
         2.: P(rl_i is from r.distr.) * P(allele is >= al_i | G1) * P(read generated open evidence | rl_i, al_i)

        :param annotations: list(Annotation) - closed annotated reads (both primers set)
        :param filt_annotations: list(Annotation) - open annotated reads (only one primer set)
        :param index_rep: int - index of a repetition
        :param verbose: bool - print more stuff?
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: dict(tuple(int, int):float) - directory of model indices to their likelihood
        """
        # generate closed observed and read_length arrays
        observed_annots = np.array([ann.module_repetitions[index_rep] for ann in annotations])
        rl_annots = np.array([len(ann.read_seq) for ann in annotations])
        closed_annots = np.ones_like(observed_annots, dtype=bool)

        # generate open observed and read_length arrays
        observed_fa = np.array([ann.module_repetitions[index_rep] for ann in filt_annotations])
        rl_fa = np.array([len(ann.read_seq) for ann in filt_annotations])
        closed_fa = np.zeros_like(observed_fa, dtype=bool)

        # join them and keep the information if they are open or closed
        observed_arr = np.concatenate((observed_annots, observed_fa)).astype(int)
        rl_arr = np.concatenate((rl_annots, rl_fa)).astype(int)
        closed_arr = np.concatenate([closed_annots, closed_fa]).astype(bool)

        # generate the boundaries:
        overhead = 3
        if len(observed_annots) == 0:
            max_rep = max(observed_fa) + overhead  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, max(observed_fa) - overhead)  # inclusive
        else:
            max_rep = max(observed_annots) + overhead + 1  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, min(observed_annots) - overhead)  # inclusive

        # expanded allele
        e_allele = max_rep
        if len(observed_fa) > 0:
            e_allele = max(max_rep, max(observed_fa) + 1)

        # generate all the models
        self.construct_models(min_rep, max_rep, e_allele)

        # go through every model and evaluate:
        evaluated_models = {}
        if monoallelic:
            models = generate_models_one_allele(min_rep, max_rep)
        else:
            models = generate_models(min_rep, max_rep, multiple_backgrounds=True)
        for m1, m2 in models:
            evaluated_models[(m1, m2)] = 0.0
            # go through every read
            for obs, rl, closed in zip(observed_arr, rl_arr, closed_arr):
                lh = self.likelihood_read(obs, rl, m1, None if m2 == 'X' else m2, closed=closed)
                # TODO weighted sum according to the closeness/openness of reads?
                evaluated_models[(m1, m2)] += np.log(lh)

        return evaluated_models

    def predict(self, lh_dict: dict[tuple[int | str, int | str], float]) -> tuple[np.ndarray, tuple[int, int]]:
        # convert to a numpy array:
        lh_array = np.zeros((self.max_rep, self.max_rep + 1))
        for (k1, k2), v in lh_dict.items():
            if k2 == 'X':  # if we have mono-allelic
                k2 = k1
            # B is the smallest, E is the largest!
            if k2 == 'B' or k1 == 'E' or (isinstance(k1, int) and isinstance(k2, int) and k2 < k1):
                k1, k2 = k2, k1
            if k1 == 'B':
                k1 = 0
            if k2 == 'B':
                k2 = 0
            if k1 == 'E':  # only if k2 is 'E' too.
                k1 = 0
            if k2 == 'E':
                k2 = self.max_rep
            lh_array[k1, k2] = v

        # get minimal and maximal likelihood
        ind_good = (lh_array < 0.0) & (lh_array > -1e10) & (lh_array != np.nan)
        if len(lh_array[ind_good]) == 0:
            return lh_array, (0, 0)
        lh_array[~ind_good] = -np.inf
        best = sorted(np.unravel_index(np.argmax(lh_array), lh_array.shape))
        prediction = (int(best[0]), int(best[1]))

        # output best option
        return lh_array, prediction

    def convert_to_sym(self, best: tuple[int, int], monoallelic: bool) -> tuple[int | str, int | str]:
        """
        Convert numeric alleles to their symbolic representations.
        :param best: (int, int) - numeric representation of alleles
        :param monoallelic: bool - if this is monoallelic version
        :return: (int|str, int|str) - symbolic representation of alleles
        """
        # convert it to symbols
        if best[0] == 0 and best[1] == self.max_rep:
            best_sym = ('E', 'E')
        else:
            def fn1(x):
                return 'E' if x == self.max_rep else 'B' if x == 0 else x
            best_sym = tuple(map(fn1, best))

        # if mono-allelic return 'X' as second allele symbol
        if monoallelic:
            best_sym = (best_sym[0], 'X')

        return best_sym

    def get_confidence(
        self, lh_array: np.ndarray, predicted: tuple[int, int], monoallelic: bool = False
    ) -> Confidences:
        """
        Get confidence of a prediction.
        :param lh_array: 2D-ndarray - log likelihoods of the prediction
        :param predicted: tuple(int, int) - predicted alleles
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: tuple[float, float, float | str, float, float, float, float] - prediction confidence of
        all, first, and second allele(s), background and expanded states
        """
        # get confidence
        lh_corr_array = lh_array - np.max(lh_array)
        lh_sum = np.sum(np.exp(lh_corr_array))
        confidence: float = np.exp(lh_corr_array[predicted[0], predicted[1]]) / lh_sum
        confidence1: float
        confidence2: float
        if predicted[0] == predicted[1]:  # same alleles - we compute the probability per allele
            confidence1 = np.sum(np.exp(lh_corr_array[predicted[0], :])) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        elif predicted[1] == lh_corr_array.shape[0]:  # expanded allele - expanded is only on one side of the array
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        else:  # normal behavior - different alleles , no expanded, compute all likelihoods of the alleles
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = (
                np.sum(np.exp(lh_corr_array[:, predicted[1]]))
                + np.sum(np.exp(lh_corr_array[predicted[1], :]))
                - np.exp(lh_corr_array[predicted[1], predicted[1]])
            ) / lh_sum

        confidence_back: float = np.exp(lh_corr_array[0, 0]) / lh_sum
        confidence_back_all: float = np.sum(np.exp(lh_corr_array[0, :])) / lh_sum
        confidence_exp: float = np.exp(lh_corr_array[0, self.max_rep]) / lh_sum
        confidence_exp_all: float = np.sum(np.exp(lh_corr_array[:, self.max_rep])) / lh_sum

        if monoallelic:
            # confidence2 = '---'  # TODO: fix this
            confidence2 = np.nan

        return (
            confidence, confidence1, confidence2,
            confidence_back, confidence_back_all,
            confidence_exp, confidence_exp_all
        )

    def genotype(
        self, spanning: list[Annotation], partial: list[Annotation], index_rep: int, monoallelic: bool = False
    ) -> tuple[np.ndarray | None, tuple[str | int, str | int], Confidences]:
        """
        Genotype based on all annotations - infer likelihoods
        :param spanning: list(Annotation) - good (blue) annotations
        :param partial: list(Annotation) - (grey) annotations with one primer
        :param index_rep: int - index of a repetition
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: predicted symbols and confidences
        """
        # if we do not have any good annotations, then quit
        if len(spanning) == 0 and len(partial) == 0:
            return None, ('B', 'B'), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)

        # infer likelihoods
        lh_dict = self.infer(spanning, partial, index_rep, monoallelic)
        lh_array, predicted = self.predict(lh_dict)

        # adjust for no spanning reads (should output Background)
        if len(spanning) == 0:
            predicted = (0, 0)

        # convert numbers to symbols
        predicted_sym = self.convert_to_sym(predicted, monoallelic)

        # get confidence of our prediction
        confidence = self.get_confidence(lh_array, predicted, monoallelic)

        # return predicted and confidence
        return lh_array, predicted_sym, confidence


def draw_pcolor(
    model: Inference, lh_array: np.ndarray, name: str, lognorm: bool = True
) -> ProbHeatmap:
    ind_good = (lh_array < 0.0) & (lh_array > -1e10) & (lh_array != np.nan)
    z_min, z_max = min(lh_array[ind_good]), max(lh_array[ind_good])
    max_str = len(lh_array)
    if lognorm:
        lh_view = -np.log(-lh_array)
        z_min = -np.log(-z_min)
        z_max = -np.log(-z_max)
    else:
        lh_view = lh_array.copy()

    # background (B, i) - copy it below min_rep
    lh_view[model.min_rep - 1, :] = lh_view[0, :]

    lh_copy = lh_view.copy()
    lh_copy[-1, model.min_rep] = lh_copy[0, 0]
    lh_copy[-1, model.min_rep + 1] = lh_copy[0, model.max_rep]

    title = '%s likelihood of options (%s)' % ('Loglog' if lognorm else 'Log', name)
    # print(lh_copy.shape, lognorm, title, max_str)
    heatmap_data = save_pcolor_plotly_file(model, lh_copy, lognorm, title, max_str)
    return heatmap_data


def save_pcolor_plotly_file(
    model: Inference, lh_copy: np.ndarray, lognorm: bool, title: str, max_str: int, start_ticks: int = 5, step_ticks: int = 5
) -> ProbHeatmap:
    text = [['' for _ in range(max_str - model.min_rep + 1)] for _ in range(max_str - model.min_rep + 1)]
    text[-1][0] = 'B'
    text[-1][1] = 'E'

    hovertext = []
    for j in ['B'] + list(range(model.min_rep, max_str)):
        inner = [f'{j}/{i}' for i in list(range(model.min_rep, max_str)) + ['E']]
        hovertext.append(inner)

    hovertext[0][-1] = 'E/E'
    hovertext[-1][0] = 'B'
    hovertext[-1][1] = 'E'

    z: list[list[float | None]] = lh_copy[model.min_rep - 1:, model.min_rep:].tolist()
    for i in range(len(z)):
        for j in range(len(z[0])):
            if z[i][j] == -np.inf:
                z[i][j] = None
    hovertext2: list[list[str]] = hovertext
    y_tickvals: list[int] = list(range(start_ticks - model.min_rep + 1, max_str - model.min_rep + 1, step_ticks)) + [0]
    y_ticktext: list[int | str] = list(range(start_ticks, max_str, step_ticks)) + ['B']
    x_tickvals: list[int] = list(range(start_ticks - model.min_rep, max_str - model.min_rep, step_ticks)) + [int(max_str - model.min_rep)]
    x_ticktext: list[int | str] = list(range(start_ticks, max_str, step_ticks)) + ['E(>%d)' % (model.max_with_e - 2)]
    x_pos: float = max_str - model.min_rep - 0.5

    return (z, hovertext2, y_tickvals, y_ticktext, x_tickvals, x_ticktext, x_pos)


def highlight_subpart(seq: str, highlight: int | list[int]) -> tuple[str, str]:
    """
    Highlights subpart of a motif sequence
    :param seq: str - motif sequence
    :param highlight: int/list(int) - part ot highlight
    :return: str, str - motif sequence with highlighted subpart, highlighted subpart
    """
    if highlight is None:
        return seq, ''

    str_part = []
    highlight1 = np.array(highlight)
    split = [f'{s}]' for s in seq.split(']') if s != '']
    for h in highlight1:
        str_part.append(split[h])
        split[h] = f'<b><u>{split[h]}</u></b>'
    return ''.join(split), ''.join(str_part)


def float_to_str(c: float | str, percents: bool = False, decimals: int = 1) -> str:
    """
    Convert float confidence to string.
    :param c: float/str - confidence
    :param percents: bool - whether to output as a percents or not
    :param decimals: int - how many decimals to round to
    :return: str - converted to string
    """
    if isinstance(c, float):
        return f'{c * 100: .{decimals}f}%' if percents else f'{c: .{decimals}f}'
    return c


def generate_row(sequence: str, result: dict, postfilter: PostFilter) -> tuple:
    """
    Generate rows of a summary table in html report.
    :param sequence: str - motif sequence
    :param result: pd.Series - result row to convert to table
    :param postfilter: PostFilter - postfilter dict from config
    :return: str - html string with rows of the summary table
    """
    highlight = list(map(int, str(result['repetition_index']).split('_')))
    sequence, _subpart = highlight_subpart(sequence, highlight)

    # shorten sequence:
    keep = 10
    first = sequence.find(',')
    last = sequence.rfind(',')
    smaller_seq = sequence if first == -1 else '...' + sequence[first - keep:last + keep + 1] + '...'

    # errors:
    errors = f'{postfilter.max_rel_error * 100:.0f}%'
    if postfilter.max_abs_error is not None:
        errors += f' (abs={postfilter.max_abs_error})'

    # fill templates:
    updated_result = {
        'conf_allele1': float_to_str(result['conf_allele1'], percents=True),
        'conf_allele2': float_to_str(result['conf_allele2'], percents=True),
        'confidence': float_to_str(result['confidence'], percents=True),
        'motif_nomenclature': smaller_seq,
        'indels': float_to_str(result['indels'], decimals=2),
        'mismatches': float_to_str(result['mismatches'], decimals=2),
        'indels_a1': float_to_str(result['indels_a1'], decimals=2),
        'mismatches_a1': float_to_str(result['mismatches_a1'], decimals=2),
        'indels_a2': float_to_str(result['indels_a2'], decimals=2),
        'mismatches_a2': float_to_str(result['mismatches_a2'], decimals=2)
    }
    # return ROW_STRING.format(**{**result, **updated_result})
    motif_name = result['motif_name']
    motif_nomenclature = updated_result['motif_nomenclature']
    allele1 = result['allele1']
    conf_allele1 = updated_result['conf_allele1']
    reads_a1 = result['reads_a1']
    indels_a1 = updated_result['indels_a1']
    mismatches_a1 = updated_result['mismatches_a1']

    allele2 = result['allele2']
    conf_allele2 = updated_result['conf_allele2']
    reads_a2 = result['reads_a2']
    indels_a2 = updated_result['indels_a2']
    mismatches_a2 = updated_result['mismatches_a2']

    confidence = updated_result['confidence']
    indels = updated_result['indels']
    mismatches = updated_result['mismatches']
    quality_reads = result['quality_reads']
    one_primer_reads = result['one_primer_reads']

    row_tuple = (
        motif_name, motif_nomenclature,
        allele1, conf_allele1, reads_a1, indels_a1, mismatches_a1,
        allele2, conf_allele2, reads_a2, indels_a2, mismatches_a2,
        confidence, indels, mismatches, quality_reads, one_primer_reads
    )

    return row_tuple
    # return (result, updated_result)


def generate_motifb64(seq: str, row: dict) -> tuple:
    highlight = list(map(int, str(row['repetition_index']).split('_')))
    # print(f"{highlight=}") -> [1, 2]
    sequence, _ = highlight_subpart(seq, highlight)
    motif_name = row['motif_name']
    motif_name_part1 = f'{motif_name.replace("/", "_")}'
    motif_name_part2 = f'{",".join(map(str, highlight)) if highlight is not None else "mot"}'
    motif_name_long = f'{motif_name_part1}_{motif_name_part2}'
    motif_clean = re.sub(r'[^\w_]', '', motif_name_long)
    motif_id = motif_clean.rsplit('_', 1)[0]
    motif_clean_id = motif_id if highlight == [1] else motif_clean  # trick to solve static html
    # motif_clean_id sucks... and unfortunatelly it is used as module_id in json

    a1 = row['allele1']
    a2 = row['allele2']
    conf_total = float_to_str(row['confidence'], percents=True)
    conf_a1 = float_to_str(row['conf_allele1'], percents=True)
    conf_a2 = float_to_str(row['conf_allele2'], percents=True)
    if (a1 == 'B' and a2 == 'B') or (a1 == 0 and a2 == 0):
        result = f'BG {conf_total}'
    else:
        result = f'{str(a1):2s} ({conf_a1}) {str(a2):2s} ({conf_a2}) total {conf_total}'

    alignment = f"{motif_name}/alignments.html"

    return (motif_clean_id, highlight, motif_id, motif_name, result, alignment, sequence)


Confidences: TypeAlias = tuple[float, float, float, float, float, float, float]
GenotypeInfo: TypeAlias = tuple[
    int, list[Annotation], list[Annotation], list[Annotation], tuple[str | int, str | int],
    Confidences, np.ndarray[Any, Any] | None, Inference
]
Hist2DGraph: TypeAlias = tuple[list[list[int]], list[list[int]], list[list[str]], str, str]
HistReadCounts: TypeAlias = tuple[list[int], list[int], list[int]]
ProbHeatmap: TypeAlias = tuple[list[list[float | None]], list[list[str]], list[int], list[int | str], list[int], list[int | str], float]
GraphData: TypeAlias = tuple[HistReadCounts | None, ProbHeatmap | None, Hist2DGraph | None]


# %%
if __name__ == '__main__':
    main()
