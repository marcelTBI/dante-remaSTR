from __future__ import annotations

from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter, ArgumentTypeError
from datetime import datetime
from typing import TextIO, Iterable, Iterator, Any
from collections import Counter
from copy import copy

import csv
import gzip
import os
import re
import sys
import textwrap
import enum
import shutil
import math
import functools
import itertools

import numpy as np
import numpy.typing as npt
import pandas as pd

import plotly.graph_objects as go  # type: ignore
from scipy.stats import binom  # type: ignore
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt  # noqa
import matplotlib.patheffects as patheffects  # noqa

VERSION = "0.9.0"
MAX_REPETITIONS = 40
MOTIF_COLUMN_NAME = 'motif'
DANTE_DESCRIPTION = '''
    DANTE = Da Amazing NucleoTide Exposer (Remastered)
    --------------------------------------------------
Genotyping and reporting from annotated reads generated by the remaSTR
program. The reads are filtered, clustered, and a genotype is inferred for
each motif. If a verbose option is switched on, DANTE creates a directory
for each of the annotated motifs and a summary report in HTML (report.html).
Otherwise only a table with all genotypes, confidences, and supporting
information.
'''


def main() -> None:
    start_time = datetime.now()
    args = load_arguments()

    print('DANTE_remaSTR = "Da Amazing NucleoTide Exposer" (remastered)')
    print(f'DANTE_remaSTR Starting : {start_time:%Y-%m-%d %H:%M:%S}')

    data = []
    for motif_table in generate_groups(sys.stdin):
        result = process_group(args, motif_table)
        data.append(result)

    all_motifs, rl_df, input_len, filtered_len = consume_iterator(data)

    print(f'Kept {filtered_len:4d}/{input_len:4d} ({filtered_len / input_len * 100.0:5.1f}%) reads.')
    print(f'Writing outputs: {datetime.now():%Y-%m-%d %H:%M:%S}')

    out_file = args.output_dir + "/variants.tsv"
    rl_df.to_csv(out_file, sep='\t')
    write_vcf(rl_df, args.output_dir)

    if args.verbose:
        all_motifs = sorted(all_motifs)
        merge_all_profiles(args.output_dir, all_motifs, rl_df)

        post_filter = PostFilter(args)
        write_report(all_motifs, rl_df, post_filter, args.output_dir, args.nomenclatures)

    end_time = datetime.now()
    print(f'DANTE_remaSTR Stopping : {end_time:%Y-%m-%d %H:%M:%S}')
    print(f'Total time of run      : {end_time - start_time}')


def load_arguments() -> Namespace:
    """
    Loads and parses the arguments.
    :return: args - parsed arguments
    """
    parser = ArgumentParser(
        formatter_class=RawDescriptionHelpFormatter,
        description=textwrap.dedent(DANTE_DESCRIPTION)
    )

    # TODO: cancel Postfilter parameters
    postfilter = parser.add_argument_group('Post-filter')
    postfilter.add_argument(
        '--min-flank-len', '-fl', type=positive_int, default=3,
        help='Minimal flank length in bases. Default=3'
    )
    postfilter.add_argument(
        '--min-rep-len', '-rl', type=positive_int, default=3,
        help='Minimal repetition length in bases. Default=3'
    )
    postfilter.add_argument(
        '--min-rep-cnt', '-rc', type=positive_int, default=1,
        help='Minimal repetition count. Default=1'
    )
    postfilter.add_argument(
        '--max-abs-error', '-ea', type=positive_int, default=None,
        help='Maximal number of errors. Default=All'
    )
    postfilter.add_argument(
        '--max-rel-error', '-er', type=probability, default=1.0,
        help='Maximal ratio of errors to read length. Default=All'
    )

    options = parser.add_argument_group('Options')
    options.add_argument(
        '--nomenclatures', '-n', type=positive_int, default=5,
        help='Number of nomenclature strings to add to reports. Default=5'
    )
    options.add_argument(
        '--output-dir', '-o', type=str, default="dante_out",
        help='Output destination (directory). Default=./dante_out/'
    )
    options.add_argument(
        '--cutoff-alignments', type=positive_int, default=20,
        help='How many bases to keep beyond annotated part. Default=20'
    )
    options.add_argument(
        '--male', action='store_true',
        help='Indicate that the sample is male. Process motifs from chrX/chrY as mono-allelic.'
    )
    options.add_argument(
        '--verbose', '-v', action='store_true',
        help='Print all the outputs. Default is to print only the result table to stdout.'
    )

    debug = parser.add_argument_group('Debug')
    debug.add_argument(
        '--skip-quality-under', type=positive_int, default=0,
        help='Skip reads that have quality lower than this value. Default=0, so skip none.'
    )
    debug.add_argument(
        '--cut-quality-under', type=positive_int, default=0,
        help='Cut parts of reads that have quality lower than this value. Default=0, so cut nothing.'
    )

    args = parser.parse_args()

    return args


def positive_int(value: str) -> int:
    try:
        int_value = int(value)
    except ValueError:
        raise ArgumentTypeError(f'Value {value} is not integer') from None
    if int_value < 0:
        raise ArgumentTypeError(f'Value {value} is negative')
    return int_value


def positive_nonzero_int(value: str) -> int:
    int_value = positive_int(value)
    if int_value == 0:
        raise ArgumentTypeError(f'Value {value} cannot be 0')
    return int_value


def probability(value: str) -> float:
    try:
        float_value = float(value)
    except ValueError:
        raise ArgumentTypeError(f'Value {value} is not float') from None
    if 0 <= float_value <= 1:
        return float_value
    raise ArgumentTypeError(f'Value {value} is not in interval <0, 1>')


# --- garbage under this line -------------------------------------------------
# define base mapping to regex for nucleotide symbols
base_mapping = {
    'A': 'A',     'C': 'C',     'G': 'G', 'T': 'T',
    'R': '[GA]',  'Y': '[CT]',  'K': '[GT]', 'M': '[AC]', 'S': '[GC]', 'W': '[AT]',
    'D': '[GAT]', 'H': '[ACT]', 'V': '[GCA]',  # AGT missing?
    'N': '[ACTG]'
}


class Motif:
    """
    Class to represent DNA motifs.

    :ivar chrom: Chromosome name.
    :ivar start: Start position of the motif.
    :ivar end: End position of the motif.
    :ivar modules: A list of tuples containing sequence and repetition count.
    :ivar name: name of the Motif
    :ivar motif: motif nomenclature
    """

    def __init__(self, motif: str, name: str | None = None):
        """
        Initialize a Motif object.
        :param motif: The motif string in the format "chrom:start_end[A][B]..."
        :param name: optional name of the motif
        """
        # remove whitespace
        self.motif = motif.strip().replace(' ', '')
        self.name = (name if name is not None else self.motif).replace(':', '-').replace('.', '_').replace('/', '_')

        # extract prefix, first number, second number
        tmp = re.match(r'([^:]+):g\.(\d+)_(\d+)(.*)', self.motif)
        if tmp is None:
            raise ValueError(f"{self.motif} has incorrect format")
        self.chrom, start, end, remainder = tmp.groups()

        # extract sequence and repetition count
        self.modules = [(str(seq), int(num)) for seq, num in re.findall(r'([A-Z]+)\[(\d+)', remainder)]
        self.modules = [('left_flank', 1)] + self.modules + [('right_flank', 1)]

        # convert to ints
        self.start = int(start)
        self.end = int(end)

    def __getitem__(self, index: int) -> tuple[str, int]:
        """
        Returns module at given index.
        :param index: The index of the module to fetch.
        :return: The module at the given index.
        """
        return self.modules[index]

    def __str__(self) -> str:
        """
        Returns string representation of the Motif object.
        :return: String representation in the format "chrom:start_end[A][B]..."
        """
        return f'{self.chrom}:g.{self.start}_{self.end}' + self.modules_str(include_flanks=False)

    def __lt__(self, obj):
        """
        Less than for sorting purposes.
        :return: bool - if this object comes before the other
        """
        return self.name < obj.name

    def __eq__(self, obj):
        """
        Equal to for sorting purposes.
        :return: bool - if this object is equal to the other
        """
        return self.name == obj.name

    def modules_str(self, include_flanks: bool = False) -> str:
        """
        Returns string representation of modules
        :param include_flanks: bool - include flank modules?
        :return: String representation of modules
        """
        if include_flanks:
            return ''.join([f'{seq}[{num}]' for seq, num in self.modules])
        return ''.join([f'{seq}[{num}]' for seq, num in self.modules[1:-1]])

    def module_str(self, module_number: int) -> str:
        """
        Returns string representation of modules
        :param module_number: int - module number
        :return: String representation of modules
        """
        seq, num = self.modules[module_number]
        return f'{seq}[{num}]'

    def dir_name(self) -> str:
        """
        Returns possible directory name of the motif.
        :return: str - directory name for the motif
        """
        return self.name

    # TODO: remove me
    def get_repeating_modules(self) -> list[tuple[int, str, int]]:
        """
        Returns list of modules with more than one repetition.
        :return: List of tuples containing index, sequence, and repetition count.
        """
        return [(int(i), str(seq), int(num)) for i, (seq, num) in enumerate(self.modules) if num > 1]

    def get_location_subpart(self, index: int) -> tuple[int, int]:
        """
        Returns the chromosome location of a subpart of a motive
        :param index: int - index of a module
        :return: start and end location of the subpart
        """
        start = self.start
        for module in self.modules[1: index]:
            seq, rep = module
            start += len(seq) * rep

        return start, start + len(self.modules[index][0]) * self.modules[index][1]


class Annotation:
    """
    Encapsulate sequence of states from HMM and provide its readable representation and filters
    """

    def __init__(
        self, read_id: str, mate_order: int, read_seq: str, expected_seq: str,
        states: str, probability: float, motif: Motif
    ):
        """
        :param read_id: str - read ID
        :param read_seq: str - read sequence
        :param mate_order: int - mate order (0 - unpaired, 1 - left pair, 2 - right pair)
        :param expected_seq: str - expected sequence as in motif
        :param states: str - sequence of states (numbers of modules)
        :param probability: Probability of generating sequence by the most likely sequence of HMM states
        :param motif: Sequence of tuples (sequence, repeats) as specified by user
        """

        # Store arguments into instance variables
        self.read_id = read_id
        self.mate_order = mate_order
        self.ordered = mate_order > 0
        self.left_pair = mate_order == 1
        self.read_seq = read_seq
        self.expected_seq = expected_seq
        self.states = states
        self.probability = probability
        self.motif = motif
        self.n_modules = len(motif.modules)

        # Calculate insertion/deletion/mismatch string
        self.mismatches_string = self.__get_errors()

        # Calculate number of insertions, deletions and normal bases
        self.n_insertions = self.mismatches_string.count('I')
        self.n_deletions = self.mismatches_string.count('D')
        self.n_mismatches = self.mismatches_string.count('M')

        # Number of STR motif repetitions and sequences of modules
        self.module_bases = self.__get_bases_per_module()
        self.module_repetitions = self.__get_module_repetitions()
        self.module_sequences = self.__get_module_sequences()

        # get left flank length
        self.left_flank_len = self.__get_left_flank()

    def __str__(self) -> str:
        """
        Return the annotation.
        :return: str - annotation
        """
        return '\n'.join([f'{self.read_id} {str(self.module_bases)} {str(self.module_repetitions)}', self.read_seq,
                          self.expected_seq, self.states, self.mismatches_string])

    def __get_errors(self) -> str:
        """
        Count errors in annotation and the error line.
        :return: str - error line
        """
        err_line = []
        for exp, read in zip(self.expected_seq.upper(), self.read_seq.upper()):
            if exp == '-' or read in base_mapping.get(exp, ''):
                err_line.append('_')
            elif read == '_':
                err_line.append('D')
            elif exp == '_':
                err_line.append('I')
            else:
                err_line.append('M')

        return ''.join(err_line)

    def __get_bases_per_module(self) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of bases of input sequence that were generated by the module
        :return: Number of bases generated by each module
        """
        # Count the module states
        return tuple(self.states.count(chr(ord('0') + i)) for i in range(self.n_modules))

    def __get_left_flank(self) -> int:
        """
        Get length of a left flank.
        :return: int - number of bases of left flank before module '0' (usually module '0' is still left flank)
        """
        for i, state in enumerate(self.states):
            if state != '-':
                return i
        return len(self.states)

    def __get_module_repetitions(self) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of repetitions of module in annotation
        :return: Number of repetitions generated by each module
        """
        # Count the module states
        repetitions = self.__get_bases_per_module()

        # Divide by the module length where applicable
        # TODO: this is not right for grey ones, where only closed ones should be counted, so round is not right.
        return tuple(1 if reps == 1 and cnt > 0 else round(cnt / len(seq))
                     for (seq, reps), cnt in zip(self.motif.modules, repetitions))

    def __get_module_sequences(self) -> tuple[str, ...]:
        """
        List of sequences, each per module
        :return: list(str)
        """
        sequences = [''] * self.n_modules
        for i in range(self.n_modules):
            state_char = chr(ord('0') + i)
            first = self.states.find(state_char)
            if first > -1:
                last = self.states.rfind(state_char)
                sequences[i] = self.read_seq[first:last + 1]
        return tuple(sequences)

    def get_module_errors(self, module_num: int, overhang: int | None = None) -> tuple[int, int, int]:
        """
        Get the number of insertions and deletions or mismatches in a certain module.
        If overhang is specified, look at specified number of bases around the module as well.
        :param module_num: int - 0-based module number to count errors
        :param overhang: int - how long to look beyond module, if None, one length of STR module
        :return: int, int, int - number of insertions and deletions, mismatches, length of the interval
        """
        # get overhang as module length
        if overhang is None:
            seq, _ = self.motif.modules[module_num]
            overhang = len(seq)

        # define module character
        char_to_search = chr(ord('0') + module_num)

        # if the annotation does not have this module, return 0
        if char_to_search not in self.states:
            return 0, 0, 0

        # search for the annotation of the module
        start = max(0, self.states.find(char_to_search) - overhang)
        end = min(self.states.rfind(char_to_search) + overhang + 1, len(self.states))

        # count errors
        indels = self.mismatches_string[start:end].count('I') + self.mismatches_string[start:end].count('D')
        mismatches = self.mismatches_string[start:end].count('M')

        # return indels, mismatches, and length
        return indels, mismatches, end - start

    def info_value(self) -> tuple[int, int]:
        """
        Evaluate the info value of the Annotation.
        :return: int, int
        """
        return sum(self.module_repetitions), sum(self.module_bases)

    def info_value_str(self, index_str: int) -> tuple[int, int, int]:
        """
        Evaluate the info value of the Annotation at the STR location
        :param index_str: int - index of the STR
        :return: int, int, int
        """
        return self.primers(index_str), self.module_repetitions[index_str], self.module_bases[index_str]

    def has_less_errors(self, max_errors: float | int, relative=False) -> bool:
        """
        Check if this annotation has fewer errors than max_errors.
        Make it relative to the annotated length if relative is set.
        :param max_errors: int/float - number of max_errors (relative if relative is set)
        :param relative: bool - if the errors are relative to the annotated length
        :return: bool - True if the number of errors is less than allowed
        """
        errors = self.n_deletions + self.n_insertions + self.n_mismatches

        if max_errors is None or errors == 0:
            return True

        if relative:
            return errors / float(sum(self.module_bases)) <= max_errors
        return errors <= max_errors

    def primers(self, index_rep: int) -> int:
        """
        Count how any primers it has on repetition index.
        :param index_rep: int - index of the repetition, that we are looking at
        :return: int - number of primers (0-2)
        """
        primers = 0
        if index_rep > 0 and self.module_repetitions[index_rep - 1] > 0:
            primers += 1
        if index_rep + 1 < len(self.module_repetitions) and self.module_repetitions[index_rep + 1] > 0:
            primers += 1
        return primers

    def is_annotated_right(self) -> bool:
        """
        Is it annotated in a way that it is interesting?
        More than one module annotated + modules are not missing in the middle.
        :return: bool - annotated right?
        """

        # remove those that starts/ends in background but don't have a neighbour module
        starts_background = self.states[0] in '_-'
        ends_background = self.states[-1] in '_-'
        if starts_background and self.module_repetitions[0] == 0:
            return False
        if ends_background and self.module_repetitions[-1] == 0:
            return False

        # remove those with jumping modules
        started = False
        ended = False
        for repetition in self.module_repetitions:
            if repetition > 0:
                started = True
                if ended:
                    return False
            if repetition == 0 and started:
                ended = True

        # pass?
        return True

    def same_start_fragment(self, annotation: Annotation) -> bool:
        """
        Return True if both sequences can be produced from the same start of a fragment.
        :param annotation: Annotation - second annotation
        :return: bool - True if both sequences can be produced from the same start of a fragment
        """
        comp_length = min(len(self.read_seq), len(annotation.read_seq))
        return self.read_seq[:comp_length] == annotation.read_seq[:comp_length]

    def same_end_fragment(self, annotation: Annotation) -> bool:
        """
        Return True if both sequences can be produced from the same end of a fragment.
        :param annotation: Annotation | None - second annotation
        :return: bool - True if both sequences can be produced from the same end of a fragment
        """
        comp_length = min(len(self.read_seq), len(annotation.read_seq))
        return self.read_seq[-comp_length:] == annotation.read_seq[-comp_length:]

    def get_str_repetitions(self, index_str: int) -> tuple[bool, int] | None:
        """
        Get the number of str repetitions for a particular index.
        :param index_str: int - index of a str
        :return: (bool, int) - closed?, number of str repetitions
        """
        if self.is_annotated_right():
            primer1 = index_str > 0 and self.module_repetitions[index_str - 1] > 0
            primer2 = index_str + 1 < len(self.module_repetitions) and self.module_repetitions[index_str + 1] > 0
            if primer1 or primer2:
                return primer1 and primer2, self.module_repetitions[index_str]
        return None

    @staticmethod
    def find_with_regex(read_sequence: str, motif_sequence: str, search_pos: int = 0) -> int:
        """
        Find the first occurrence of a motif sequence in the read sequence using regular expressions.
        :param read_sequence: The sequence to search in.
        :param motif_sequence: The motif sequence (as a regex) to search for.
        :param search_pos: The position to start the search from.
        :return: int - The start position of the first occurrence of the motif sequence. Returns -1 if not found.
        """
        # convert motif sequence to regex
        motif_regex = ''.join(base_mapping[char] for char in motif_sequence)

        # compile the regular expression pattern
        pattern = re.compile(motif_regex)

        # search for the pattern in the read sequence starting from search_pos
        match = pattern.search(read_sequence, search_pos)

        # return the start position if a match is found, else return -1
        return match.start() if match else -1

    def get_nomenclature(
        self, index_rep: int | None = None, index_rep2: int | None = None, include_flanking: bool = True
    ) -> str:
        """
        Get HGVS nomenclature.
        :param index_rep: int - index of the first repetition (None if include all)
        :param index_rep2: int - index of the second repetition (None if include all)
        :param include_flanking: boolean - include flanking regions (i.e. first and last module)
        :return: str - HGVS nomenclature string
        """
        # prepare data
        if index_rep is not None:
            if index_rep2 is not None:
                data = zip(
                    [self.module_repetitions[index_rep], self.module_repetitions[index_rep2]],
                    [self.motif[index_rep], self.motif[index_rep2]],
                    [self.module_sequences[index_rep], self.module_sequences[index_rep2]]
                )
            else:
                data = zip(
                    [self.module_repetitions[index_rep]],
                    [self.motif[index_rep]],
                    [self.module_sequences[index_rep]]
                )
        elif include_flanking:
            data = zip(self.module_repetitions, self.motif.modules, self.module_sequences)
        else:
            data = zip(self.module_repetitions[1:-1], self.motif.modules[1:-1], self.module_sequences[1:-1])

        # iterate and build the nomenclature string
        nomenclatures = []
        for repetitions, (motif_sequence, _), read_sequence in data:
            nomenclature = ''
            if repetitions == 1:
                if len(read_sequence) > 0:
                    nomenclature += f'{read_sequence}[1]'
            else:
                reps = 0
                search_pos = 0
                found_rep_seq = ''
                while True:
                    search_found = self.find_with_regex(read_sequence, motif_sequence, search_pos)
                    if search_found == search_pos:
                        # setup current rep. sequence
                        if reps == 0:
                            found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]

                        if read_sequence[search_found:search_found + len(motif_sequence)] == found_rep_seq:
                            # regular continuation
                            reps += 1
                        else:
                            # interruption, but in line with searched motif
                            nomenclature += f'{found_rep_seq}[{reps}]'
                            found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                            reps = 1
                    elif search_found == -1:
                        # the end, we did not find any other STRs
                        if reps > 0:
                            nomenclature += f'{found_rep_seq}[{reps}]'
                        if len(read_sequence[search_pos:]) > 0:
                            nomenclature += f'{read_sequence[search_pos:]}[1]'
                        break
                    else:
                        # some interruption
                        if reps > 0:
                            nomenclature += f'{found_rep_seq}[{reps}]'
                        if len(read_sequence[search_pos:search_found]) > 0:
                            nomenclature += f'{read_sequence[search_pos:search_found]}[1]'
                        found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                        reps = 1
                    # update search pos and iterate
                    search_pos = search_found + len(motif_sequence)
            nomenclatures.append(nomenclature)

        return '\t'.join(nomenclatures)

    def get_shortened_annotation(self, shorten_length: int) -> Annotation:
        """
        Get shortened annotation with specified shorten length beyond annotated modules.
        :param shorten_length: int - how many bases to keep beyond modules
        :return: Annotation - shortened annotation
        """

        # search for start
        start = -1
        for i in range(len(self.states)):
            if self.states[i] != '-':
                start = i
                break

        # search for end
        end = -1
        for i in range(len(self.states) - 1, -1, -1):
            if self.states[i] != '-':
                end = i
                break

        # adjust start and end for shorten length
        start = max(start - shorten_length, 0)
        end = min(end + 1 + shorten_length, len(self.states))  # +1 for use as list range

        # return shortened Annotation
        return Annotation(self.read_id, self.mate_order, self.read_seq[start:end], self.expected_seq[start:end],
                          self.states[start:end], self.probability, self.motif)


def errors_per_read(
    errors: list[tuple[int, int, int]], relative: bool = False
) -> tuple[float | str, float | str]:
    """
    Count number of errors per read. Relative per length or absolute number.
    :param errors: list[tuple[int, int, int]] - indels, mismatches and length of module
    :param relative: bool - relative?
    :return: tuple[float, float] - number of indels, mismatches per hundred reads
    """
    # if we have no reads, return '---'
    if len(errors) == 0:
        return '---', '---'

    if relative:
        mean_length = np.mean([length for _, _, length in errors])
        return (
            float(np.mean([indels / float(length) for indels, _, length in errors]) * mean_length),
            float(np.mean([mismatches / float(length) for _, mismatches, length in errors]) * mean_length)
        )
    return (
        float(np.mean([indels for indels, _, _ in errors])),
        float(np.mean([mismatches for _, mismatches, _ in errors]))
    )


def generate_result_line(
    motif_class: Motif, predicted: tuple[str | int, str | int], confidence: tuple[float | str, ...],
    qual_num: int, primer_num: int, filt_num: int, module_number: int,
    qual_annot: list[Annotation] | None = None,
    second_module_number: int | None = None
) -> dict:
    """
    Generate result line from the template string.
    :param motif_class: Motif - motif class
    :param predicted: tuple[str, str] - predicted alleles (number or 'B'/'E')
    :param confidence: tuple[7x float/str] - confidences of prediction
    :param qual_num: int - number of reads with both primers
    :param primer_num: int - number of reads with exactly one primer
    :param filt_num: int - number of filtered out reads (no primers, many errors, ...)
    :param module_number: int - module number in motif
    :param qual_annot: list[Annotation] - list of quality annotations for error and number of reads
    :param second_module_number: int/None - second module number in motif
    :return: dict - result dictionary
    """
    # setup motif info
    start, end = motif_class.get_location_subpart(module_number)
    motif_seq = motif_class.module_str(module_number)
    if second_module_number is not None:
        _, end = motif_class.get_location_subpart(second_module_number)
        motif_seq = ','.join(
            [motif_class.module_str(i) for i in range(module_number, second_module_number + 1)]
        )

    reads_a1: int | str
    reads_a2: int | str
    indels_rel: float | str
    indels_rel1: float | str
    indels_rel2: float | str
    mismatches_rel: float | str
    mismatches_rel1: float | str
    mismatches_rel2: float | str
    # get info about errors and number of reads from quality annotations if provided
    reads_a1 = reads_a2 = '---'
    indels_rel = mismatches_rel = '---'
    indels_rel1 = mismatches_rel1 = '---'
    indels_rel2 = mismatches_rel2 = '---'
    if qual_annot is not None:
        # get info about number of reads
        a1 = int(predicted[0]) if isinstance(predicted[0], int) else None
        a2 = int(predicted[1]) if isinstance(predicted[1], int) else None
        reads_a1 = 0 if a1 is None else len(
            [a for a in qual_annot if a.module_repetitions[module_number] == a1]
        )
        reads_a2 = 0 if a2 is None else len(
            [a for a in qual_annot if a.module_repetitions[module_number] == a2]
        )

        # get info about errors
        errors = [a.get_module_errors(module_number) for a in qual_annot]
        errors_a1 = [a.get_module_errors(module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a1]
        errors_a2 = [a.get_module_errors(module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a2]
        assert len([l for i, m, l in errors if l == 0]) == 0

        # extract error metrics
        indels_rel, mismatches_rel = errors_per_read(errors, relative=True)
        indels_rel1, mismatches_rel1 = errors_per_read(errors_a1, relative=True)
        indels_rel2, mismatches_rel2 = errors_per_read(errors_a2, relative=True)

    # return dictionary
    return {
        'motif_name': motif_class.name, 'motif_nomenclature': motif_class.motif, 'motif_sequence': motif_seq,
        'chromosome': motif_class.chrom, 'start': start, 'end': end, 'allele1': predicted[0],
        'allele2': predicted[1], 'confidence': confidence[0], 'conf_allele1': confidence[1],
        'conf_allele2': confidence[2], 'reads_a1': reads_a1, 'reads_a2': reads_a2, 'indels': indels_rel,
        'mismatches': mismatches_rel, 'indels_a1': indels_rel1, 'indels_a2': indels_rel2,
        'mismatches_a1': mismatches_rel1, 'mismatches_a2': mismatches_rel2, 'quality_reads': qual_num,
        'one_primer_reads': primer_num, 'filtered_reads': filt_num,
        'conf_background': confidence[3] if len(confidence) > 3 else '---',
        'conf_background_all': confidence[4] if len(confidence) > 4 else '---',
        'conf_extended': confidence[5] if len(confidence) > 5 else '---',
        'conf_extended_all': confidence[6] if len(confidence) > 6 else '---',
        'repetition_index': module_number
        if second_module_number is None
        else f'{module_number}_{second_module_number}'
    }


# this has mixed functionality - it does prediction and it does writing
def too_complex_function_inferring_and_creating_reports(
    args: Namespace,
    annotations1: list[Annotation],
    postfilter_class: PostFilter,
    motif_class: Motif,
    motif_str: str,
    module_number: int,
    read_distribution: npt.NDArray[np.int64],
    result_lines: list,
    prev_module: tuple[int, str, int] | None,
    file_pcolor: str | None,
    file_output: str | None
):
    monoallelic1 = args.male and chrom_from_string(motif_class.chrom) in [
        ChromEnum.X, ChromEnum.Y
    ]

    # setup post filtering - no primers, insufficient quality, ...
    qual_annot1, filt_annot1 = postfilter_class.get_filtered(annotations1, module_number, both_primers=True)
    primer_annot1, filt_annot2 = postfilter_class.get_filtered(filt_annot1, module_number, both_primers=False)

    # why is this a class?
    # run inference - this takes most of the time (for no --verbose)
    model = Inference(
        read_distribution, None, str_rep=args.min_rep_cnt,
        minl_primer1=args.min_flank_len, minl_primer2=args.min_flank_len,
        minl_str=args.min_rep_len
    )

    predicted1, confidence1 = model.genotype(
        qual_annot1, primer_annot1, module_number, file_pcolor, file_output,
        motif_str, monoallelic1
    )

    # append to the result line
    result_lines.append(generate_result_line(
        motif_class, predicted1, confidence1,
        len(qual_annot1), len(primer_annot1), len(filt_annot1),
        module_number, qual_annot=qual_annot1
    ))

    # infer phasing (if we are not on the first repeating module)
    last_num1 = None
    both_good_annot1 = None
    one_good_annot1 = None
    none_good_annot1 = None
    phasing1 = None
    supp_reads1 = None
    if prev_module is not None:
        # get the last module number
        last_num1 = prev_module[0]

        # post filtering
        both_good_annot1, filtered_annot1 = postfilter_class.get_filtered_list(
            annotations1, [last_num1, module_number], both_primers=[True, True]
        )
        left_good_annot1, left_bad_annot1 = postfilter_class.get_filtered_list(
            filtered_annot1, [last_num1, module_number], both_primers=[False, True]
        )
        right_good_annot1, none_good_annot1 = postfilter_class.get_filtered_list(
            left_bad_annot1, [last_num1, module_number], both_primers=[True, False]
        )
        one_good_annot1 = left_good_annot1 + right_good_annot1

        # infer phasing
        phasing1, supp_reads1 = phase(both_good_annot1, last_num1, module_number)

        # append to the result line
        result_lines.append(generate_result_line(
            motif_class, phasing1, supp_reads1,
            len(both_good_annot1), len(one_good_annot1), len(none_good_annot1),
            last_num1, second_module_number=module_number
        ))

    return (
        (qual_annot1, primer_annot1, filt_annot2),
        (both_good_annot1, one_good_annot1, none_good_annot1),
        phasing1, supp_reads1, confidence1, predicted1, result_lines, last_num1
    )


def report(
    args, motif_dir, motif_class, module_number, last_num1, prev_module,
    ann_qual, ann_primer, ann_filter, ann_2good, ann_1good, ann_0good,
    phasing, supp_reads, confidence, predicted
):
    # TODO: merge meaning prev_module and last_num1

    # write files if needed
    write_all(
        ann_qual, ann_primer, ann_filter,
        motif_dir, motif_class, module_number,
        zip_it=False, cutoff_alignments=args.cutoff_alignments
    )

    if prev_module is not None:
        # write files
        write_all(
            ann_2good, ann_1good, ann_0good,
            motif_dir, motif_class, last_num1, second_module_number=module_number,
            zip_it=False, cutoff_alignments=args.cutoff_alignments
        )

        # write phasing into a file
        save_phasing(
            f'{motif_dir}/phasing_{last_num1}_{module_number}.txt', phasing, supp_reads
        )

    # write the alignments
    if confidence is not None:
        # get number of precise alignments for each allele
        a1 = int(predicted[0]) if isinstance(predicted[0], int) else None
        a2 = int(predicted[1]) if isinstance(predicted[1], int) else None

        if a1 is not None and a1 > 0:
            write_alignment(
                f'{motif_dir}/alignment_{module_number}_a{a1}.fasta',
                ann_qual, module_number, a1,
                zip_it=False, cutoff_after=args.cutoff_alignments
            )
        if a2 is not None and a2 != a1 and a2 != 0:
            write_alignment(
                f'{motif_dir}/alignment_{module_number}_a{a2}.fasta',
                ann_qual, module_number, a2,
                zip_it=False, cutoff_after=args.cutoff_alignments
            )


def process_group(
    args: Namespace, df: pd.DataFrame
) -> tuple[Motif, list[dict], int, int]:
    """
    Process the group as pandas Dataframe. Return motif name if processed correctly or None otherwise.
    :param args: argparse.Namespace - namespace of program arguments
    :param df: pd.DataFrame - contains information about annotated reads for a single motif to process
    :param motif_str: str - motif nomenclature
    :return: Motif, list(dict), int, int - motif, result lines, input length, length of filtered intput
    """
    motif_str = df[MOTIF_COLUMN_NAME].iloc[0]

    # build motif class
    name = None if 'name' not in df.columns or df.iloc[0]['name'] in ['None', ''] else df.iloc[0]['name']
    motif_class = Motif(motif_str, name)

    # filter/cut those with low quality
    input_len = len(df)
    filtered_len = len(df)
    if filtered_len == 0:
        # TODO: maybe it would be nice to warn user?
        return motif_class, [], input_len, filtered_len

    # https://github.com/pandas-dev/pandas-stubs/issues/1001
    # create annotations from rows
    annotations: list[Annotation] = []
    for _, row in df.iterrows():
        annotations.append(Annotation(
            row['read_id'], row['mate_order'], row['read'], row['reference'],
            row['modules'], row['log_likelihood'], motif_class
        ))

    # infer read distribution
    read_distribution = np.bincount([len(ann.read_seq) for ann in annotations], minlength=100)

    # create report for each repeating module
    result_lines: list[dict] = []
    repeating_modules = motif_class.get_repeating_modules()
    postfilter_class = PostFilter(args)
    for i, (module_number, _, _) in enumerate(repeating_modules):
        prev_module = None if i == 0 else repeating_modules[i - 1]
        file_pcolor, file_output = None, None
        if args.verbose:
            motif_dir = f'{args.output_dir}/{motif_class.dir_name()}'
            os.makedirs(motif_dir, exist_ok=True)
            file_pcolor = f'{motif_dir}/pcolor_{module_number}'
            file_output = f'{motif_dir}/allcall_{module_number}.txt'

        result = too_complex_function_inferring_and_creating_reports(
            module_number=module_number,
            postfilter_class=postfilter_class,
            motif_class=motif_class, read_distribution=read_distribution,
            motif_str=motif_str, result_lines=result_lines, prev_module=prev_module,
            args=args, file_pcolor=file_pcolor, file_output=file_output, annotations1=annotations
        )
        postfilter_counts, postfilter_counts_phasing, \
            phasing, supp_reads, confidence, predicted, result_lines, last_num1 = result

        if args.verbose:
            report(
                args, motif_dir, motif_class, module_number, last_num1, prev_module,
                postfilter_counts[0], postfilter_counts[1], postfilter_counts[2],
                postfilter_counts_phasing[0], postfilter_counts_phasing[1], postfilter_counts_phasing[2],
                phasing, supp_reads, confidence, predicted
            )

    if args.verbose:
        # generate nomenclatures for all modules:
        for i, (_seq, reps) in enumerate(motif_class.modules):
            # write files if needed
            if reps == 1:
                # setup post filtering - no primers, insufficient quality, ...
                qual_annot, _ = postfilter_class.get_filtered(annotations, i, both_primers=True)

                # gather and write nomenclatures
                if len(qual_annot) > 0:
                    write_histogram_nomenclature(
                        f'{motif_dir}/nomenclatures_{i}.txt', qual_annot, index_rep=i
                    )
        # try to get the overall nomenclature:
        for module_number, _, _ in repeating_modules:
            annotations, _ = postfilter_class.get_filtered(annotations, module_number, both_primers=True)
        write_histogram_nomenclature(f'{motif_dir}/nomenclature.txt', annotations)

    # return motif name in case it was processed normally
    return motif_class, result_lines, input_len, filtered_len


def generate_groups(input_stream: TextIO, chunk_size: int = 1000000) -> Iterator[pd.DataFrame]:
    """
    Generate sub-parts of the input table according to "column_name".
    Able to process even large files.
    :param input_stream: TextIO - input stream
    :param chunk_size: int - chunk size for table processing
    :return: Iterator[pd.DataFrame] - sub-parts of the input table
    """
    column_name = MOTIF_COLUMN_NAME
    # initialize reading
    current_group_data = pd.DataFrame()

    # read the output of remaSTR into annotations
    for chunk in pd.read_csv(input_stream, sep='\t', chunksize=chunk_size, iterator=True, quoting=csv.QUOTE_NONE):

        # identify the unique groups in the chunk
        unique_groups = chunk[column_name].unique()

        # got through the groups
        for group in unique_groups:

            # filter rows for the current group from the chunk
            group_data = chunk[chunk[column_name] == group]

            # if this group is a continuation of the previous group from the last chunk
            if not current_group_data.empty and current_group_data[column_name].iloc[0] == group:
                current_group_data = pd.concat([current_group_data, group_data])
                continue  # Move to the next group in the current chunk

            # if there's data in current_group_data, process and empty it
            if not current_group_data.empty:
                yield current_group_data
                current_group_data = pd.DataFrame()  # reset

            # check if the group is at end the chunk (probably continues into next one)
            if group == unique_groups[-1]:
                current_group_data = group_data
            else:
                # process the group as normally
                yield group_data

    # process the last group
    if not current_group_data.empty:
        yield current_group_data


def consume_iterator(
    results_iterator: Iterable[tuple[Motif, list[dict], int, int]]
) -> tuple[list[Motif], pd.DataFrame, int, int]:
    """
    Consume iterator of results.
    :param results_iterator: generator - motif and its corresponding results of modules
    :return: list[Motif], pd.DataFrame, int - motifs in list and table of all results, input length
    """
    # consume iterator of results
    all_motifs: list[Motif] = []
    all_result_lines: list[dict[Any, Any]] = []
    all_input_len = 0
    all_filtered_len = 0
    for motif, rls, input_l, filtered_l in results_iterator:
        if filtered_l > 0:
            # append data
            all_motifs.append(motif)
            all_result_lines.extend(rls)
            all_input_len += input_l
            all_filtered_len += filtered_l

    df = pd.DataFrame.from_records(all_result_lines)
    df.sort_values(by=['motif_name'], kind='stable')
    return (all_motifs, df, all_input_len, all_filtered_len)


def normalize_ref_alt(ref: str, alt: str) -> tuple[str, str]:
    suffix = 0
    it = zip(reversed(ref), reversed(alt))
    for _ in range(min(len(ref), len(alt)) - 1):
        (a, b) = next(it)
        if a == b:
            suffix += 1

    return (ref[:len(ref) - suffix], alt[:len(alt) - suffix])


def make_vcf_line(
    chrom: str, pos: str, unit: str, ref_copies: str, alt_copies: str, genotype: str,
    lines: list[str]
):
    if ref_copies == alt_copies:
        return  # they are the same, there is no variant
    if "|" in alt_copies:
        return  # skip over phased variants, TODO: later

    ref_seq = unit * int(ref_copies)
    if alt_copies == "B":
        info = f"REF={ref_copies};RU={unit};BG;END={pos}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq, "<BG>", ".", "PASS", info, "GT", genotype
        ))
    elif alt_copies == "E":
        info = f"REF={ref_copies};RU={unit};EXP;END={pos}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq[0], "<EXP>", ".", "PASS", info, "GT", genotype
        ))
    else:
        alt_seq = unit * int(alt_copies)

        svlen = len(alt_seq) - len(ref_seq)
        svtype = "INS" if svlen > 0 else "DEL"
        (ref_seq, alt_seq) = normalize_ref_alt(ref_seq, alt_seq)

        info = f"REF={ref_copies};RU={unit};SVLEN={svlen};SVTYPE={svtype}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq, alt_seq, ".", "PASS", info, "GT", genotype
        ))


def write_vcf(df: pd.DataFrame, out: str) -> None:
    lines = []
    lines.append('##fileformat=VCFv4.1\n')
    lines.append('##ALT=<ID=BG,Description="Background">\n')
    lines.append('##ALT=<ID=EXP,Description="Expansion of unknown (large) size">\n')
    lines.append('##FILTER=<ID=PASS,Description="All filters passed">\n')
    lines.append('##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">\n')
    lines.append('##INFO=<ID=END,Number=1,Type=Integer,Description="End position of the variant">\n')
    lines.append('##INFO=<ID=BG,Number=0,Type=Flag,Description="Background variant">\n')
    lines.append('##INFO=<ID=EXP,Number=0,Type=Flag,Description="Expansion variant">\n')
    lines.append('##INFO=<ID=REF,Number=1,Type=Integer,Description="Reference copy number">\n')
    lines.append('##INFO=<ID=RU,Number=1,Type=String,Description="Repeat unit in ref orientation">\n')
    lines.append('##INFO=<ID=SVLEN,Number=1,Type=Integer,Description="Alt length - Ref length">\n')
    lines.append('##INFO=<ID=SVTYPE,Number=1,Type=String,Description="Type of structural variant">\n')
    lines.append('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tsample\n')

    records: list[str] = []
    for _, row in df.iterrows():
        m1 = re.match(r"([A-Z]+)\[([0-9]+)\]", row["motif_sequence"])
        if m1 is None:
            print(f"{row['motif_sequence']} returned None")
            continue
        unit, copies = m1.groups()
        allele1 = str(row["allele1"])
        allele2 = str(row["allele2"])

        if allele1 == allele2:
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele1"]), "1/1", records)
        else:
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele1"]), "1/.", records)
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele2"]), "./1", records)

    records.sort(key=chr_and_pos)
    with open(f"{out}/variants.vcf", "w") as f:
        f.writelines(lines + records)


def chr_and_pos(line: str) -> tuple[int, int]:
    m1 = re.match(r"(chr[0-9XY]+)\t([0-9]+)\t.*", line)
    if m1 is None:
        raise ValueError(f"got {line}")
    chrom, pos = m1.groups()
    chrom2: int = {
        "chr1":  1,  "chr2":  2,  "chr3":  3,  "chr4":  4,  "chr5": 5,   "chr6": 6,
        "chr7":  7,  "chr8":  8,  "chr9":  9,  "chr10": 10, "chr11": 11, "chr12": 12,
        "chr13": 13, "chr14": 14, "chr15": 15, "chr16": 16, "chr17": 17, "chr18": 18,
        "chr19": 19, "chr20": 20, "chr21": 21, "chr22": 22, "chrX": 23,  "chrY": 24
    }[chrom]
    return (chrom2, int(pos))


class PostFilter:
    """
    Class that encapsulates post-filtering.
    """

    def __init__(self, args: Namespace):
        """
        Initialize post-filter class.
        :param args: arguments of the program
        """
        self.min_flank_len = args.min_flank_len
        self.min_rep_len = args.min_rep_len
        self.min_rep_cnt = args.min_rep_cnt
        self.max_rel_error = args.max_rel_error
        self.max_abs_error = args.max_abs_error

    def quality_annotation(self, ann: Annotation, module_number: int, both_primers: bool = True) -> bool:
        """
        Is this annotation good?
        :param ann: Annotation - annotation to be evaluated
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: bool - quality annotation?
        """
        is_right = ann.is_annotated_right()

        primers = ann.primers(module_number)
        has_primers = primers == 2 if both_primers else primers >= 1

        has_less_errors = (
            ann.has_less_errors(self.max_rel_error, relative=True)
            and ann.has_less_errors(self.max_abs_error, relative=False)
        )

        left_flank = sum(ann.module_bases[module_number + 1:]) >= self.min_flank_len
        right_flank = sum(ann.module_bases[:module_number]) >= self.min_flank_len
        has_flanks = left_flank and right_flank if both_primers else left_flank or right_flank

        has_repetitions = (
            ann.module_bases[module_number] >= self.min_rep_len
            and ann.module_repetitions[module_number] >= self.min_rep_cnt
        )

        _seq, reps = ann.motif.modules[module_number]

        return is_right and has_primers and has_less_errors and has_flanks and (has_repetitions or reps == 1)

    def get_filtered_list(self, annotations: list[Annotation], module_number: list[int],
                          both_primers: list[bool] | None = None) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations (list of modules).
        :param annotations: list(Annotation) - annotations
        :param module_number: list(int) - module numbers
        :param both_primers: list(bool) or None - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # adjust input if needed
        if both_primers is None:
            both_primers = [True] * len(module_number)
        assert len(both_primers) == len(module_number)

        # filter annotations
        quality_annotations = [
            an for an in annotations
            if all((
                self.quality_annotation(an, mn, both_primers=bp) for mn, bp in zip(module_number, both_primers)
            ))
        ]
        filtered_annotations = [an for an in annotations if an not in quality_annotations]

        return quality_annotations, filtered_annotations

    def get_filtered(
        self, annotations: list[Annotation], module_number: int,
        both_primers: bool = True
    ) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations.
        :param annotations: list(Annotation) - annotations
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # pick quality annotations
        quality_annotations = []
        filtered_annotations = []
        for an in annotations:
            if self.quality_annotation(an, module_number, both_primers):
                quality_annotations.append(an)
            else:
                filtered_annotations.append(an)

        # quality_annotations = [an for an in annotations if self.quality_annotation(
        #    an, module_number, both_primers=both_primers)]
        # filtered_annotations = [an for an in annotations if an not in quality_annotations]

        return quality_annotations, filtered_annotations


def has_good_quality(row: dict, min_qual: int, first_module: int, last_module: int) -> bool:
    """
    Checks if the read has good quality everywhere in the annotated part.
    :param row: dict - the read, its sequence, data, quality?
    :param min_qual: int - minimal quality to have
    :param first_module: int - first module of the annotation to look at
    :param last_module: int - last module of the annotation to look at
    :return: bool - True if the read has enough quality bases in the annotated part
    """
    # first adjust the lengths due to insertions
    indices_to_except = set([i for i, ltr in enumerate(row['read']) if ltr == '_'])
    seq = ''.join(c for i, c in enumerate(row['read']) if i not in indices_to_except)
    ref = ''.join(c for i, c in enumerate(row['reference']) if i not in indices_to_except)
    mod = ''.join(c for i, c in enumerate(row['modules']) if i not in indices_to_except)
    qual = [ord(q) - ord('!') for q in row['quality']]

    assert len(seq) == len(ref) == len(mod) == len(qual), (
        row['motif'], row['read_sn'], len(seq), len(qual), row['read'], seq, row['quality']
    )

    # identify annotated place
    annot_start = mod.find(chr(ord('0') + first_module))
    if annot_start == -1:
        if mod[0] != '-' and ord(mod[0]) - ord('0') <= last_module:
            annot_start = 0
        else:
            return True
    annot_end = mod.rfind(chr(ord('0') + last_module))
    annot_end = len(mod) if annot_end == -1 else annot_end + 1

    # look if there is enough quality in the annotated part
    return all(q >= min_qual for q in qual[annot_start:annot_end])


def cut_low_quality(row: dict, min_qual: int) -> dict:
    """
    Cut parts of the read with low quality
    :param row: dict - the read, its sequence, data, quality?
    :param min_qual: int - minimal quality to have
    :return: dict - row with updated sequences/qualities
    """
    # first adjust the quality column due to insertions
    indices_ins = [i for i, ltr in enumerate(row['read']) if ltr == '_']
    result = []
    last_idx = 0
    for idx in sorted(indices_ins):
        result.append(row['quality'][last_idx:idx])  # Append substring up to current index
        result.append('X')  # Insert 'X' (high quality)
        last_idx = idx  # Update last index to current
    result.append(row['quality'][last_idx:])  # Append the remaining part of the string
    qual = [ord(q) - ord('!') for q in ''.join(result)]

    assert len(row['read']) == len(qual), (row['motif'], row['read_sn'], len(qual), row['read'], row['quality'])

    # cut low quality from relevant parts of row
    keep_ids = np.array(qual) >= min_qual
    row['read'] = ''.join(np.array(list(row['read']))[keep_ids])
    row['reference'] = ''.join(np.array(list(row['reference']))[keep_ids])
    row['modules'] = ''.join(np.array(list(row['modules']))[keep_ids])
    row['quality'] = ''.join([chr(q + ord('!')) for q in np.array(qual)[keep_ids]])

    # return the modified row
    return row


class ChromEnum(enum.Enum):
    X = 'X'
    Y = 'Y'
    NORM = 'NORM'


def chrom_from_string(chrom_str: str) -> ChromEnum:
    """
    Converts a string to a ChromEnum object.
    :param chrom_str: str - the string to convert to a ChromEnum object
    :return ChromEnum - enum object representing the chromosome
    """
    return (
        ChromEnum.X if chrom_str in ['chrX', 'NC_000023'] else
        ChromEnum.Y if chrom_str in ['chrY', 'NC_000024'] else
        ChromEnum.NORM
    )


def write_annotations(out_file: str, annotations: list[Annotation], zip_it: bool = True) -> None:
    """
    Stores annotations in alignment format into output text file
    :param out_file: Alignment file
    :param annotations: Annotated reads
    :param zip_it: bool - whether to gzip the resulting file
    """
    if zip_it and not out_file.endswith('.gz'):
        out_file += '.gz'
    with gzip.open(out_file, 'wt') if zip_it else open(out_file, 'w') as fw:
        for annot in annotations:
            fw.write(str(annot) + '\n')


def write_alignment(
    out_file: str, annotations: list[Annotation], index_rep: int, allele: int | None = None,
    index_rep2: int | None = None, allele2: int | None = None, zip_it: bool = True,
    cutoff_after: int | None = None, right_align: bool = False
) -> None:
    # TODO this needs complete rework
    """
    Creates a multi-alignment of all annotations into output text file
    :param out_file: str - alignment filename
    :param annotations: list(Annotation) - annotated reads
    :param index_rep: int - index of repetition module of a motif
    :param allele: int/None - which allele to print only, if None print all of them
    :param index_rep2: int - index of second repetition module of a motif
    :param allele2: int/None - which allele2 to print only, if None print all of them
    :param zip_it: bool - whether to gzip the resulting file
    :param right_align: bool - whether we deal with right alignment file
    :param cutoff_after: int - how many bases to keep outside the annotated motif (None for keep all)
    """
    # select annotations
    if allele is not None:
        if allele2 is not None and index_rep2 is not None:
            annotations = [
                a for a in annotations
                if a.module_repetitions[index_rep] == allele and a.module_repetitions[index_rep2] == allele2
            ]
        else:
            annotations = [a for a in annotations if a.module_repetitions[index_rep] == allele]

    # apply cutoff
    if cutoff_after is not None:
        annotations = [a.get_shortened_annotation(cutoff_after) for a in annotations]

    # setup alignments
    alignments = [''] * len(annotations)  # alignment strings
    align_inds = np.zeros(len(annotations), dtype=int)  # indices of annotations that were processed
    states = []  # has numbers of states in the final multiple alignment

    while True:
        # get minimal state:
        min_comp = (True, 'Z', -1)
        total_done = 0
        for i, (annot, ai) in enumerate(zip(annotations, align_inds)):
            if ai >= len(annot.states):
                total_done += 1
                continue
            state = annot.states[ai]
            comparator = (state != 'I', state, i)
            min_comp = min(comparator, min_comp)

        # if we have done every state, end:
        if total_done >= len(alignments):
            break

        states.append(min_comp[1])

        # now print all states, that are minimal:
        for i, (annot, ai) in enumerate(zip(annotations, align_inds)):
            if ai >= len(annot.states):
                alignments[i] += '_'  # put ends of each alignment to be of same length
                continue
            if annot.states[ai] == min_comp[1]:
                alignments[i] += annot.read_seq[ai]
                align_inds[i] += 1
            else:
                alignments[i] += '_'

    # sort according to motif count:
    left_flank = np.array([-(ann.module_bases[0] + ann.left_flank_len) for ann in annotations])
    left_flank_exist = np.array([-(ann.module_repetitions[0]) if not right_align else 0 for ann in annotations])
    if index_rep2 is not None:
        # sorting first with 1st allele then with second
        reps1 = np.array([-ann.module_bases[index_rep] for ann in annotations])
        reps2 = np.array([-ann.module_bases[index_rep2] for ann in annotations])
        # sort by existence of left flank, first allele, second, left flank len.
        sort_inds = np.lexsort((left_flank, reps2, reps1, left_flank_exist))
    else:
        reps = np.array([-ann.module_bases[index_rep] for ann in annotations])
        sort_inds = np.lexsort((left_flank, reps, left_flank_exist))
    annotations = np.array(annotations)[sort_inds]
    alignments = list(np.array(alignments)[sort_inds])

    def move_right(alignment: str, start: int, end: int) -> str:
        """
        Shift first part of the alignment to the right.
        :param alignment: str - alignment of the read
        :param start: int - start idx for shift
        :param end: int - one after end idx for a shift
        :return: str - alignment, where first part is shifted to right
        """
        align_part = alignment[start:end]
        # find last empty:
        idx = 0
        for idx in reversed(range(len(align_part))):
            if align_part[idx] != '_':
                break
        idx += 1

        # return shifted alignment
        return alignment[:start] + ('_' * (len(align_part) - idx)) + align_part[:idx] + alignment[end:]

    def get_range(symbol: str = '0') -> tuple[int, int]:
        """
        Get range of a module
        :param symbol: str - symbol for state to get range for
        :return: int, int - start and (one after) end range coordinates
        """
        try:
            first_idx = states.index(symbol)
            last_idx = len(states) - states[-1::-1].index(symbol) - 1
            return first_idx, last_idx + 1
        except ValueError:
            return -1, -1

    def get_left_flank() -> int:
        """
        Get range of a left flank.
        :return: int - (one after) nd of the left flank before module '0'
        """
        for i, state in enumerate(states):
            if state != '-':
                return i
        return -1

    # for every alignment, shift the left flank right
    end = get_left_flank()
    if end != -1:
        for i in range(len(alignments)):
            alignments[i] = move_right(alignments[i], 0, end)

    # for every alignment, shift the first module right
    start0, end0 = get_range('0')
    if start0 != -1:
        for i in range(len(alignments)):
            alignments[i] = move_right(alignments[i], start0, end0)

    # in addition, those that have only '_' in state '0' (missing left flank), shift right also '1' state
    start1, end1 = get_range('1')
    first_zero_idx = len(alignments)
    for i in range(len(alignments)):
        if start1 != -1 and (start0 == -1 or alignments[i][start0:end0].count('_') == end0 - start0 or right_align):
            first_zero_idx = min(first_zero_idx, i)
            alignments[i] = move_right(alignments[i], start1, end1)

    # add empty line if we have some alignments without left flank
    annot_names = [annot.read_id for annot in annotations]
    if first_zero_idx != len(alignments) and not right_align:
        alignments = alignments[:first_zero_idx] + ['_' * len(alignments[0])] + alignments[first_zero_idx:]
        annot_names = annot_names[:first_zero_idx] + ['empty_line'] + annot_names[first_zero_idx:]

    # print to file
    if zip_it and not out_file.endswith('.gz'):
        out_file += '.gz'
    with gzip.open(out_file, 'wt') if zip_it else open(out_file, 'w') as fw:
        # print alignments
        for annot_name, align in zip(annot_names, alignments):
            print(f'>{annot_name}', file=fw)
            print(align, file=fw)


def sorted_repetitions(annotations: list[Annotation]) -> list[tuple[tuple[int, ...], int]]:
    """
    Aggregate same repetition counts for annotations and sort them according to quantity of repetitions of each module
    :param annotations: Annotated reads
    :return: list of (repetitions, count), sorted by repetitions
    """
    count_dict = Counter(tuple(annot.module_repetitions) for annot in annotations)
    return sorted(count_dict.items(), key=lambda k: k[0])


def write_histogram(out_file: str, annotations: list[Annotation], ) -> None:
    """
    Stores quantity of different combinations of module repetitions into text file
    :param out_file: str - output file for repetitions
    :param annotations: Annotated reads
    """
    # setup
    sorted_reps = sorted_repetitions(annotations)

    # write repetitions.txt
    with open(out_file, 'w') as fw:
        for repetitions, counts in sorted_reps:
            rep_code = '\t'.join(map(str, repetitions))
            fw.write(f'{counts}\t{rep_code}\n')


def write_profile(profile_file: str, annotations: list[Annotation], index_rep: int) -> None:
    """
    Stores quantity of different combinations of module repetitions into text file
    :param profile_file: str - output file for repetitions
    :param annotations: Annotated reads
    :param index_rep: int - index of the first repetition
    """
    # setup
    sorted_reps = sorted_repetitions(annotations)

    # write profile
    length = max([0] + [x[0][index_rep] for x in sorted_reps])
    profile = np.zeros(length + 1, dtype=int)

    for repetitions, counts in sorted_reps:
        profile[repetitions[index_rep]] += counts

    with open(profile_file, 'w') as f:
        f.write('\t'.join(map(str, profile)))


def write_histogram_nomenclature(
    out_file: str, annotations: list[Annotation],
    index_rep: int | None = None, index_rep2: int | None = None
) -> None:
    """
    Stores quantity of different nomenclature strings into text file
    :param out_file: str - output file for repetitions
    :param annotations: Annotated reads
    :param index_rep: int - index of the first repetition (None if include all)
    :param index_rep2: int - index of the second repetition (None if include all)
    """
    # count nomenclature strings:
    count_dict = Counter(annot.get_nomenclature(index_rep, index_rep2, False) for annot in annotations)
    count_dict2 = sorted(count_dict.items(), key=lambda k: (-k[1], k[0]))

    # write nomenclatures to file
    with open(out_file, 'w') as fw:
        for nomenclature, count in count_dict2:
            fw.write(f'{count}\t{nomenclature}\n')


def write_histogram_image2d(
    out_prefix: str, deduplicated: list[Annotation], index_rep: int, index_rep2: int, seq: str, seq2: str
) -> None:
    """
    Stores quantity of different combinations of module repetitions,
    generates separate graph image for each module
    :param out_prefix: Output file prefix
    :param deduplicated: list[Annotation] - read pairs
    :param index_rep: int - index of repetition module of a motif
    :param index_rep2: int - index of the second repetition module of a motif
    :param seq: str - module of the repetition
    :param seq2: str - 2nd module of the repetition
    """
    if deduplicated is None or len(deduplicated) == 0:
        return

    dedup_reps: list[tuple[tuple[bool, int], tuple[bool, int]]] = []
    for x in deduplicated:
        r_1 = x.get_str_repetitions(index_rep)
        r_2 = x.get_str_repetitions(index_rep2)
        if r_1 is not None and r_2 is not None:
            dedup_reps.append((r_1, r_2))

    if len(dedup_reps) == 0:
        return

    # assign maximals
    xm = max(r for (_, r), _ in dedup_reps)
    ym = max(r for _, (_, r) in dedup_reps)
    max_ticks = max(ym, xm) + 2  # TODO: why does max_ticks exist?
    xm = max(MAX_REPETITIONS, xm)
    ym = max(MAX_REPETITIONS, ym)

    # create data containers
    data = np.zeros((xm + 1, ym + 1), dtype=int)
    data_primer = np.zeros((xm + 1, ym + 1), dtype=int)
    for ((c1, r1), (c2, r2)) in dedup_reps:
        if c1 and c2:
            data[r1, r2] += 1
        if c1 and not c2:
            data_primer[r1, r2:] += 1
        if not c1 and c2:
            data_primer[r1:, r2] += 1

    str1 = 'STR %d [%s]' % (index_rep + 1, seq.split('-')[-1])
    str2 = 'STR %d [%s]' % (index_rep2 + 1, seq2.split('-')[-1])

    # plot_histogram_image2d_matplotlib(out_prefix, data, data_primer, str1, str2, max_ticks)
    plot_histogram_image2d_plotly(out_prefix, data, data_primer, str1, str2, max_ticks)


def plot_histogram_image2d_matplotlib(
    out_prefix: str, data: np.ndarray, data_primer: np.ndarray,
    str1: str, str2: str, max_ticks: int,
) -> None:
    cmblue = matplotlib.colormaps['Blues']
    cmap_blue = cmblue(np.arange(int(cmblue.N * 0.15), int(cmblue.N * 0.8)))  # start from light blue to deep blue
    cmap_blue[0, -1] = 0.0  # Set alpha on the lowest element only

    cmgrey = matplotlib.colormaps['Greys']
    cmap_grey = cmgrey(np.arange(int(cmgrey.N * 0.15), int(cmgrey.N * 0.6)))  # start from light grey to deep grey
    cmap_grey[0, -1] = 0.0  # Set alpha on the lowest element only

    # plot pcolor
    plt.figure(figsize=(12, 8))
    img2 = plt.pcolor(
        data_primer[:max_ticks, :max_ticks], cmap=matplotlib.colors.ListedColormap(cmap_grey), alpha=0.4,
        edgecolor=(1.0, 1.0, 1.0, 0.0), lw=0, vmin=np.min(data_primer), vmax=np.max(data_primer) + 0.01
    )
    img1 = plt.pcolor(
        data[:max_ticks, :max_ticks], cmap=matplotlib.colors.ListedColormap(cmap_blue),
        vmin=np.min(data), vmax=np.max(data) + 0.01
    )
    plt.xticks()
    plt.ylabel(str1)
    plt.xlabel(str2)
    plt.colorbar(img1)
    plt.colorbar(img2)

    # setup ticks
    start_ticks = 5
    step_ticks = 5
    plt.xticks(
        np.array(range(start_ticks, max_ticks + 1, step_ticks)) + 0.5,
        [str(x) for x in range(start_ticks, max_ticks + 1, step_ticks)]
    )
    plt.yticks(
        np.array(range(start_ticks, max_ticks + 1, step_ticks)) + 0.5,
        [str(x) for x in range(start_ticks, max_ticks + 1, step_ticks)]
    )

    # output it
    plt.savefig(out_prefix + '.pdf')
    plt.savefig(out_prefix + '.png')
    plt.close()


def plot_histogram_image2d_plotly(
    out_prefix: str, data: np.ndarray, data_primer: np.ndarray,
    str1: str, str2: str, max_ticks: int,
) -> None:
    def parse_labels(num, num_primer):
        if num == 0 and num_primer == 0:
            return ''
        if num == 0 and num_primer != 0:
            return '0/%s' % str(num_primer)
        if num != 0 and num_primer == 0:
            return '%s/0' % str(num)
        return '%s/%s' % (str(num), str(num_primer))

    text = [[parse_labels(data[i, j], data_primer[i, j]) for j in range(data.shape[1])] for i in range(data.shape[0])]

    fig = go.Figure()

    cmgrey = matplotlib.colormaps['Greys']
    cmap_grey = cmgrey(np.arange(int(cmgrey.N * 0.15), int(cmgrey.N * 0.6)))  # start from light grey to deep grey
    cmap_grey[0, -1] = 0.0  # Set alpha on the lowest element only
    cmap_grey_plotly = [(i, f'rgba({c[0]}, {c[1]}, {c[2]}, {c[3]})') for i, c in [
        (0.0, cmap_grey[0]), (0.01, cmap_grey[1]), (1.0, cmap_grey[-1])
    ]]
    if np.sum(data_primer[:max_ticks, :max_ticks]) > 0:
        fig.add_trace(go.Heatmap(
            z=data_primer[:max_ticks, :max_ticks], name='Repetitions heatmap',
            showscale=True, colorbar_x=1.3, colorbar_title='Partial reads', colorscale=cmap_grey_plotly
        ))

    cmblue = matplotlib.colormaps['Blues']
    cmap_blue = cmblue(np.arange(int(cmblue.N * 0.15), int(cmblue.N * 0.8)))  # start from light blue to deep blue
    cmap_blue[0, -1] = 0.0  # Set alpha on the lowest element only
    cmap_blue_plotly = [(i, f'rgba({c[0]}, {c[1]}, {c[2]}, {c[3]})') for i, c in [
        (0.0, cmap_blue[0]), (0.01, cmap_blue[1]), (1.0, cmap_blue[-1])
    ]]
    fig.add_trace(go.Heatmap(
        z=data[:max_ticks, :max_ticks], text=text, name='Repetitions heatmap',
        showscale=True, colorbar_title='Full reads', colorscale=cmap_blue_plotly
    ))

    fig.update_traces(
        texttemplate='%{text}', textfont_size=7,
        hovertemplate='<b>{name1}:\t%{y}<br>{name2}:\t%{x}</b><br>Full / Partial:\t%{text}'.
        format(name1=str1, y='{y}', name2=str2, x='{x}', text='{text}')
    )
    fig.update_layout(width=800, height=600, template='simple_white')
    fig.update_yaxes(title_text=str1)
    fig.update_xaxes(title_text=str2)

    with open(out_prefix + '.json', 'w') as f:
        f.write(fig.to_json())

    # fig.write_image(out_prefix + '_plotly.pdf')


def write_histogram_image(
    out_prefix: str, annotations: list[Annotation], filt_annot: list[Annotation], index_rep: int
) -> None:
    """
    Stores quantity of different combinations of module repetitions, generates separate graph image for each module
    :param out_prefix: Output file prefix
    :param annotations: Annotated reads.
    :param filt_annot: Annotated reads (filtered)
    :param index_rep: int - index of repetition module of a motif
    """
    if len(annotations) == 0 and len(filt_annot) == 0:
        return

    repetitions = sorted_repetitions(annotations)
    repetitions_filt = sorted_repetitions(filt_annot)

    spanning_counts = [(r[index_rep], c) for r, c in repetitions]
    filtered_counts = [(r[index_rep], c) for r, c in repetitions_filt]
    inread_counts: list[tuple] = []

    xm = max(
        [r for r, c in spanning_counts]
        + [r for r, c in filtered_counts]
        + [r for r, c in inread_counts]
        + [MAX_REPETITIONS]
    )

    # set data
    dist = [0] * (xm + 1)
    for r, c in spanning_counts:
        dist[r] += c

    dist_filt = dist.copy()
    for r, c in filtered_counts:
        dist_filt[r] += c

    dist_inread = dist_filt.copy()
    for r, c in inread_counts:
        dist_inread[r] += c

    # print(dist, dist_filt, dist_inread, sep="\n", end="\n\n")
    # plot_histogram_image_matplotlib(out_prefix, dist, dist_filt, dist_inread, xm)
    plot_histogram_image_plotly(out_prefix, dist, dist_filt, dist_inread)


def plot_histogram_image_matplotlib(
    out_prefix: str, spanning: list[int], flanking: list[int], inread: list[int], xm: int
) -> None:
    width = 0.9
    plt.figure(figsize=(20, 8))
    # create barplots
    rects_inread = plt.bar(np.arange(xm + 1), inread, width, color='orange', alpha=0.4)
    rects_filt = plt.bar(np.arange(xm + 1), flanking, width, color='lightgrey')
    rects = plt.bar(np.arange(xm + 1), spanning, width)
    plt.xticks(np.arange(1, xm + 1))
    plt.ylabel('Counts')
    plt.xlabel('STR repetitions')
    _, max_y = plt.ylim()
    plt.xlim((0, xm + 1))

    # label numbers
    for rect, rect_filt, rect_inread in zip(rects, rects_filt, rects_inread):
        if rect.get_height() > 0:
            plt.text(
                rect.get_x() + rect.get_width() / 2.,
                rect.get_height() + max_y / 100.0,
                '%d' % int(rect.get_height()), ha='center', va='bottom'
            )
        if rect_filt.get_height() != rect.get_height():
            plt.text(rect_filt.get_x() + rect_filt.get_width() / 2., rect_filt.get_height() + max_y / 100.0,
                     '%d' % int(rect_filt.get_height() - rect.get_height()), ha='center', va='bottom', color='grey')
        if rect_inread is not None and rect_inread.get_height() != rect_filt.get_height():
            plt.text(rect_inread.get_x() + rect_inread.get_width() / 2., rect_inread.get_height() + max_y / 100.0,
                     '%d' % int(rect_inread.get_height() - rect_filt.get_height()),
                     ha='center', va='bottom', color='orange')

    # output it
    plt.savefig(out_prefix + '.pdf')
    plt.savefig(out_prefix + '.png')
    plt.close()


def plot_histogram_image_plotly(
    out_prefix: str, spanning: list[int], flanking: list[int], inread: list[int]
) -> None:
    dist_text = ['' if d == 0 else str(d) for d in spanning]
    dist_filt_text = ['' if df - d == 0 else str(df - d) for df, d in zip(flanking, spanning)]
    dist_inread_text = ['' if di - df == 0 else str(di - df) for di, df in zip(inread, flanking)]

    fig = go.Figure()
    fig.add_bar(y=inread, text=dist_inread_text, name='Inread reads', marker_color='#FF6600', textfont_color='#FF6600')
    fig.add_bar(y=flanking, text=dist_filt_text, name='Partial reads', marker_color='#CCCCCC', textfont_color='#CCCCCC')
    fig.add_bar(y=spanning, text=dist_text, name='Full reads', marker_color='#636EFA', textfont_color='#636EFA')

    fig.update_traces(textposition='outside', texttemplate='%{text}', hovertemplate='%{text}', textfont_size=7)
    fig.update_layout(
        width=800, height=450, title='Histogram of repetitions', hovermode='x', yaxis_fixedrange=True,
        template='simple_white', barmode='overlay'
    )
    fig.update_yaxes(title_text='Read counts')
    fig.update_xaxes(
        title_text='STR repetitions', tickmode='array', tickvals=list(range(5, len(spanning), 5)),
        ticktext=list(range(5, len(spanning), 5))
    )
    with open(out_prefix + '.json', 'w', encoding='utf-8') as f:
        f.write(fig.to_json())

    # fig.write_image(out_prefix + '_plotly.pdf')


def write_all(
    quality_annotations: list[Annotation], filt_primer: list[Annotation], filtered_annotations: list[Annotation],
    motif_dir: str, motif_class: Motif, module_number: int, cutoff_alignments: int,
    second_module_number: int | None = None, zip_it: bool = True
) -> None:
    """
    Write all output files: quality annotations, one-primer annotations, filtered annotations, statistics,
    repetitions + images.
    :param quality_annotations: list(Annotation) - list of blue annotations
    :param filt_primer: list(Annotation) - list of grey annotations
    :param filtered_annotations: list(Annotation) - list of filtered out annotation
    :param motif_dir: str - path to motif directory
    :param motif_class: Motif - motif class
    :param module_number: int - index of first studied repetition in modules
    :param second_module_number: int - index of second studied repetition in modules (optional)
    :param zip_it: bool - whether to gzip the resulting file
    :param cutoff_alignments: int - how many bases to keep beyond annotated modules in alignments
    :return: None
    """
    # create dir if not exists
    os.makedirs(motif_dir, exist_ok=True)

    # create suffix for files:
    suffix = str(module_number) if second_module_number is None else f'{module_number}_{second_module_number}'

    # write output files
    write_annotations(f'{motif_dir}/annotations_{suffix}.txt', quality_annotations, zip_it=zip_it)
    if len(quality_annotations) and max(len(a.read_seq) for a in quality_annotations) > 200:
        write_annotations(
            f'{motif_dir}/annotations_{suffix}_short.txt',
            [a.get_shortened_annotation(cutoff_alignments) for a in quality_annotations],
            zip_it=zip_it
        )
    write_annotations(f'{motif_dir}/filtered_{suffix}.txt', filtered_annotations, zip_it=zip_it)
    if len(filtered_annotations) and max(len(a.read_seq) for a in filtered_annotations) > 200:
        write_annotations(
            f'{motif_dir}/filtered_{suffix}_short.txt',
            [a.get_shortened_annotation(cutoff_alignments) for a in filtered_annotations],
            zip_it=zip_it
        )
    write_annotations(f'{motif_dir}/filtered_primer_{suffix}.txt', filt_primer, zip_it=zip_it)
    if len(filt_primer) and max(len(a.read_seq) for a in filt_primer) > 200:
        write_annotations(
            f'{motif_dir}/filtered_primer_{suffix}_short.txt',
            [a.get_shortened_annotation(cutoff_alignments) for a in filt_primer],
            zip_it=zip_it
        )
    write_alignment(f'{motif_dir}/alignment_{suffix}.fasta', quality_annotations, module_number,
                    index_rep2=second_module_number, zip_it=zip_it, cutoff_after=cutoff_alignments)
    write_alignment(f'{motif_dir}/alignment_filtered_{suffix}.fasta', filt_primer, module_number,
                    index_rep2=second_module_number, zip_it=zip_it, cutoff_after=cutoff_alignments)
    write_alignment(f'{motif_dir}/alignment_filtered_left_{suffix}.fasta',
                    [a for a in filt_primer if a.module_bases[0] > 0], module_number,
                    index_rep2=second_module_number, zip_it=zip_it, cutoff_after=cutoff_alignments)
    write_alignment(f'{motif_dir}/alignment_filtered_right_{suffix}.fasta',
                    [a for a in filt_primer if a.module_bases[-1] > 0], module_number,
                    index_rep2=second_module_number, zip_it=zip_it, cutoff_after=cutoff_alignments, right_align=True)

    # write histogram image
    if second_module_number is not None:
        write_histogram_image2d(
            f'{motif_dir}/repetitions_{suffix}', quality_annotations + filt_primer, module_number, second_module_number,
            motif_class.module_str(module_number), motif_class.module_str(second_module_number)
        )
    else:
        write_histogram_image(f'{motif_dir}/repetitions_{suffix}', quality_annotations, filt_primer, module_number)
        write_profile(f'{motif_dir}/profile_{suffix}.txt', quality_annotations, index_rep=module_number)

    # write histogram txt files
    write_histogram(f'{motif_dir}/repetitions_{suffix}.txt', quality_annotations)
    write_histogram_nomenclature(
        f'{motif_dir}/nomenclatures_{suffix}.txt', quality_annotations, index_rep=module_number,
        index_rep2=second_module_number
    )
    write_histogram(f'{motif_dir}/repetitions_grey_{suffix}.txt', filt_primer)
    write_histogram_nomenclature(
        f'{motif_dir}/nomenclatures_grey_{suffix}.txt', filt_primer, index_rep=module_number,
        index_rep2=second_module_number
    )


def custom_format(template: str, **kwargs) -> str:
    """
    Custom format of strings for only those that we provide
    :param template: str - string to format
    :param kwargs: dict - dictionary of strings to replace
    :return: str - formatted string
    """
    for k, v in kwargs.items():
        template = template.replace('{%s}' % k, v)

    return template


def find_file(filename: str, include_gzip: bool = False) -> str | None:
    """
    Find if we have a file with the provided filename.
    :param filename: str - file name
    :param include_gzip: bool - try the gzipped suffix
    :return: str/None - filename if exists, None if not
    """
    if os.path.exists(filename):
        return filename
    gzipped = filename + '.gz'
    if include_gzip and os.path.exists(gzipped):
        return gzipped
    return None


def generate_nomenclatures(filename: str, motif: Motif, nomenclature_limit: int) -> list[str]:
    """
    Generate nomenclature string lines from nomenclature file. Maximally generate nomenclature_limit lines.
    :param filename: str - file name of the nomenclature file
    :param motif: Motif - motif class
    :param nomenclature_limit: int - limit how many nomenclatures to generate
    :return: list[str] - array of nomenclature lines
    """
    if not os.path.exists(filename):
        return []

    with open(filename, 'r') as noms:
        lines = []
        for line in noms:
            if line == '' or line is None:
                break

            line_split = line.split('\t')
            motif_parts = [f'<td>{s}</td>' for s in line_split[1:]]
            ref = f'{motif.chrom}:g.{motif.start}_{motif.end}'
            nom_row = NOMENCLATURE_STRING.format(count=line_split[0] + 'x', ref=ref, parts='\n    '.join(motif_parts))
            lines.append(nom_row)

            # end?
            if len(lines) >= nomenclature_limit:
                break

    return lines


def merge_all_profiles(report_dir: str, motifs: list[Motif], result_table: pd.DataFrame):
    all_profiles = f'{report_dir}/all_profiles.txt'
    all_true = f'{report_dir}/all_profiles.true'

    with open(all_profiles, 'w') as pf, open(all_true, 'w') as tf:
        for motif in motifs:
            for _, result in result_table[result_table['motif_name'] == motif.name].iterrows():

                # adjust helper variables
                phasing = '_' in str(result['repetition_index'])
                suffix = result['repetition_index']
                dir_name = f'{report_dir}/{motif.dir_name()}'

                if not phasing:
                    # add to profiles
                    with open(f'{dir_name}/profile_{suffix}.txt') as po:
                        line = po.readline()
                        pf.write(f'{motif.name}_{suffix}\t{line}\n')

                    # add to true
                    tf.write(f'{motif.name}_{suffix}\t{result["allele1"]}\t{result["allele2"]}\n')


def write_report(
    motifs: list[Motif], result_table: pd.DataFrame, post_filter: PostFilter, report_dir: str, nomenclature_limit: int
) -> None:
    """
    Generate and write a report.
    :param motifs - list of motifs to write results
    :param result_table - table of results
    :param post_filter - post-filter arguments
    :param report_dir - dir name for reports
    :param nomenclature_limit - number of lines from nomenclature.txt to print
    :return: None
    """
    alignments: dict[str, tuple] = {}

    mcs: dict[str, str] = {}  # first table in html, one value, one row
    ms: dict[str, list[str]] = {}  # graphs, starts at data
    rows: dict[str, list[str]] = {}  # table with confidences

    for motif in motifs:
        seq = motif.modules_str(include_flanks=True)
        for _, result in result_table[result_table['motif_name'] == motif.name].iterrows():

            # adjust helper variables
            suffix = result['repetition_index']
            dir_name = f'{report_dir}/{motif.dir_name()}'

            # read files
            rep_file = find_file(f'{dir_name}/repetitions_{suffix}.json')
            pcol_file = find_file(f'{dir_name}/pcolor_{suffix}.json')
            align_file = find_file(f'{dir_name}/alignment_{suffix}.fasta')
            filt_align_file = find_file(f'{dir_name}/alignment_filtered_{suffix}.fasta')
            filt_left_file = find_file(f'{dir_name}/alignment_filtered_left_{suffix}.fasta')
            filt_right_file = find_file(f'{dir_name}/alignment_filtered_right_{suffix}.fasta')

            nomenclature_lines = generate_nomenclatures(
                f'{dir_name}/nomenclatures_{suffix}.txt', motif, nomenclature_limit
            )

            # generate rows of tables and images
            row = generate_row(seq, result, post_filter)
            rows[motif.name] = rows.get(motif.name, []) + [row]

            # add the tables
            mc, m, a = generate_motifb64(
                seq, result,
                rep_file, pcol_file, align_file, filt_align_file, filt_left_file, filt_right_file,
                nomenclature_lines, post_filter
            )
            if motif.name in mcs:
                ms[motif.name].append(m)
                alignments[motif.dir_name()][1].append(a[1])
            else:
                mcs[motif.name] = mc
                ms[motif.name] = [m]
                alignments[motif.dir_name()] = (a[0], [a[1]])

    script_dir = os.path.dirname(os.path.abspath(__file__)) + "/dante_remastr_standalone_templates"
    template_file = f'{script_dir}/report.html'
    generate_report_html(
        template_file, report_dir, motifs, nomenclature_limit,
        rows, mcs, ms
    )

    # write alignments as html files
    for motif_dir_name in alignments.keys():
        # generate_html_alignment()
        template_alignments = open(f'{script_dir}/alignments.html', 'r').read()
        template_alignments = custom_format(
            template_alignments, sample=motif_dir_name, motif_desc=alignments[motif_dir_name][0]
        )
        template_alignments = custom_format(template_alignments, alignments='\n'.join(alignments[motif_dir_name][1]))

        with open(f'{report_dir}/{motif_dir_name}/alignments.html', 'w') as f:
            f.write(template_alignments)

    # copy javascript and css files
    shutil.copy2(f'{script_dir}/msa.min.gz.js', f'{report_dir}/msa.min.gz.js')
    shutil.copy2(f'{script_dir}/plotly-2.14.0.min.js', f'{report_dir}/plotly-2.14.0.min.js')
    shutil.copy2(f'{script_dir}/jquery-3.6.1.min.js', f'{report_dir}/jquery-3.6.1.min.js')
    shutil.copy2(f'{script_dir}/datatables.min.js', f'{report_dir}/datatables.min.js')
    shutil.copy2(f'{script_dir}/styles.css', f'{report_dir}/styles.css')


def generate_report_html(
    template_file: str, data_dir: str, motifs: list[Motif], nomenclature_limit: int,
    rows: dict[str, list[str]], mcs: dict[str, str], ms: dict[str, list[str]],
) -> None:
    template = open(template_file, 'r').read()
    sample = os.path.basename(data_dir)
    contents_table = CONTENTS.format(table='\n'.join(sorted(mcs.values())))
    motifs_content = contents_table + '\n' + MAKE_DATATABLE_STRING

    tabs = []
    for motif in sorted(motifs, key=lambda x: x.name):
        motif_clean = re.sub(r'[^\w_]', '', motif.name.replace('/', '_'))
        nomenclature_file = f'{data_dir}/{motif.dir_name()}/nomenclature.txt'
        nomenclature_lines = generate_nomenclatures(nomenclature_file, motif, nomenclature_limit)
        tabs.append(MOTIF_SUMMARY.format(
            motif_id=motif_clean, nomenclatures='\n'.join(nomenclature_lines), table='\n'.join(rows[motif.name]), motifs='\n'.join(ms[motif.name])
        ))
    motifs_tab = '\n'.join(tabs)

    template = custom_format(template, sample=sample, version=VERSION)
    template = custom_format(template, motifs_content=motifs_content)
    template = custom_format(template, table='', motifs=motifs_tab)

    with open(f'{data_dir}/report.html', 'w') as f:
        f.write(template)


def combine_distribs(deletes, inserts):
    """
    Combine insert and delete models/distributions
    :param deletes: ndarray - delete distribution
    :param inserts: ndarray - insert distribution
    :return: ndarray - combined array of the same length
    """
    # how much to fill?
    to_fill = sum(deletes == 0.0) + 1
    while to_fill < len(inserts) and inserts[to_fill] > 0.0001:
        to_fill += 1

    # create the end array
    end_distr = np.zeros_like(deletes, dtype=float)

    # fill it!
    for i, a in enumerate(inserts[:to_fill]):
        end_distr[i:] += (deletes * a)[:len(deletes) - i]

    return end_distr


def const_rate(_n, p1=0.0, _p2=1.0, _p3=1.0):
    return p1


def linear_rate(n, p1=0.0, p2=1.0, _p3=1.0):
    return p1 + p2 * n


def quadratic_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * n + p3 * n * n


def exp_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * math.exp(p3 * n)


def clip(value, minimal, maximal):
    """
    Clips value to range <minimal, maximal>
    :param value: ? - value
    :param minimal: ? - minimal value
    :param maximal: ? - maximal value
    :return: ? - clipped value
    """
    return min(max(minimal, value), maximal)


def model_full(rng, model_params, n, rate_func=linear_rate):
    """
    Create binomial model for both deletes and inserts of STRs
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param n: int - target allele number
    :param rate_func: function - rate function for deletes
    :return: ndarray - combined distribution
    """
    p1, p2, p3, q = model_params
    deletes = binom.pmf(np.arange(rng), n, clip(1 - rate_func(n, p1, p2, p3), 0.0, 1.0))
    inserts = binom.pmf(np.arange(rng), n, q)
    return combine_distribs(deletes, inserts)


def model_template(rng, model_params, rate_func=linear_rate):
    """
    Partial function for model creation.
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param rate_func: function - rate function for deletes
    :return: partial function with only 1 parameter - n - target allele number
    """
    return functools.partial(model_full, rng, model_params, rate_func=rate_func)


def generate_models(
    min_rep: int, max_rep: int, multiple_backgrounds: bool = True
) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :param multiple_backgrounds: bool - whether to generate all background states
    :return: generator of allele pairs (numbers or 'E' or 'B')
    """
    for model_index1 in range(min_rep, max_rep):
        for model_index2 in range(model_index1, max_rep):
            yield model_index1, model_index2
        yield model_index1, 'E'
        if multiple_backgrounds:
            yield 'B', model_index1

    yield 'B', 'B'
    yield 'E', 'E'


def generate_models_one_allele(min_rep: int, max_rep: int) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :return: generator of allele pairs (numbers or 'E' or 'B'), 'X' for non-existing allele
    """
    for model_index1 in range(min_rep, max_rep):
        yield model_index1, 'X'

    yield 'B', 'X'
    yield 'E', 'X'


class Inference:
    """ Class for inference of alleles. """

    MIN_REPETITIONS = 1

    # default parameters for inference (miSeq default)
    DEFAULT_MODEL_PARAMS = (0.00716322, 0.000105087, 0.0210812, 0.0001648)
    DEFAULT_FIT_FUNCTION = 'linear'

    def __init__(
        self,
        read_distribution, params_file, str_rep=3, minl_primer1=5, minl_primer2=5, minl_str=5,
        p_bckg_closed=None, p_bckg_open=None, p_expanded=None
    ):
        """
        Initialization of the Inference class + setup of all models and their probabilities.
        :param read_distribution: ndarray(int) - read distribution
        :param params_file: str - filename of parameters, None for defaults
        :param str_rep: int - length of the STR
        :param minl_primer1: int - minimal length of the left primer
        :param minl_primer2: int - minimal length of the right primer
        :param minl_str: int - minimal length of the STR
        :param p_bckg_closed: float - probability of the background model for closed observation
        :param p_bckg_open: float - probability of the background model for open observation
        :param p_expanded: float - probability of the expanded model (if None it is equal to other models)
        """
        # assign variables
        self.str_rep = str_rep
        self.minl_primer1 = minl_primer1
        self.minl_primer2 = minl_primer2
        self.minl_str = minl_str
        self.read_distribution = read_distribution
        self.sum_reads = np.sum(read_distribution)
        self.params_file = params_file
        self.p_expanded = p_expanded
        self.p_bckg_closed = p_bckg_closed
        self.p_bckg_open = p_bckg_open

    def construct_models(self, min_rep, max_rep, e_model):
        """
        Construct all models needed for current inference.
        :param min_rep: int - minimal allele to model
        :param max_rep: int - maximal allele to model
        :param e_model: int - model for expanded alleles
        :return: None
        """
        # extract params
        model_params, rate_func_str = self.read_params(self.params_file)
        str_to_func = {'linear': linear_rate, 'const': const_rate, 'exponential': exp_rate, 'square': quadratic_rate}
        rate_func = const_rate
        if rate_func_str in str_to_func.keys():
            rate_func = str_to_func[rate_func_str]

        # save min_rep and max_rep
        self.min_rep = min_rep
        self.max_rep = max_rep  # non-inclusive
        self.max_with_e = e_model + 1  # non-inclusive

        # get models
        mt = model_template(self.max_with_e, model_params, rate_func)
        self.background_model = np.concatenate([
            np.zeros(self.min_rep, dtype=float),
            np.ones(self.max_with_e - self.min_rep, dtype=float) / float(self.max_with_e - self.min_rep)
        ])
        self.expanded_model = mt(self.max_with_e - 1)
        self.allele_models = {i: mt(i) for i in range(min_rep, max_rep)}
        self.models = {'E': self.expanded_model, 'B': self.background_model}
        self.models.update(self.allele_models)

        # get model likelihoods
        open_to_closed = 10.0

        l_others = 1.0
        l_bckg_open = 0.01
        l_exp = 1.01

        l_bckg_model_open = 1.0

        if self.p_expanded is None:
            self.p_expanded = l_exp
        if self.p_bckg_open is None and self.p_bckg_closed is None:
            self.p_bckg_open = l_bckg_open
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_closed is None:
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_open is None:
            self.p_bckg_open = self.p_bckg_closed * open_to_closed

        self.model_probabilities = {'E': self.p_expanded, 'B': l_bckg_model_open}
        self.model_probabilities.update({i: l_others for i in self.allele_models.keys()})

    def read_params(self, params_file):
        """
        Reads all parameters written with write_params(print_all=True)
        :param params_file: str - filename to read parameters from, if None, load default params
        :return: 4-tuple, 2-tuple, function - parameters for model,
        read count drop, and error function for model distributions
        """
        if params_file is None:
            return self.DEFAULT_MODEL_PARAMS, self.DEFAULT_FIT_FUNCTION

        # read 2nd and last line of the file
        with open(params_file) as f:
            lines = f.readlines()
            fit_function = lines[1].strip().split()[1]
            split = list(map(float, lines[-1].strip().split()))

        if len(split) < 4:
            print('ERROR: parameters were not read successfully, using defaults!', file=sys.stderr)
            return self.DEFAULT_MODEL_PARAMS, self.DEFAULT_FIT_FUNCTION

        # extract parameters from last line of file
        model_params = tuple(split[0:4])

        return model_params, fit_function

    def likelihood_rl(self, rl):
        """
        Likelihood of a read with this length.
        :param rl: int - read length
        :return: float - likelihood of a read this long
        """
        # print('rl', self.read_distribution[rl] / float(self.sum_reads))
        return self.read_distribution[rl] / float(self.sum_reads)

    @staticmethod
    def likelihood_model(model, g):
        """
        Likelihood of a generated allele al from a model of
        :param model: ndarray - model that we evaluate
        :param g: int - observed read count
        :return: float - likelihood of a read coming from this model
        """
        return model[g]

    def likelihood_coverage(self, true_length, rl, closed=True):
        """
        Likelihood of generating a read with this length and this allele.
        :param true_length: int - true number of repetitions of an STR
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of a read being generated with this attributes
        """
        whole_inside_str = max(0, true_length * self.str_rep + self.minl_primer1 + self.minl_primer2 - rl + 1)
        # closed_overlapping = max(0, rl - self.minl_primer1 - self.minl_primer2 - true_length * self.str_rep + 1)
        open_overlapping = max(0, rl + true_length * self.str_rep - 2 * self.minl_str + 1)

        assert open_overlapping > whole_inside_str, '%d open %d whole inside %d %d %d' % (
            open_overlapping, whole_inside_str, true_length, rl, self.minl_str)

        return 1.0 / float(open_overlapping - whole_inside_str)

    def likelihood_read_allele(self, model, observed, rl, closed=True):
        """
        Likelihood of generation of read with observed allele count and rl.
        :param model: ndarray - model for the allele
        :param observed: int - observed allele count
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return:
        """
        if closed:
            return (self.likelihood_rl(rl)
                    * self.likelihood_model(model, observed)
                    * self.likelihood_coverage(observed, rl, True))
        number_of_options = 0
        partial_likelihood = 0
        for true_length in itertools.chain(range(observed, self.max_rep), [self.max_with_e - 1]):
            partial_likelihood += (self.likelihood_model(model, true_length)
                                   * self.likelihood_coverage(true_length, rl, False))
            number_of_options += 1

        return self.likelihood_rl(rl) * partial_likelihood / float(number_of_options)

    @functools.lru_cache()
    def likelihood_read(
        self, observed: int, rl: int, model_index1: int, model_index2: int | None = None, closed: bool = True
    ) -> float:
        """
        Compute likelihood of generation of a read from either of those models.
        :param observed: int - observed allele count
        :param rl: int - read length
        :param model_index1: char/int - model index for left allele
        :param model_index2: char/int - model index for right allele or None if mono-allelic
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of this read generation
        """
        # TODO: tuto podla mna nemoze byt len tak +, chyba tam korelacia modelov, ale v ramci zjednodusenia asi ok
        allele1_likelihood = (self.model_probabilities[model_index1]
                              * self.likelihood_read_allele(self.models[model_index1], observed, rl, closed))
        allele2_likelihood = 0.0 if model_index2 is None else (
            self.model_probabilities[model_index2]
            * self.likelihood_read_allele(self.models[model_index2], observed, rl, closed))
        p_bckg = self.p_bckg_closed if closed else self.p_bckg_open
        bckgrnd_likelihood = p_bckg * self.likelihood_read_allele(self.models['B'], observed, rl, closed)

        # alleles_intersection = min(
        #   model_prob_j, model_prob_i)
        #   * self.likelihood_read_intersection(model_i, model_j, observed, rl, closed)
        # if alleles_intersection > 0.0:
        #    print('%g %g %g %s %s %d' % (alleles_intersection, allele2_likelihood, allele1_likelihood,
        #    str(model_index1), str(model_index2), observed))

        assert not np.isnan(allele2_likelihood)
        assert not np.isnan(allele1_likelihood)
        assert not np.isnan(bckgrnd_likelihood)
        # assert alleles_intersection <= max(allele1_likelihood, allele2_likelihood), '%g %g %g %s %s %d' % (
        #    alleles_intersection, allele2_likelihood, allele1_likelihood, str(model_index1), str(model_index2),
        #    observed)

        # print('read_%s' % (str(closed)), observed, 'all1_lh', allele1_likelihood, 'all2_lh', allele2_likelihood)

        return allele1_likelihood + allele2_likelihood + bckgrnd_likelihood  # - alleles_intersection

    def infer(
        self, annotations: list[Annotation], filt_annotations: list[Annotation],
        index_rep: int, monoallelic: bool = False
    ) -> dict[tuple[int | str, int | str], float]:
        """
        Does all the inference,
        computes for which 2 combination of alleles are these annotations and parameters the best.
        argmax_{G1, G2} P(G1, G2 | AL, COV, RL)
            ~ P(AL, COV, RL | G1, G2) * P(G1, G2)
            = prod_{read_i} P(al_i, cov_i, rl_i | G1, G2) * P(G1, G2)
            = independent G1 G2
            = prod_{read_i} P(al_i, cov_i, rl_i | G1) * P(al_i, cov_i, rl_i | G2) * P(G1) * P(G2)
            {here G1, G2 is from possible alleles, background, and expanded, priors are from params}

         P(al_i, cov_i, rl_i | G1) - 2 options:
             1. closed evidence (al_i = X), we know X;
             2. open evidence (al_i >= X), cl_i == True if i is closed

         1.: P(al_i, cov_i, rl_i, cl_i | G1)
            = P(rl_i from read distrib.) * p(allele is al_i | G1) * P(read generated closed evidence | rl_i, al_i)
         2.: P(rl_i is from r.distr.) * P(allele is >= al_i | G1) * P(read generated open evidence | rl_i, al_i)

        :param annotations: list(Annotation) - closed annotated reads (both primers set)
        :param filt_annotations: list(Annotation) - open annotated reads (only one primer set)
        :param index_rep: int - index of a repetition
        :param verbose: bool - print more stuff?
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: dict(tuple(int, int):float) - directory of model indices to their likelihood
        """
        # generate closed observed and read_length arrays
        observed_annots = np.array([ann.module_repetitions[index_rep] for ann in annotations])
        rl_annots = np.array([len(ann.read_seq) for ann in annotations])
        closed_annots = np.ones_like(observed_annots, dtype=bool)

        # generate open observed and read_length arrays
        observed_fa = np.array([ann.module_repetitions[index_rep] for ann in filt_annotations])
        rl_fa = np.array([len(ann.read_seq) for ann in filt_annotations])
        closed_fa = np.zeros_like(observed_fa, dtype=bool)

        # join them and keep the information if they are open or closed
        observed_arr = np.concatenate((observed_annots, observed_fa)).astype(int)
        rl_arr = np.concatenate((rl_annots, rl_fa)).astype(int)
        closed_arr = np.concatenate([closed_annots, closed_fa]).astype(bool)

        # generate the boundaries:
        overhead = 3
        if len(observed_annots) == 0:
            max_rep = max(observed_fa) + overhead  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, max(observed_fa) - overhead)  # inclusive
        else:
            max_rep = max(observed_annots) + overhead + 1  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, min(observed_annots) - overhead)  # inclusive

        # expanded allele
        e_allele = max_rep
        if len(observed_fa) > 0:
            e_allele = max(max_rep, max(observed_fa) + 1)

        # generate all the models
        self.construct_models(min_rep, max_rep, e_allele)

        # go through every model and evaluate:
        evaluated_models = {}
        if monoallelic:
            models = generate_models_one_allele(min_rep, max_rep)
        else:
            models = generate_models(min_rep, max_rep, multiple_backgrounds=True)
        # for m1, m2 in generate_models_one_allele(min_rep, max_rep)
        # if monoallelic else generate_models(min_rep, max_rep, multiple_backgrounds=True):
        for m1, m2 in models:
            evaluated_models[(m1, m2)] = 0.0
            # go through every read
            for obs, rl, closed in zip(observed_arr, rl_arr, closed_arr):
                lh = self.likelihood_read(obs, rl, m1, None if m2 == 'X' else m2, closed=closed)
                # TODO weighted sum according to the closeness/openness of reads?
                evaluated_models[(m1, m2)] += np.log(lh)

        return evaluated_models

    def save_pcolor_file(
        self, display_file: str,
        lh_view: np.ndarray, z_min: float, z_max: float,
        title: str, max_str: int, start_ticks: int = 5, step_ticks: int = 5
    ):
        # background (B,B)
        bg_size = max(2, (len(lh_view) - self.min_rep) // 6)
        if len(lh_view) - self.min_rep <= 6:
            bg_size = 1
        lh_view[-bg_size:, self.min_rep:self.min_rep + bg_size] = lh_view[0, 0]
        # expanded (E,E)
        lh_view[-bg_size:, self.min_rep + bg_size:self.min_rep + 2 * bg_size] = lh_view[0, self.max_rep]

        # plotting
        plt.figure()
        plt.title(title)
        plt.xlabel('2nd allele')
        plt.ylabel('1st allele')
        plt.xticks(
            np.concatenate([
                np.array(range(start_ticks - self.min_rep, max_str - self.min_rep, step_ticks)),
                [max_str - self.min_rep]
            ]) + 0.5,
            [str(x) for x in range(start_ticks, max_str, step_ticks)] + ['E(>%d)' % (self.max_with_e - 2)]
        )
        plt.yticks(
            np.concatenate([
                np.array(range(start_ticks - self.min_rep + 1, max_str - self.min_rep + 1, step_ticks)), [0]
            ]) + 0.5,
            [str(x) for x in range(start_ticks, max_str, step_ticks)] + ['B'])

        # palette = copy(plt.cm.jet)
        palette = copy(plt.colormaps["jet"])
        palette.set_under('gray', 1.0)
        plt.pcolor(lh_view[self.min_rep - 1:, self.min_rep:], cmap=palette, vmin=z_min, vmax=z_max)
        # plt.colorbar()

        # draw dividing line(s):
        plt.plot(
            [max_str - self.min_rep, max_str - self.min_rep],
            [0, max_str - self.min_rep + 1], 'k', linewidth=3
        )
        plt.plot([0, max_str - self.min_rep + 1], [1, 1], 'k', linewidth=3)

        # text background:
        plt.text(
            float(bg_size) / 2.0, max_str - self.min_rep + 1 - float(bg_size) / 2.0,
            'BG', size=20, horizontalalignment='center',
            verticalalignment='center', path_effects=[patheffects.withStroke(linewidth=2.5, foreground="w")])
        # text expanded
        plt.text(
            bg_size + float(bg_size) / 2.0, max_str - self.min_rep + 1 - float(bg_size) / 2.0,
            'Exp', size=20, horizontalalignment='center',
            verticalalignment='center', path_effects=[patheffects.withStroke(linewidth=2.5, foreground="w")])

        # save
        plt.savefig(display_file)
        plt.close()

    def save_pcolor_plotly_file(
        self, display_file: str,
        lh_copy: np.ndarray, lognorm: bool,
        title: str, max_str: int, start_ticks: int = 5, step_ticks: int = 5
    ):
        text = [['' for _ in range(max_str - self.min_rep + 1)] for _ in range(max_str - self.min_rep + 1)]
        text[-1][0] = 'B'
        text[-1][1] = 'E'

        hovertext = []
        for i in list(range(self.min_rep, max_str)) + ['E']:
            inner = [f'{j}/{i}' for j in ['B'] + list(range(self.min_rep, max_str))]
            hovertext.append(inner)

        hovertext[0][-1] = 'E/E'
        hovertext[-1][0] = 'B'
        hovertext[-1][1] = 'E'

        fig = go.Figure()
        fig.add_trace(go.Heatmap(z=lh_copy[self.min_rep - 1:, self.min_rep:],
                                 text=text, name='', hovertext=hovertext,
                                 showscale=True, colorscale='Jet'))
        fig.add_vline(x=max_str - self.min_rep - 0.5, line_width=5, line_color='black', opacity=1)
        fig.add_hline(y=0.5, line_width=5, line_color='black', opacity=1)

        fig.update_traces(
            texttemplate='%{text}', textfont_size=15,
            hovertemplate='<b>%{{hovertext}} - {log} likelihood:\t%{{z}}</b>'.format(
                log='Loglog' if lognorm else 'Log'))
        fig.update_layout(width=500, height=450,
                          template='simple_white',
                          yaxis_fixedrange=True, xaxis_fixedrange=True,
                          title=title)
        fig.update_yaxes(
            title_text='1st allele', tickmode='array',
            tickvals=np.concatenate([
                np.array(range(start_ticks - self.min_rep + 1, max_str - self.min_rep + 1, step_ticks)),
                [0]
            ]),
            ticktext=list(range(start_ticks, max_str, step_ticks)) + ['B'])
        fig.update_xaxes(
            title_text='2nd allele', tickmode='array',
            tickvals=np.concatenate([
                np.array(range(start_ticks - self.min_rep, max_str - self.min_rep, step_ticks)),
                [max_str - self.min_rep]
            ]),
            ticktext=list(range(start_ticks, max_str, step_ticks)) + ['E(>%d)' % (self.max_with_e - 2)])

        with open(display_file, 'w') as f:
            f.write(fig.to_json())

        # fig.write_image(display_file + '_plotly.png')

    def print_pcolor(
        self, lh_dict: dict[tuple[int | str, int | str], float],
        display_file: str | None,
        name: str, lognorm: bool = True
    ) -> tuple[np.ndarray, tuple[int, int]]:
        """
        Get maximum likelihood option and alternatively print it to image file.
        :param lh_dict: dict(tuple(int, int):float) - directory of model indices to their likelihood
        :param display_file: str|None - filename for pcolor image output
        :param name: str - name to use in title
        :param lognorm: bool - use loglog scale in displaying likelihood array
        :return: tuple(int, int) - option with the highest likelihood
        """
        # convert to a numpy array:
        lh_array = np.zeros((self.max_rep, self.max_rep + 1))
        for (k1, k2), v in lh_dict.items():
            if k2 == 'X':  # if we have mono-allelic
                k2 = k1
            # B is the smallest, E is the largest!
            if k2 == 'B' or k1 == 'E' or (isinstance(k1, int) and isinstance(k2, int) and k2 < k1):
                k1, k2 = k2, k1
            if k1 == 'B':
                k1 = 0
            if k2 == 'B':
                k2 = 0
            if k1 == 'E':  # only if k2 is 'E' too.
                k1 = 0
            if k2 == 'E':
                k2 = self.max_rep
            lh_array[k1, k2] = v

        # get minimal and maximal likelihood
        ind_good = (lh_array < 0.0) & (lh_array > -1e10) & (lh_array != np.nan)
        if len(lh_array[ind_good]) == 0:
            return lh_array, (0, 0)
        lh_array[~ind_good] = np.NINF

        # generate image file if specified:
        if display_file is not None:
            z_min, z_max = min(lh_array[ind_good]), max(lh_array[ind_good])
            max_str = len(lh_array)
            if lognorm:
                lh_view = -np.log(-lh_array)
                z_min = -np.log(-z_min)
                z_max = -np.log(-z_max)
            else:
                lh_view = lh_array.copy()

            # background (B, i) - copy it below min_rep
            lh_view[self.min_rep - 1, :] = lh_view[0, :]

            lh_copy = lh_view.copy()
            lh_copy[-1, self.min_rep] = lh_copy[0, 0]
            lh_copy[-1, self.min_rep + 1] = lh_copy[0, self.max_rep]

            title = '%s likelihood of options (%s)' % ('Loglog' if lognorm else 'Log', name)
            # self.save_pcolor_file(display_file + '.pdf', lh_view, z_min, z_max, title, max_str)
            # self.save_pcolor_file(display_file + '.png', lh_view, z_min, z_max, title, max_str)
            self.save_pcolor_plotly_file(display_file + '.json', lh_copy, lognorm, title, max_str)

        # output best option
        best = sorted(np.unravel_index(np.argmax(lh_array), lh_array.shape))
        return lh_array, (int(best[0]), int(best[1]))

    def convert_to_sym(self, best: tuple[int, int], monoallelic: bool) -> tuple[int | str, int | str]:
        """
        Convert numeric alleles to their symbolic representations.
        :param best: (int, int) - numeric representation of alleles
        :param monoallelic: bool - if this is monoallelic version
        :return: (int|str, int|str) - symbolic representation of alleles
        """
        # convert it to symbols
        if best[0] == 0 and best[1] == self.max_rep:
            best_sym = ('E', 'E')
        else:
            def fn1(x):
                return 'E' if x == self.max_rep else 'B' if x == 0 else x
            best_sym = tuple(map(fn1, best))

        # if mono-allelic return 'X' as second allele symbol
        if monoallelic:
            best_sym = (best_sym[0], 'X')

        return best_sym

    def get_confidence(
        self, lh_array: np.ndarray, predicted: tuple[int, int], monoallelic: bool = False
    ) -> tuple[float, float, float | str, float, float, float, float]:
        """
        Get confidence of a prediction.
        :param lh_array: 2D-ndarray - log likelihoods of the prediction
        :param predicted: tuple(int, int) - predicted alleles
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: tuple[float, float, float | str, float, float, float, float] - prediction confidence of
        all, first, and second allele(s), background and expanded states
        """
        # get confidence
        lh_corr_array = lh_array - np.max(lh_array)
        lh_sum = np.sum(np.exp(lh_corr_array))
        confidence = np.exp(lh_corr_array[predicted[0], predicted[1]]) / lh_sum
        if predicted[0] == predicted[1]:  # same alleles - we compute the probability per allele
            confidence1 = np.sum(np.exp(lh_corr_array[predicted[0], :])) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        elif predicted[1] == lh_corr_array.shape[0]:  # expanded allele - expanded is only on one side of the array
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        else:  # normal behavior - different alleles , no expanded, compute all likelihoods of the alleles
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = (
                np.sum(np.exp(lh_corr_array[:, predicted[1]]))
                + np.sum(np.exp(lh_corr_array[predicted[1], :]))
                - np.exp(lh_corr_array[predicted[1], predicted[1]])
            ) / lh_sum

        confidence_back = np.exp(lh_corr_array[0, 0]) / lh_sum
        confidence_back_all = np.sum(np.exp(lh_corr_array[0, :])) / lh_sum
        confidence_exp = np.exp(lh_corr_array[0, self.max_rep]) / lh_sum
        confidence_exp_all = np.sum(np.exp(lh_corr_array[:, self.max_rep])) / lh_sum

        if monoallelic:
            confidence2 = '---'

        return (
            confidence, confidence1, confidence2,
            confidence_back, confidence_back_all,
            confidence_exp, confidence_exp_all
        )

    # this should be normal function outside the class, there is no need for staticmethod
    @staticmethod
    def write_output(
        file_desc: str | TextIO, predicted: tuple[int | str, int | str],
        conf: tuple[float, float, float | str, float, float, float, float],
        name: str | int
    ):
        """
        Write result of one prediction.
        :param file_desc: file descriptor - where to write to
        :param predicted: tuple(int/char, int/char) - predicted alleles
        :param conf: confidence of prediction (whole, 1st allele, 2nd allele, background and expanded alleles)
        :param name: str/int - name/number of the sample
        :return: None
        """
        def write_output_fd(f, predicted, conf, name):
            print(f'Predicted alleles for {name}: (confidence = {float_to_str(conf[1], percents=True)})', file=f)
            print(f'\t{str(predicted[0]):3s} (confidence = {float_to_str(conf[1], percents=True)})', file=f)
            print(f'\t{str(predicted[1]):3s} (confidence = {float_to_str(conf[2], percents=True)})', file=f)
            print(f'B   B   {float_to_str(conf[3], percents=True)}', file=f)
            print(f'all B   {float_to_str(conf[4], percents=True)}', file=f)
            print(f'B   E   {float_to_str(conf[5], percents=True)}', file=f)
            print(f'all E   {float_to_str(conf[6], percents=True)}', file=f)

        if type(file_desc) is str:
            with open(file_desc, 'w') as f:
                write_output_fd(f, predicted, conf, name)
        else:
            write_output_fd(file_desc, predicted, conf, name)

    def genotype(
        self, annotations: list[Annotation], filt_annotations: list[Annotation], index_rep: int,
        file_pcolor: str | None,
        file_output: str | None,
        name: str, monoallelic: bool = False
    ) -> tuple[tuple[str | int, str | int], tuple[float, float, float | str, float, float, float, float]]:
        """
        Genotype based on all annotations - infer likelihoods, print pcolor and write output
        :param annotations: list(Annotation) - good (blue) annotations
        :param filt_annotations: list(Annotation) - (grey) annotations with one primer
        :param index_rep: i
        nt - index of a repetition
        :param file_pcolor: str - file prefix for a pcolor image
        :param file_output: str - file for genotyping output
        :param name: str - name of the sample
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: tuple - predicted symbols and confidences
        """
        # if we do not have any good annotations, then quit
        if len(annotations) == 0 and len(filt_annotations) == 0:
            return ('B', 'B'), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)

        # infer likelihoods
        lh_dict = self.infer(annotations, filt_annotations, index_rep, monoallelic=monoallelic)

        # print pcolor image
        # creates file .pdf, .png and .json
        lh_array, predicted = self.print_pcolor(lh_dict, file_pcolor, name)

        # adjust for no spanning reads (should output Background)
        if len(annotations) == 0:
            predicted = (0, 0)

        # convert numbers to symbols
        predicted_sym = self.convert_to_sym(predicted, monoallelic)

        # get confidence of our prediction
        confidence = self.get_confidence(lh_array, predicted, monoallelic)

        # write output
        if file_output is not None:
            self.write_output(file_output, predicted_sym, confidence, name)

        # return predicted and confidence
        return predicted_sym, confidence


def phase(
    annotations: list[Annotation], module_number1: int, module_number2: int
) -> tuple[tuple[str, str], tuple[str, str, str]]:
    """
    Infer phasing based on the Annotations.
    :param annotations: list(Annotation) - good (blue) annotations
    :param module_number1: int - index of a repetition
    :param module_number2: int - index of the second repetition
    :return: tuple - predicted symbols and confidences
    """
    # resolve trivial case
    if len(annotations) == 0:
        return ('-|-', '-|-'), ('-/0', '-/0', '-/0')

    # gather module repetitions from annotations and count them
    repetitions = Counter([
        (ann.module_repetitions[module_number1], ann.module_repetitions[module_number2]) for ann in annotations
    ])

    # pick the highest two
    most_common = repetitions.most_common(2)
    rep1, cnt1 = most_common[0]
    rep2, cnt2 = most_common[1] if len(most_common) >= 2 else (('-', '-'), 0)

    # output phasing with number of supported reads
    phasing = (f'{rep1[0]}|{rep1[1]}', f'{rep2[0]}|{rep2[1]}')
    supported_reads = (f'{cnt1 + cnt2}/{len(annotations)}', f'{cnt1}/{len(annotations)}', f'{cnt2}/{len(annotations)}')
    return phasing, supported_reads


def save_phasing(phasing_file: str, phasing: tuple[str, str], supp_reads: tuple[str, str, str]) -> None:
    """
    Save the phasing information to a file.
    :param phasing_file: str - filename where to save the phasing information
    :param phasing: tuple - phasing information
    :param supp_reads: tuple - supporting reads (number of reads that support the
    """
    # save into a file:
    with open(phasing_file, 'w') as f:
        f.write(f'{phasing[0]}\t{phasing[1]}\n')
        f.write(f'{supp_reads[0]}\t{supp_reads[1]}\t{supp_reads[2]}\n')


CONTENTS = """
<table class="tg" id="content-tg">
    <thead>
        <tr>
            <th class="tg-s6z2">Motif</th>
        </tr>
    </thead>
    <tbody>
        {table}
    </tbody>
</table>
"""

MAKE_DATATABLE_STRING = """
<script>
    $(document).ready( function () {{
    $('#content-tg').DataTable();
}} );
</script>
"""

# this is repeated for each motif
CONTENT_STRING = """
<tr>
    <td class="tg-s6z2">
        <a href="#{motif_name}" class="tablinks"
        onclick="openTab(event, '{motif_name}'); $('.{motif_name}').trigger('content-change');">{motif}</a>
    </td>
</tr>
"""

CONTENT_STRING_EMPTY = ""

ROW_STRING = """
  <tr>
    <td class="tg-s6z2">{motif_name}</td>
    <td class="tg-s6z2">{motif_nomenclature}</td>
    <td class="tg-s6z2">{allele1}</td>
    <td class="tg-s6z2">{conf_allele1}</td>
    <td class="tg-s6z2">{reads_a1}</td>
    <td class="tg-s6z2">{indels_a1}</td>
    <td class="tg-s6z2">{mismatches_a1}</td>
    <td class="tg-s6z2">{allele2}</td>
    <td class="tg-s6z2">{conf_allele2}</td>
    <td class="tg-s6z2">{reads_a2}</td>
    <td class="tg-s6z2">{indels_a2}</td>
    <td class="tg-s6z2">{mismatches_a2}</td>
    <td class="tg-s6z2">{confidence}</td>
    <td class="tg-s6z2">{indels}</td>
    <td class="tg-s6z2">{mismatches}</td>
    <td class="tg-s6z2">{quality_reads}</td>
    <td class="tg-s6z2">{one_primer_reads}</td>
  </tr>
"""

NOMENCLATURE_STRING = """
<tr>
    <td>{count}</td>
    <td>{ref}</td>
    {parts}
</tr>
"""

MOTIF_SUMMARY = """
<div class="tabcontent" id="{motif_id}" style="display: none">
<h2 class="summary_nomenclatures">Nomenclatures</h2>
<table class="nomtg">
    <tbody>
        {nomenclatures}
    </tbody>
</table>
<h2 class="summary">Summary table</h2>
<table class="tg" id="tg-{motif_id}">
    <thead>
        <tr>
            <th class="tg-s6z2" rowspan="3">Motif</th>
            <th class="tg-s6z2" rowspan="3">Sequence</th>
            <th class="tg-s6z2" colspan="5">Allele 1</th>
            <th class="tg-s6z2" colspan="5">Allele 2</th>
            <th class="tg-s6z2" rowspan="3">Overall<br>confidence</th>
            <th class="tg-s6z2" colspan="2">Errors per read</th>
            <th class="tg-s6z2" colspan="2">Reads</th>
        </tr>
        <tr>
            <td class="tg-smaller" rowspan="2">prediction</td>
            <td class="tg-smaller" rowspan="2">confidence</td>
            <td class="tg-smaller" rowspan="2">reads</td>
            <td class="tg-smaller" colspan="2">Errors per read</td>
            <td class="tg-smaller" rowspan="2">prediction</td>
            <td class="tg-smaller" rowspan="2">confidence</td>
            <td class="tg-smaller" rowspan="2">reads</td>
            <td class="tg-smaller" colspan="2">Errors per read</td>
            <td class="tg-smaller" rowspan="2">indel</td>
            <td class="tg-smaller" rowspan="2">mismatch</td>
            <td class="tg-smaller" rowspan="2">full</td>
            <td class="tg-smaller" rowspan="2">partial</td>
        </tr>
        <tr>
            <td class="tg-smaller">indel</td>
            <td class="tg-smaller">mismatch</td>
            <td class="tg-smaller">indel</td>
            <td class="tg-smaller">mismatch</td>
        </tr>
    </thead>
    <tbody>
        {table}
    </tbody>
</table>

<script>
    $(document).ready( function () {{
    $('#tg-{motif_id}').DataTable({{scrollX: true}});
}} );
</script>

{motifs}
</div>
"""

MOTIF_STRINGB64 = """
<h2 id="data-{motif_name}">{motif}</h2>
<p>{sequence}</p>
Nomenclatures:<br>
<table class="nomtg">
    <tbody>
        {nomenclatures}
    </tbody>
</table>
postfilter: bases {post_bases} , repetitions {post_reps} , max. errors {errors}<br>
alleles: {result}<br>
<table class="plots">
    <tr>
        <td colspan="1">
            <div class="hist pic100 {motif_id}" id="hist-{motif_name}"></div>
        </td>
        <td colspan="1">
            <div class="pcol pic100 {motif_id}" id="pcol-{motif_name}"></div>
        </td>
    </tr>
</table>

<script>
    {{
        let hist_data = {motif_reps};
        let pcol_data = {motif_pcolor};

        let updateGraph = () => {{
            if (document.getElementById('{motif_id}').style.display === 'block') {{
                hist_data['layout'] = {{...hist_data['layout'],
                    width: (window.innerWidth-50) * 0.6,
                    height: (window.innerWidth-50) * 0.35,
                    legend: {{...hist_data['layout']['legend'], x: 0.85, y: 0.85 }}}};
                pcol_data['layout'] = {{...pcol_data['layout'],
                                        width: (window.innerWidth-50) * 0.4,
                                        height: (window.innerWidth-50) * 0.35}};
                Plotly.react('hist-{motif_name}', hist_data);
                Plotly.react('pcol-{motif_name}', pcol_data);
            }}
        }};

        $(document).ready(function() {{
            $('.{motif_id}').bind("content-change", updateGraph);
            window.addEventListener('resize', updateGraph, true);
        }});
    }}
</script>

<!-- TODO: this is incorrect -->
<p><a href="{alignment}">Link to alignments</a></p>
<p><a href="#content">Back to content</a></p>
"""

MOTIF_STRINGB64_REPONLY = """
<h2 id="data-{motif_name}">{motif}</h2>
<p>{sequence}</p>
Nomenclatures:<br>
<table class="nomtg">
    <tbody>
        {nomenclatures}
    </tbody>
</table>
postfilter: bases {post_bases} , repetitions {post_reps} , max. errors {errors}<br>
alleles: {result}<br>
<table class="plots">
    <tr>
        <td colspan="1">
            <div class="hist {motif_id}" id="hist2d-{motif_name}"></div>
        </td>
    </tr>
</table>

<script>
    {{
        let hist_data = {motif_reps};

        let updateGraph = () => {{
            if (document.getElementById('{motif_id}').style.display === 'block') {{
                hist_data['layout'] = {{...hist_data['layout'],
                                        width: (window.innerWidth-50) * 0.5,
                                        height: (window.innerWidth-50) * 0.45}};
                Plotly.react('hist2d-{motif_name}', hist_data);
            }}
        }};

        $(document).ready(function() {{
            $('.{motif_id}').bind("content-change", updateGraph);
            window.addEventListener('resize', updateGraph, true);
        }});
    }}
</script>

<p><a href="{alignment}">Link to alignments</a></p>
<p><a href="#content">Back to content</a></p>
"""

MOTIF_STRING_EMPTY = ""

ALIGNMENT_STRING = """
  <p>{sequence}</p>
  {alignment}
  <hr>
"""

ALIGN_VIS = """
  <details>
    <summary>{display_text}</summary>
    <div id="A{name}" class="align">press "Run with JS"</div>
    <script>
        var fasta = `{fasta}`;
        var seqs = msa.io.fasta.parse(fasta);
        var opts = {{
            el: document.getElementById("A{name}"),
            vis: {{
                conserv: false,
                metaIdentity: true,
                overviewbox: true,
                seqlogo: {seq_logo}
            }},
            seqs: seqs,
            colorscheme: {{"scheme": "nucleotide"}},
            // smaller menu for JSBin
            menu: "small",
            bootstrapMenu: true
        }};
        var m = new msa.msa(opts);
        m.render()
    </script>
  </details>
"""


def highlight_subpart(seq: str, highlight: int | list[int]) -> tuple[str, str]:
    """
    Highlights subpart of a motif sequence
    :param seq: str - motif sequence
    :param highlight: int/list(int) - part ot highlight
    :return: str, str - motif sequence with highlighted subpart, highlighted subpart
    """
    if highlight is None:
        return seq, ''

    str_part = []
    highlight1 = np.array(highlight)
    split = [f'{s}]' for s in seq.split(']') if s != '']
    for h in highlight1:
        str_part.append(split[h])
        split[h] = f'<b><u>{split[h]}</u></b>'
    return ''.join(split), ''.join(str_part)


def float_to_str(c: float | str, percents: bool = False, decimals: int = 1) -> str:
    """
    Convert float confidence to string.
    :param c: float/str - confidence
    :param percents: bool - whether to output as a percents or not
    :param decimals: int - how many decimals to round to
    :return: str - converted to string
    """
    if isinstance(c, float):
        return f'{c * 100: .{decimals}f}%' if percents else f'{c: .{decimals}f}'
    return c


def generate_row(sequence: str, result: pd.Series, postfilter: PostFilter) -> str:
    """
    Generate rows of a summary table in html report.
    :param sequence: str - motif sequence
    :param result: pd.Series - result row to convert to table
    :param postfilter: PostFilter - postfilter dict from config
    :return: str - html string with rows of the summary table
    """
    highlight = list(map(int, str(result['repetition_index']).split('_')))
    sequence, _subpart = highlight_subpart(sequence, highlight)

    # shorten sequence:
    keep = 10
    first = sequence.find(',')
    last = sequence.rfind(',')
    smaller_seq = sequence if first == -1 else '...' + sequence[first - keep:last + keep + 1] + '...'

    # errors:
    errors = f'{postfilter.max_rel_error * 100:.0f}%'
    if postfilter.max_abs_error is not None:
        errors += f' (abs={postfilter.max_abs_error})'

    # fill templates:
    updated_result = {
        'conf_allele1': float_to_str(result['conf_allele1'], percents=True),
        'conf_allele2': float_to_str(result['conf_allele2'], percents=True),
        'confidence': float_to_str(result['confidence'], percents=True),
        'motif_nomenclature': smaller_seq,
        'indels': float_to_str(result['indels'], decimals=2),
        'mismatches': float_to_str(result['mismatches'], decimals=2),
        'indels_a1': float_to_str(result['indels_a1'], decimals=2),
        'mismatches_a1': float_to_str(result['mismatches_a1'], decimals=2),
        'indels_a2': float_to_str(result['indels_a2'], decimals=2),
        'mismatches_a2': float_to_str(result['mismatches_a2'], decimals=2)}
    return ROW_STRING.format(**{**result, **updated_result})  # TODO: WTF?


def get_alignment_name(alignment_file: str, allele: int) -> str:
    """
    Get alignment file of subpart of the alignment with allele count specified.
    :param alignment_file: str - alignment file name
    :param allele: int - allele repetition count
    :return: str - alignment file name for the subpart of alignment
    """
    # find where is .fasta
    fasta_index = alignment_file.rfind('.fasta')
    # insert '_aX' before .fasta
    return alignment_file[:fasta_index] + '_a' + str(allele) + alignment_file[fasta_index:]


def generate_motifb64(
    sequence: str, result_in: pd.Series, repetition: str | None, pcolor: str | None, alignment: str | None,
    filtered_alignment: str | None, filtered_left_alignment: str | None, filtered_right_alignment: str | None,
    nomenclature_lines: list[str], postfilter: PostFilter
) -> tuple[str, str, tuple[str, str]]:
    """
    Generate part of a html report for each motif.
    :param sequence: str - motif sequence
    :param repetition: str - filename of repetitions figures
    :param pcolor: str - filename of pcolor figures
    :param alignment: str/None - filename of alignment file
    :param filtered_alignment: str/None - filename of filtered alignment file
    :param filtered_left_alignment: str/None - filename of filtered left alignment file
    :param filtered_right_alignment: str/None - filename of filtered right alignment file
    :param nomenclature_lines: list[str] - list of nomenclature strings
    :param postfilter: PostFilter - postfilter arguments
    :param static: bool - generate static code?
    :return: (str, str) - content and main part of the html report for motifs
    """
    # prepare and generate alignments
    highlight = list(map(int, str(result_in['repetition_index']).split('_')))
    sequence, _subpart = highlight_subpart(sequence, highlight)
    motif = result_in['motif_name']
    motif_name_part1 = f'{result_in["motif_name"].replace("/", "_")}'
    motif_name_part2 = f'{",".join(map(str, highlight)) if highlight is not None else "mot"}'
    motif_name = f'{motif_name_part1}_{motif_name_part2}'
    motif_clean = re.sub(r'[^\w_]', '', motif_name)
    motif_clean_id = motif_clean.rsplit('_', 1)[0] if highlight == [1] else motif_clean  # trick to solve static html
    align_html_a1 = ''
    align_html_a2 = ''

    a1 = result_in['allele1']
    a2 = result_in['allele2']
    conf_total = float_to_str(result_in['confidence'], percents=True)
    conf_a1 = float_to_str(result_in['conf_allele1'], percents=True)
    conf_a2 = float_to_str(result_in['conf_allele2'], percents=True)
    if (a1 == 'B' and a2 == 'B') or (a1 == 0 and a2 == 0):
        result = f'BG {conf_total}'
    else:
        result = f'{str(a1):2s} ({conf_a1}) {str(a2):2s} ({conf_a2}) total {conf_total}'
        if alignment is not None:
            align_html_a1 = generate_alignment(
                f'{motif_clean}_{str(a1)}', get_alignment_name(alignment, a1), motif_clean.split('_')[0],
                f'Allele 1 ({str(a1):2s}) alignment visualization')
            if a1 != a2:
                align_html_a2 = generate_alignment(
                    f'{motif_clean}_{str(a2)}', get_alignment_name(alignment, a2), motif_clean.split('_')[0],
                    f'Allele 2 ({str(a2):2s}) alignment visualization')

    # errors:
    errors = f'{postfilter.max_rel_error * 100:.0f}%'
    if postfilter.max_abs_error is not None:
        errors += f' (abs={postfilter.max_abs_error})'

    # return content and picture parts:
    motif_templates = {'pcol': MOTIF_STRINGB64, 'no-pcol': MOTIF_STRINGB64_REPONLY}

    if repetition is None:
        return (
            CONTENT_STRING_EMPTY,
            MOTIF_STRING_EMPTY,
            (motif, '')
        )

    reps = open(repetition, 'r').read()
    align_html = generate_alignment(
        motif_clean, alignment, motif_clean.split('_')[0],
        'Spanning reads alignment visualization'
    )
    filt_align_html = generate_alignment(
        motif_clean + '_filtered', filtered_alignment, motif_clean.split('_')[0],
        'Partial reads alignment visualization', seq_logo=False
    )
    left_align_html = generate_alignment(
        motif_clean + '_filtered_left', filtered_left_alignment, motif_clean.split('_')[0],
        'Left flank reads alignment visualization'
    )
    right_align_html = generate_alignment(
        motif_clean + '_filtered_right', filtered_right_alignment, motif_clean.split('_')[0],
        'Right flank reads alignment visualization'
    )

    # select template
    motif_template = motif_templates['no-pcol' if pcolor is None else 'pcol']

    # read pcolor if available
    pcol = '' if pcolor is None else open(pcolor, 'r').read()

    # return filled valid template
    return (
        CONTENT_STRING.format(motif_name=motif_clean.rsplit('_', 1)[0], motif=motif),
        motif_template.format(
            post_bases=postfilter.min_rep_len, post_reps=postfilter.min_rep_cnt, motif_name=motif_clean_id,
            motif_id=motif_clean.rsplit('_', 1)[0], motif=motif, motif_reps=reps, result=result,
            motif_pcolor=pcol, alignment=f'{motif_name.replace(" ", "%20")}/alignments.html', sequence=sequence,
            errors=errors, nomenclatures='\n'.join(nomenclature_lines)
        ),
        (motif, ALIGNMENT_STRING.format(
            sequence=sequence, alignment=align_html + align_html_a1 + align_html_a2
            + filt_align_html + left_align_html + right_align_html
        ))
    )


def generate_alignment(
    motif: str, alignment_file: str | None, motif_id: str,
    display_text: str = 'Click to toggle alignment visualization',
    seq_logo: bool = True
) -> str:
    """
    Generate HTML code for the fancy alignment.
    :param motif: str - name of the motif
    :param alignment_file: str - filename of the alignment file
    :param motif_id: str - motif identification
    :param display_text: str - string to display when the alignment is hidden
    :param seq_logo: bool - display sequence logo?
    :return: str - code of the fancy alignment
    """
    if alignment_file is None:
        return ''

    try:
        with gzip.open(alignment_file, 'rt') if alignment_file.endswith('.gz') else open(alignment_file) as f:
            string = f.read()
        string = string[:string.find('#')]

        return ALIGN_VIS.format(
            fasta=string, name=motif, motif_id=motif_id,
            display_text=display_text, seq_logo='true' if seq_logo else 'false'
        )
    except (IOError, TypeError, AttributeError):
        return ''


if __name__ == '__main__':
    # this is good practice so the variables do not pollute the global scope
    main()
