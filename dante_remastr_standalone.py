from __future__ import annotations

from argparse import ArgumentParser, Namespace, RawDescriptionHelpFormatter, ArgumentTypeError
from datetime import datetime
from typing import TextIO, Iterator, TypeAlias, Any
from collections import Counter
from jinja2 import Environment, FileSystemLoader

import csv
import os
import re
import sys
import json
import textwrap
import enum
import shutil
import math
import functools
import itertools

import numpy as np
import pandas as pd

from scipy.stats import binom  # type: ignore

VERSION = "0.9.0"
DANTE_DESCRIPTION = '''
    DANTE = Da Amazing NucleoTide Exposer (Remastered)
    --------------------------------------------------
Genotyping and reporting from annotated reads generated by the remaSTR
program. The reads are filtered, clustered, and a genotype is inferred for
each motif. If a verbose option is switched on, DANTE creates a directory
for each of the annotated motifs and a summary report in HTML (report.html).
Otherwise only a table with all genotypes, confidences, and supporting
information.
'''
MOTIF_COLUMN_NAME = 'motif'
MIN_FLANK_LEN = 3
MIN_REP_LEN = 3
MIN_REP_CNT = 1
MAX_ABS_ERROR = None
MAX_REL_ERROR = 1.0
MAX_REPETITIONS = 40

BASE_MAPPING = {
    'A': 'A', 'C': 'C', 'G': 'G', 'T': 'T',
    'M': '[AC]', 'R': '[AG]', 'W': '[AT]', 'S': '[CG]', 'Y': '[CT]', 'K': '[GT]',
    'V': '[ACG]', 'H': '[ACT]', 'D': '[AGT]', 'B': '[CGT]',
    'N': '[ACGT]'
}

# vocabulary:
# Motif
# Nomenclature
# Locus - repetitive module of a Motif
# Allele

# prediction could be from -3, -2, ..., n
# where -3 would be translated to X, -2 to E, -1 to B, and numbers to numbers


def main() -> None:
    start_time = datetime.now()
    args = load_arguments()
    # args = load_arguments_fake()

    print('DANTE_remaSTR = "Da Amazing NucleoTide Exposer" (remastered)')
    print(f'DANTE_remaSTR Starting : {start_time:%Y-%m-%d %H:%M:%S}')

    all_motifs = []
    all_annotations = []
    all_genotypes = []
    all_haplotypes = []
    for motif_table in generate_groups(args.input_tsv):
        motif = create_motif(motif_table)
        motif_monoallelic = args.male and chrom_from_string(motif.chrom) in [ChromEnum.X, ChromEnum.Y]
        annotations = create_annotations(motif_table, motif)
        genotypes = genotype_group(motif, motif_monoallelic, annotations)
        haplotypes = phase_group(motif, annotations)

        all_motifs.append(motif)
        all_annotations.append(annotations)
        all_genotypes.append(genotypes)
        all_haplotypes.append(haplotypes)
        del motif_table, motif, motif_monoallelic, annotations, genotypes, haplotypes

    # core computation ends here
    # next lines are only outputing things
    # os.makedirs(args.output_dir, exist_ok=True)

    print(f'Writing tsv output: {datetime.now():%Y-%m-%d %H:%M:%S}')
    variants_df = construct_dataframe(all_motifs, all_genotypes, all_haplotypes)
    variants_df.to_csv(args.output_dir + "/variants.tsv", sep='\t')

    print(f'Writing vcf output: {datetime.now():%Y-%m-%d %H:%M:%S}')
    write_vcf(variants_df, args.output_dir)

    print(f'Writing phased predictions: {datetime.now():%Y-%m-%d %H:%M:%S}')
    write_phased_predictions(all_motifs, all_genotypes, all_haplotypes, args.output_dir + "/phased_predictions.tsv")

    if args.verbose:
        script_dir = os.path.dirname(__file__) + "/dante_remastr_standalone_templates"

        print(f'Writing alignment htmls: {datetime.now():%Y-%m-%d %H:%M:%S}')
        for (motif, genotype, phasing) in zip(all_motifs, all_genotypes, all_haplotypes):
            write_alignment_html(motif, genotype, phasing, script_dir, args.output_dir, args.cutoff_alignments)

        print(f'Writing html report: {datetime.now():%Y-%m-%d %H:%M:%S}')
        write_report(all_motifs, all_annotations, all_genotypes, all_haplotypes, script_dir, args.output_dir)

        # copy javascript and css files
        shutil.copy2(f'{script_dir}/msa.min.gz.js',         f'{args.output_dir}/msa.min.gz.js')
        shutil.copy2(f'{script_dir}/plotly-2.14.0.min.js',  f'{args.output_dir}/plotly-2.14.0.min.js')
        shutil.copy2(f'{script_dir}/jquery-3.6.1.min.js',   f'{args.output_dir}/jquery-3.6.1.min.js')
        shutil.copy2(f'{script_dir}/datatables.min.js',     f'{args.output_dir}/datatables.min.js')
        shutil.copy2(f'{script_dir}/styles.css',            f'{args.output_dir}/styles.css')

    end_time = datetime.now()
    print(f'DANTE_remaSTR Stopping : {end_time:%Y-%m-%d %H:%M:%S}')
    print(f'Total time of run      : {end_time - start_time}')


def write_alignment_html(
    motif: Motif, genotype: list[GenotypeInfo], phasing: list[None | tuple],
    script_dir: str, output_dir: str, flank_size: int
) -> None:
    seq = motif.modules_str(include_flanks=True)
    motif_dir = f'{output_dir}/{motif.dir_name()}'
    os.makedirs(motif_dir, exist_ok=True)
    motif_desc = motif.name

    data2 = []
    for gt in genotype:
        (module_number, anns_spanning, anns_flanking, _, predicted, _, _, _) = gt

        suffix = str(module_number)
        highlight = list(map(int, str(module_number).split('_')))
        sequence, _subpart = highlight_subpart(seq, highlight)
        motif_name_part1 = f'{motif_desc.replace("/", "_")}'
        motif_name_part2 = f'{",".join(map(str, highlight)) if highlight is not None else "mot"}'
        motif_name = f'{motif_name_part1}_{motif_name_part2}'
        motif_clean = re.sub(r'[^\w_]', '', motif_name)
        motif_id = motif_clean.split('_')[0]

        a1 = int(predicted[0]) if isinstance(predicted[0], int) else None
        a2 = int(predicted[1]) if isinstance(predicted[1], int) else None

        anns_flanking_left = [a for a in anns_flanking if a.module_bases[0] > 0]
        anns_flanking_right = [a for a in anns_flanking if a.module_bases[-1] > 0]

        data = []

        if a1 is not None and a1 > 0:
            name = f'{motif_clean}_{str(a1)}'
            display_text = f'Allele 1 ({str(a1):2s}) alignment visualization'
            fasta = write_alignment(anns_spanning, module_number, a1, cutoff_after=flank_size)
            seq_logo = 'true'
            data.append((name, display_text, fasta, seq_logo))

        if a2 is not None and a2 != a1 and a2 != 0:
            name = f'{motif_clean}_{str(a2)}'
            display_text = f'Allele 2 ({str(a2):2s}) alignment visualization'
            fasta = write_alignment(anns_spanning, module_number, a2, cutoff_after=flank_size)
            seq_logo = 'true'
            data.append((name, display_text, fasta, seq_logo))

        name = motif_clean
        display_text = 'Spanning reads alignment visualization'
        fasta = write_alignment(anns_spanning, module_number, index_rep2=None, cutoff_after=flank_size)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered'
        display_text = 'Partial reads alignment visualization'
        fasta = write_alignment(anns_flanking, module_number, index_rep2=None, cutoff_after=flank_size)
        seq_logo = 'false'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered_left'
        display_text = 'Left flank reads alignment visualization'
        fasta = write_alignment(anns_flanking_left, module_number, index_rep2=None, cutoff_after=flank_size)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered_right'
        display_text = 'Right flank reads alignment visualization'
        fasta = write_alignment(anns_flanking_right, module_number, index_rep2=None, cutoff_after=flank_size, right_align=True)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        data2.append((sequence, motif_id, data))

    for ph in phasing[1:]:  # first phasing is None
        if ph is None:
            raise ValueError
        (module_number, anns_2good, anns_1good, _, _, _, prev_module_num) = ph

        suffix = f'{prev_module_num}_{module_number}'

        highlight = list(map(int, str(suffix).split('_')))
        sequence, _subpart = highlight_subpart(seq, highlight)
        motif_name_part1 = f'{motif_desc.replace("/", "_")}'
        motif_name_part2 = f'{",".join(map(str, highlight)) if highlight is not None else "mot"}'
        motif_name = f'{motif_name_part1}_{motif_name_part2}'
        motif_clean = re.sub(r'[^\w_]', '', motif_name)
        motif_id = motif_clean.split('_')[0]

        anns_1good_left = [a for a in anns_1good if a.module_bases[0] > 0]
        anns_1good_right = [a for a in anns_1good if a.module_bases[-1] > 0]

        data = []

        name = motif_clean
        display_text = 'Spanning reads alignment visualization'
        fasta = write_alignment(anns_2good, prev_module_num, index_rep2=module_number, cutoff_after=flank_size)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered'
        display_text = 'Partial reads alignment visualization'
        fasta = write_alignment(anns_1good, prev_module_num, index_rep2=module_number, cutoff_after=flank_size)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered_left'
        display_text = 'Left flank reads alignment visualization'
        fasta = write_alignment(anns_1good_left, prev_module_num, index_rep2=module_number, cutoff_after=flank_size)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        name = motif_clean + '_filtered_right'
        display_text = 'Right flank reads alignment visualization'
        fasta = write_alignment(anns_1good_right, prev_module_num, index_rep2=module_number, cutoff_after=flank_size, right_align=True)
        seq_logo = 'true'
        data.append((name, display_text, fasta, seq_logo))

        data2.append((sequence, motif_id, data))

    mt = motif.dir_name()

    env = Environment(loader=FileSystemLoader([script_dir]))
    template = env.get_template("alignments_template.html")
    output = template.render(sample=mt, motif_desc=motif_desc, data2=data2)
    with open(f"{output_dir}/{mt}/alignments.html", "w") as f:
        f.write(output)

    return


def write_phased_predictions(
    all_motifs: list[Motif], all_genotypes: list[list[GenotypeInfo]], all_haplotypes: list[list[None | tuple]], output: str
) -> None:
    result = []
    for (motif, genotype, phasing) in zip(all_motifs, all_genotypes, all_haplotypes):
        h1 = []
        h2 = []
        for gt in genotype:
            # (mod_num, spanning, flanking, filtered, predicted, conf, lh_array, model)
            (_, _, _, _, predicted, _, _, _) = gt
            h1.append(str(predicted[0]))
            h2.append(str(predicted[1]))

            # nomenclatures_local = generate_nomenclatures(anns_spanning, module_number, None, motif, nomenclature_limit)

        hp1 = []
        hp2 = []
        for ph in phasing:
            if ph is None:
                continue

            # (mod_num, 2good, 1good, 0good, phase, supp_reads, prev_mod_num)
            (_, _, _, _, phase, _, _) = ph
            hp1.append(phase[0])
            hp2.append(phase[1])

        h1_full, h2_full = phase_full_locus(h1, h2, hp1, hp2)
        result.append(f"{motif.name}\t{motif.augmented_nomenclature(h1_full)}\n")
        result.append(f"{motif.name}\t{motif.augmented_nomenclature(h2_full)}\n")

    with open(output, "w") as f:
        f.writelines(result)


def phase_full_locus(h1_full: list[str], h2_full: list[str], hp1: list[str], hp2: list[str]) -> tuple[list[str], list[str]]:
    # ['9', '15'] ['12', '16'] ['9|16'] ['12|15'] -> ['9', '16'] ['12', '15']

    n_diff: int = sum(map(lambda x: x[0] != x[1], zip(h1_full, h2_full)))
    if n_diff <= 1:
        return (h1_full, h2_full)  # there is nothing to phase

    # print()
    # print(h1_full, h2_full, hp1, hp2)
    different_prefix = False
    for i in range(len(h1_full) - 1):
        if h1_full[i] == h2_full[i]:
            if different_prefix:
                print(f"Warning: Cannot phase prefix and suffix at position {i}.\n{h1_full} {h2_full}\n{hp1} {hp2}")
            continue
        different_prefix = True

        h1_cur, _ = hp1[i].split("|")
        if h1_cur != h1_full[i]:
            hp1[i], hp2[i] = hp2[i], hp1[i]

        h1_cur, h1_next = hp1[i].split("|")
        _, h2_next = hp2[i].split("|")

        if h1_full[i] == h1_cur and h1_full[i + 1] == h1_next:
            # if h2_full[i] != h2_cur or h2_full[i + 1] != h2_next: WARN?
            pass
        elif h1_full[i] == h1_cur and h1_full[i + 1] == h2_next:
            # if h2_full[i] != h2_cur or h2_full[i + 1] != h1_next: WARN?
            h1_full[i + 1], h2_full[i + 1] = h2_full[i + 1], h1_full[i + 1]
        else:
            print(f"Warning: {h1_full[i:i + 2]} {h2_full[i:i + 2]} {hp1[i]} {hp2[i]} is inconsistent.")

    # print(f"-> {h1_full} {h2_full}\n")
    return (h1_full, h2_full)


def write_report(
    all_motifs: list[Motif], all_annotations: list[list[Annotation]],
    all_genotypes: list[list[GenotypeInfo]], all_haplotypes: list[list[None | tuple]],
    script_dir: str, output_dir: str, nomenclature_limit: int = 5
) -> None:
    post_filter = PostFilter()
    post_bases = post_filter.min_rep_len
    post_reps = post_filter.min_rep_cnt
    post_errors = f'{post_filter.max_rel_error * 100:.0f}%'
    if post_filter.max_abs_error is not None:
        post_errors += f' (abs={post_filter.max_abs_error})'
    postfilter_data = (post_bases, post_reps, post_errors)

    tabs = []
    for (motif, anns, genotype, phasing) in zip(all_motifs, all_annotations, all_genotypes, all_haplotypes):
        motif_dir = f'{output_dir}/{motif.dir_name()}'
        os.makedirs(motif_dir, exist_ok=True)
        seq = motif.modules_str(include_flanks=True)
        motif_id = re.sub(r'[^\w_]', '', motif.name.replace('/', '_'))

        repeating_modules = motif.get_repeating_modules()
        annotations = anns
        for module_number, _, _ in repeating_modules:
            annotations, _ = post_filter.get_filtered(annotations, module_number, both_primers=True)
        nomenclatures = generate_nomenclatures(annotations, None, None, motif, nomenclature_limit)

        graph_data: GraphData
        m_list1 = []
        for gt in genotype:
            (module_number, anns_spanning, anns_flanking, anns_filtered, predicted, confidence, lh_array, model) = gt
            suffix = str(module_number)

            locus_nomenclatures = generate_nomenclatures(anns_spanning, module_number, None, motif, nomenclature_limit)

            read_counts = None
            if len(anns_spanning) != 0 or len(anns_flanking) != 0:
                read_counts = write_histogram_image(anns_spanning, anns_flanking, module_number)
            else:
                print(f"Zero reads in {motif_id}")

            heatmap_data = None
            if lh_array is not None:
                heatmap_data = draw_pcolor(model, lh_array, motif.nomenclature)
            else:
                print(f"Likelihood array is None for {motif_id}.")

            row = generate_result_line(motif, predicted, confidence, len(anns_spanning), len(anns_flanking), len(anns_filtered), module_number, qual_annot=anns_spanning)
            row_tuple = generate_row(seq, row, post_filter)

            tmp = generate_motifb64(seq, row)
            (locus_id, _, _, _, _, sequence) = tmp
            del row

            graph_data = (read_counts, heatmap_data, None)

            _, _, a1_prediction, a1_confidence, a1_reads, a1_indels, a1_mismatches, a2_prediction, a2_confidence, a2_reads, a2_indels, a2_mismatches, confidence, indels, mismatches, spanning_reads, flanking_reads = row_tuple
            locus_data = (
                locus_id, sequence, locus_nomenclatures,
                (a1_prediction, a1_confidence, a1_indels, a1_mismatches, a1_reads),
                (a2_prediction, a2_confidence, a2_indels, a2_mismatches, a2_reads),
                (confidence, indels, mismatches),
                spanning_reads, flanking_reads,
                graph_data,
            )

            m_list1.append(locus_data)

        m_list2 = []
        for ph in phasing[1:]:
            if ph is None:
                raise ValueError

            (module_number, anns_2good, anns_1good, anns_0good, phasing1, supp_reads, prev_module_num) = ph
            second_module_number = module_number
            suffix = f'{prev_module_num}_{second_module_number}'

            row = generate_result_line(motif, phasing1, supp_reads, len(anns_2good), len(anns_1good), len(anns_0good), prev_module_num, second_module_number=module_number)
            row_tuple = generate_row(seq, row, post_filter)

            tmp = generate_motifb64(seq, row)
            (locus_id, _, _, _, _, sequence) = tmp

            annotations = anns_2good + anns_1good
            if len(annotations) == 0:
                print(f"{motif_dir} {suffix} is empty")

            locus_nomenclatures = generate_nomenclatures(anns_2good, prev_module_num, second_module_number, motif, nomenclature_limit)
            hist2d_data = write_histogram_image2d(annotations, prev_module_num, second_module_number, motif.module_str(prev_module_num), motif.module_str(second_module_number))

            graph_data = (None, None, hist2d_data)

            _, _, a1_prediction, a1_confidence, a1_reads, a1_indels, a1_mismatches, a2_prediction, a2_confidence, a2_reads, a2_indels, a2_mismatches, confidence, indels, mismatches, spanning_reads, flanking_reads = row_tuple
            locus_data2 = (
                locus_id, sequence, locus_nomenclatures,
                (a1_prediction, a1_confidence, a1_indels, a1_mismatches, a1_reads),
                (a2_prediction, a2_confidence, a2_indels, a2_mismatches, a2_reads),
                (confidence, indels, mismatches),
                spanning_reads, flanking_reads,
                graph_data,
            )

            m_list2.append(locus_data2)

        tabs.append((motif_id, nomenclatures, m_list1, m_list2))

    sample = os.path.basename(output_dir)
    version = VERSION
    tabs = sorted(tabs, key=lambda x: x[0])
    data = (sample, version, postfilter_data, tabs)

    # json_example = json.dumps((sample, version, postfilter_data, tabs[0:1] + tabs[11:12]), indent=4)
    json_dump = json.dumps(data, indent=4)
    with open(f"{output_dir}/data.json", "w") as f:
        f.write(json_dump)

    env = Environment(loader=FileSystemLoader([script_dir]), trim_blocks=True, lstrip_blocks=True)
    template = env.get_template("report_template.html")
    output = template.render(data=data)
    with open(f"{output_dir}/report.html", "w") as f:
        f.write(output)

    return


def generate_nomenclatures(
    annotations: list[Annotation], module_num: int | None, next_module_num: int | None, motif: Motif, nomenclature_limit: int
) -> list[tuple[str, str, str]]:

    count_dict = Counter(annot.get_nomenclature(module_num, next_module_num, False) for annot in annotations)
    count_dict2 = sorted(count_dict.items(), key=lambda k: (-k[1], k[0]))

    lines = []
    for nomenclature, count in count_dict2:
        count2 = str(count) + 'x'
        ref = f'{motif.chrom}:g.{motif.start}_{motif.end}'
        parts = ''.join([f'<td>{s}</td>' for s in nomenclature.rstrip().split('\t')])

        lines.append((count2, ref, parts))

        if len(lines) >= nomenclature_limit:
            break

    return lines


def construct_dataframe(
    all_motifs: list[Motif], all_genotypes: list[list[tuple]], all_haplotypes: list[list[None | tuple]]
) -> pd.DataFrame:
    all_result_lines: list[dict] = []
    for motif, genotype, phasing in zip(all_motifs, all_genotypes, all_haplotypes):
        rls = []
        for gt, ph in zip(genotype, phasing):

            (module_number, anns_spanning, anns_flanking, anns_filtered, predicted, confidence, _, _) = gt
            rl_gt = generate_result_line(
                motif, predicted, confidence, len(anns_spanning), len(anns_flanking), len(anns_filtered), module_number,
                qual_annot=anns_spanning
            )
            rls.append(rl_gt)

            if ph is not None:
                (module_number, anns_2good, anns_1good, anns_0good, phasing1, supp_reads, prev_module_num) = ph
                rl_ph = generate_result_line(
                    motif, phasing1, supp_reads, len(anns_2good), len(anns_1good), len(anns_0good), prev_module_num,
                    second_module_number=module_number
                )
                rls.append(rl_ph)

        all_result_lines.extend(rls)

    variants_df = pd.DataFrame.from_records(all_result_lines)
    variants_df.sort_values(by=['motif_name'], kind='stable')
    return variants_df


def load_arguments() -> Namespace:
    """
    Loads and parses the arguments.
    :return: args - parsed arguments
    """
    parser = ArgumentParser(
        formatter_class=RawDescriptionHelpFormatter,
        description=textwrap.dedent(DANTE_DESCRIPTION)
    )

    options = parser.add_argument_group('Options')
    options.add_argument(
        '--input-tsv', '-i', type=valid_input_tsv, default="",
        help='Input annotation table as obtained by remaSTR. Default=stdin'
    )
    options.add_argument(
        '--output-dir', '-o', type=str, default="dante_out",
        help='Output destination (directory). Default=./dante_out/'
    )
    options.add_argument(
        '--verbose', '-v', action='store_true',
        help='Print all the outputs. Default is to print only the result table to stdout.'
    )
    options.add_argument(
        '--male', action='store_true',
        help='Indicate that the sample is male. Process motifs from chrX/chrY as mono-allelic.'
    )
    options.add_argument(
        '--nomenclatures', '-n', type=positive_int, default=5,
        help='Number of nomenclature strings to add to reports. Default=5'
    )
    options.add_argument(
        '--cutoff-alignments', type=positive_int, default=20,
        help='How many bases to keep beyond annotated part. Default=20'
    )

    args = parser.parse_args()

    return args


def valid_input_tsv(value: str) -> TextIO:
    if value == "":
        return sys.stdin
    return open(value, "r")


def positive_int(value: str) -> int:
    try:
        int_value = int(value)
    except ValueError:
        raise ArgumentTypeError(f'Value {value} is not integer') from None
    if int_value < 0:
        raise ArgumentTypeError(f'Value {value} is negative')
    return int_value


class Motif:
    """
    Class to represent DNA motifs.

    :ivar chrom: Chromosome name.
    :ivar start: Start position of the motif.
    :ivar end: End position of the motif.
    :ivar modules: A list of tuples containing sequence and repetition count.
    :ivar name: name of the Motif
    :ivar motif: motif nomenclature
    """

    def __init__(self, motif: str, name: str | None = None) -> None:
        """
        Initialize a Motif object.
        :param motif: The motif string in the format "chrom:start_end[A][B]..."
        :param name: optional name of the motif
        """
        # remove whitespace
        nomenclature = motif.strip().replace(' ', '')
        name = (name if name is not None else nomenclature).replace(':', '-').replace('.', '_').replace('/', '_')

        # extract prefix, first number, second number
        tmp = re.match(r'([^:]+):g\.(\d+)_(\d+)(.*)', nomenclature)
        if tmp is None:
            raise ValueError(f"{nomenclature} has incorrect format")
        chrom, start, end, remainder = tmp.groups()

        # extract sequence and repetition count
        modules = [(str(seq), int(num)) for seq, num in re.findall(r'([A-Z]+)\[(\d+)', remainder)]
        modules = [('left_flank', 1)] + modules + [('right_flank', 1)]

        # store members
        self.nomenclature: str = nomenclature
        self.name: str = name
        self.chrom: str = chrom
        self.start: int = int(start)
        self.end: int = int(end)
        self.modules: list[tuple[str, int]] = modules

    def __getitem__(self, index: int) -> tuple[str, int]:
        """
        Returns module at given index.
        :param index: The index of the module to fetch.
        :return: The module at the given index.
        """
        return self.modules[index]

    def __str__(self) -> str:
        """
        Returns string representation of the Motif object.
        :return: String representation in the format "chrom:start_end[A][B]..."
        """
        return f'{self.chrom}:g.{self.start}_{self.end}' + self.modules_str(include_flanks=False)

    def __lt__(self, obj):
        """
        Less than for sorting purposes.
        :return: bool - if this object comes before the other
        """
        return self.name < obj.name

    def __eq__(self, obj):
        """
        Equal to for sorting purposes.
        :return: bool - if this object is equal to the other
        """
        return self.name == obj.name

    def augmented_nomenclature(self, rep_counts: list[str]) -> str:
        modules = []
        i = 0
        for seq, num in self.modules[1:-1]:
            if num == 1:
                modules.append(f"{seq}[{num}]")
            else:
                modules.append(f"{seq}[{rep_counts[i]}]")
                i += 1
        assert i == len(rep_counts), "Invalid augmentation."
        return f'{self.chrom}:g.{self.start}_{self.end}' + "".join(modules)

    def modules_str(self, include_flanks: bool = False) -> str:
        """
        Returns string representation of modules
        :param include_flanks: bool - include flank modules?
        :return: String representation of modules
        """
        if include_flanks:
            return ''.join([f'{seq}[{num}]' for seq, num in self.modules])
        return ''.join([f'{seq}[{num}]' for seq, num in self.modules[1:-1]])

    def module_str(self, module_number: int) -> str:
        """
        Returns string representation of modules
        :param module_number: int - module number
        :return: String representation of modules
        """
        seq, num = self.modules[module_number]
        return f'{seq}[{num}]'

    def dir_name(self) -> str:
        """
        Returns possible directory name of the motif.
        :return: str - directory name for the motif
        """
        return self.name

    def get_repeating_modules(self) -> list[tuple[int, str, int]]:
        """
        Returns list of modules with more than one repetition.
        :return: List of tuples containing index, sequence, and repetition count.
        """
        return [(int(i), str(seq), int(num)) for i, (seq, num) in enumerate(self.modules) if num > 1]

    def get_location_subpart(self, index: int) -> tuple[int, int]:
        """
        Returns the chromosome location of a subpart of a motive
        :param index: int - index of a module
        :return: start and end location of the subpart
        """
        start = self.start
        for module in self.modules[1: index]:
            seq, rep = module
            start += len(seq) * rep

        return start, start + len(self.modules[index][0]) * self.modules[index][1]


class Annotation:
    """
    Encapsulate sequence of states from HMM and provide its readable representation and filters
    """

    def __init__(
        self, read_id: str, mate_order: int, read_seq: str, expected_seq: str,
        states: str, probability: float, motif: Motif
    ):
        """
        :param read_id: str - read ID
        :param read_seq: str - read sequence
        :param mate_order: int - mate order (0 - unpaired, 1 - left pair, 2 - right pair)
        :param expected_seq: str - expected sequence as in motif
        :param states: str - sequence of states (numbers of modules)
        :param probability: Probability of generating sequence by the most likely sequence of HMM states
        :param motif: Sequence of tuples (sequence, repeats) as specified by user
        """

        # Store arguments into instance variables
        self.read_id = read_id
        self.mate_order = mate_order
        self.read_seq = read_seq
        self.expected_seq = expected_seq
        self.states = states
        self.probability = probability
        self.motif = motif  # TODO: remove this, it looks like unnecessary dependency
        self.n_modules = len(motif.modules)

        # Calculate insertion/deletion/mismatch string
        self.mismatches_string = self.__get_errors()

        # Calculate number of insertions, deletions and normal bases
        self.n_insertions = self.mismatches_string.count('I')
        self.n_deletions = self.mismatches_string.count('D')
        self.n_mismatches = self.mismatches_string.count('M')

        # Number of STR motif repetitions and sequences of modules
        self.module_bases = self.__get_bases_per_module()
        self.module_repetitions = self.__get_module_repetitions()
        self.module_sequences = self.__get_module_sequences()

        # get left flank length
        self.left_flank_len = self.__get_left_flank()

    def __str__(self) -> str:
        """
        Return the annotation.
        :return: str - annotation
        """
        return '\n'.join([f'{self.read_id} {str(self.module_bases)} {str(self.module_repetitions)}', self.read_seq,
                          self.expected_seq, self.states, self.mismatches_string])

    def __get_errors(self) -> str:
        """
        Count errors in annotation and the error line.
        :return: str - error line
        """
        err_line = []
        for exp, read in zip(self.expected_seq.upper(), self.read_seq.upper()):
            if exp == '-' or read in BASE_MAPPING.get(exp, ''):
                err_line.append('_')
            elif read == '_':
                err_line.append('D')
            elif exp == '_':
                err_line.append('I')
            else:
                err_line.append('M')

        return ''.join(err_line)

    def __get_bases_per_module(self) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of bases of input sequence that were generated by the module
        :return: Number of bases generated by each module
        """
        # Count the module states
        return tuple(self.states.count(chr(ord('0') + i)) for i in range(self.n_modules))

    def __get_left_flank(self) -> int:
        """
        Get length of a left flank.
        :return: int - number of bases of left flank before module '0' (usually module '0' is still left flank)
        """
        for i, state in enumerate(self.states):
            if state != '-':
                return i
        return len(self.states)

    def __get_module_repetitions(self) -> tuple[int, ...]:
        """
        List of integers, each value corresponds to number of repetitions of module in annotation
        :return: Number of repetitions generated by each module
        """
        # Count the module states
        repetitions = self.__get_bases_per_module()

        # Divide by the module length where applicable
        # TODO: this is not right for grey ones, where only closed ones should be counted, so round is not right.
        return tuple(
            1 if reps == 1 and cnt > 0 else round(cnt / len(seq))
            for (seq, reps), cnt in zip(self.motif.modules, repetitions)
        )

    def __get_module_sequences(self) -> tuple[str, ...]:
        """
        List of sequences, each per module
        :return: list(str)
        """
        sequences = [''] * self.n_modules
        for i in range(self.n_modules):
            state_char = chr(ord('0') + i)
            first = self.states.find(state_char)
            if first > -1:
                last = self.states.rfind(state_char)
                sequences[i] = self.read_seq[first:last + 1]
        return tuple(sequences)

    def get_module_errors(self, module_num: int, overhang: int | None = None) -> tuple[int, int, int]:
        """
        Get the number of insertions and deletions or mismatches in a certain module.
        If overhang is specified, look at specified number of bases around the module as well.
        :param module_num: int - 0-based module number to count errors
        :param overhang: int - how long to look beyond module, if None, one length of STR module
        :return: int, int, int - number of insertions and deletions, mismatches, length of the interval
        """
        # get overhang as module length
        if overhang is None:
            seq, _ = self.motif.modules[module_num]
            overhang = len(seq)

        # define module character
        char_to_search = chr(ord('0') + module_num)

        # if the annotation does not have this module, return 0
        if char_to_search not in self.states:
            return 0, 0, 0

        # search for the annotation of the module
        start = max(0, self.states.find(char_to_search) - overhang)
        end = min(self.states.rfind(char_to_search) + overhang + 1, len(self.states))

        # count errors
        indels = self.mismatches_string[start:end].count('I') + self.mismatches_string[start:end].count('D')
        mismatches = self.mismatches_string[start:end].count('M')

        # return indels, mismatches, and length
        return indels, mismatches, end - start

    def has_less_errors(self, max_errors: float | int | None, relative=False) -> bool:
        """
        Check if this annotation has fewer errors than max_errors.
        Make it relative to the annotated length if relative is set.
        :param max_errors: int/float - number of max_errors (relative if relative is set)
        :param relative: bool - if the errors are relative to the annotated length
        :return: bool - True if the number of errors is less than allowed
        """
        errors = self.n_deletions + self.n_insertions + self.n_mismatches

        if max_errors is None or errors == 0:
            return True

        if relative:
            return errors / float(sum(self.module_bases)) <= max_errors
        return errors <= max_errors

    def primers(self, index_rep: int) -> int:
        """
        Count how any primers it has on repetition index.
        :param index_rep: int - index of the repetition, that we are looking at
        :return: int - number of primers (0-2)
        """
        primers = 0
        if index_rep > 0 and self.module_repetitions[index_rep - 1] > 0:
            primers += 1
        if index_rep + 1 < len(self.module_repetitions) and self.module_repetitions[index_rep + 1] > 0:
            primers += 1
        return primers

    def is_annotated_right(self) -> bool:
        """
        Is it annotated in a way that it is interesting?
        More than one module annotated + modules are not missing in the middle.
        :return: bool - annotated right?
        """

        # remove those that starts/ends in background but don't have a neighbour module
        starts_background = self.states[0] in '_-'
        ends_background = self.states[-1] in '_-'
        if starts_background and self.module_repetitions[0] == 0:
            return False
        if ends_background and self.module_repetitions[-1] == 0:
            return False

        # remove those with jumping modules
        started = False
        ended = False
        for repetition in self.module_repetitions:
            if repetition > 0:
                started = True
                if ended:
                    return False
            if repetition == 0 and started:
                ended = True

        # pass?
        return True

    def get_str_repetitions(self, index_str: int) -> tuple[bool, int] | None:
        """
        Get the number of str repetitions for a particular index.
        :param index_str: int - index of a str
        :return: (bool, int) - closed?, number of str repetitions
        """
        if self.is_annotated_right():
            primer1 = index_str > 0 and self.module_repetitions[index_str - 1] > 0
            primer2 = index_str + 1 < len(self.module_repetitions) and self.module_repetitions[index_str + 1] > 0
            if primer1 or primer2:
                return primer1 and primer2, self.module_repetitions[index_str]
        return None

    @staticmethod
    def find_with_regex(read_sequence: str, motif_sequence: str, search_pos: int = 0) -> int:
        """
        Find the first occurrence of a motif sequence in the read sequence using regular expressions.
        :param read_sequence: The sequence to search in.
        :param motif_sequence: The motif sequence (as a regex) to search for.
        :param search_pos: The position to start the search from.
        :return: int - The start position of the first occurrence of the motif sequence. Returns -1 if not found.
        """
        # convert motif sequence to regex
        motif_regex = ''.join(BASE_MAPPING[char] for char in motif_sequence)

        # compile the regular expression pattern
        pattern = re.compile(motif_regex)

        # search for the pattern in the read sequence starting from search_pos
        match = pattern.search(read_sequence, search_pos)

        # return the start position if a match is found, else return -1
        return match.start() if match else -1

    def get_nomenclature(
        self, index_rep: int | None = None, index_rep2: int | None = None, include_flanking: bool = True
    ) -> str:
        """
        Get HGVS nomenclature.
        :param index_rep: int - index of the first repetition (None if include all)
        :param index_rep2: int - index of the second repetition (None if include all)
        :param include_flanking: boolean - include flanking regions (i.e. first and last module)
        :return: str - HGVS nomenclature string
        """
        # prepare data
        if index_rep is not None:
            if index_rep2 is not None:
                data = zip(
                    [self.module_repetitions[index_rep], self.module_repetitions[index_rep2]],
                    [self.motif[index_rep], self.motif[index_rep2]],
                    [self.module_sequences[index_rep], self.module_sequences[index_rep2]]
                )
            else:
                data = zip(
                    [self.module_repetitions[index_rep]],
                    [self.motif[index_rep]],
                    [self.module_sequences[index_rep]]
                )
        elif include_flanking:
            data = zip(self.module_repetitions, self.motif.modules, self.module_sequences)
        else:
            data = zip(self.module_repetitions[1:-1], self.motif.modules[1:-1], self.module_sequences[1:-1])

        # iterate and build the nomenclature string
        nomenclatures = []
        for repetitions, (motif_sequence, _), read_sequence in data:
            nomenclature = self.build_nomenclature_string(repetitions, motif_sequence, read_sequence)
            nomenclatures.append(nomenclature)

        return '\t'.join(nomenclatures)

    def build_nomenclature_string(self, repetitions, motif_sequence, read_sequence) -> str:
        nomenclature = ''
        if repetitions == 1:
            if len(read_sequence) > 0:
                nomenclature += f'{read_sequence}[1]'
            return nomenclature

        reps = 0
        search_pos = 0
        found_rep_seq = ''
        while True:
            search_found = self.find_with_regex(read_sequence, motif_sequence, search_pos)
            if search_found == search_pos:
                # setup current rep. sequence
                if reps == 0:
                    found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]

                if read_sequence[search_found:search_found + len(motif_sequence)] == found_rep_seq:
                    # regular continuation
                    reps += 1
                else:
                    # interruption, but in line with searched motif
                    nomenclature += f'{found_rep_seq}[{reps}]'
                    found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                    reps = 1
            elif search_found == -1:
                # the end, we did not find any other STRs
                if reps > 0:
                    nomenclature += f'{found_rep_seq}[{reps}]'
                if len(read_sequence[search_pos:]) > 0:
                    nomenclature += f'{read_sequence[search_pos:]}[1]'
                break
            else:
                # some interruption
                if reps > 0:
                    nomenclature += f'{found_rep_seq}[{reps}]'
                if len(read_sequence[search_pos:search_found]) > 0:
                    nomenclature += f'{read_sequence[search_pos:search_found]}[1]'
                found_rep_seq = read_sequence[search_found:search_found + len(motif_sequence)]
                reps = 1
            # update search pos and iterate
            search_pos = search_found + len(motif_sequence)
        return nomenclature

    def get_shortened_annotation(self, shorten_length: int) -> Annotation:
        """
        Get shortened annotation with specified shorten length beyond annotated modules.
        :param shorten_length: int - how many bases to keep beyond modules
        :return: Annotation - shortened annotation
        """

        # search for start
        start = -1
        for i in range(len(self.states)):
            if self.states[i] != '-':
                start = i
                break

        # search for end
        end = -1
        for i in range(len(self.states) - 1, -1, -1):
            if self.states[i] != '-':
                end = i
                break

        # adjust start and end for shorten length
        start = max(start - shorten_length, 0)
        end = min(end + 1 + shorten_length, len(self.states))  # +1 for use as list range

        # return shortened Annotation
        return Annotation(self.read_id, self.mate_order, self.read_seq[start:end], self.expected_seq[start:end],
                          self.states[start:end], self.probability, self.motif)


def errors_per_read(
    errors: list[tuple[int, int, int]], relative: bool = False
) -> tuple[float | str, float | str]:
    """
    Count number of errors per read. Relative per length or absolute number.
    :param errors: list[tuple[int, int, int]] - indels, mismatches and length of module
    :param relative: bool - relative?
    :return: tuple[float, float] - number of indels, mismatches per hundred reads
    """
    # if we have no reads, return '---'
    if len(errors) == 0:
        return '---', '---'

    if relative:
        mean_length = np.mean([length for _, _, length in errors])
        return (
            float(np.mean([indels / float(length) for indels, _, length in errors]) * mean_length),
            float(np.mean([mismatches / float(length) for _, mismatches, length in errors]) * mean_length)
        )
    return (
        float(np.mean([indels for indels, _, _ in errors])),
        float(np.mean([mismatches for _, mismatches, _ in errors]))
    )


def generate_result_line(
    motif: Motif, predicted: tuple[str | int, str | int], confidence: tuple[float | str, ...],
    qual_num: int, primer_num: int, filt_num: int, module_number: int,
    qual_annot: list[Annotation] | None = None,
    second_module_number: int | None = None
) -> dict:
    """
    Generate result line from the template string.
    :param motif_class: Motif - motif class
    :param predicted: tuple[str, str] - predicted alleles (number or 'B'/'E')
    :param confidence: tuple[7x float/str] - confidences of prediction
    :param qual_num: int - number of reads with both primers
    :param primer_num: int - number of reads with exactly one primer
    :param filt_num: int - number of filtered out reads (no primers, many errors, ...)
    :param module_number: int - module number in motif
    :param qual_annot: list[Annotation] - list of quality annotations for error and number of reads
    :param second_module_number: int/None - second module number in motif
    :return: dict - result dictionary
    """
    # setup motif info
    start, end = motif.get_location_subpart(module_number)
    motif_seq = motif.module_str(module_number)
    repetition_index: int | str = module_number
    if second_module_number is not None:
        _, end = motif.get_location_subpart(second_module_number)
        motif_seq = ','.join([motif.module_str(i) for i in range(module_number, second_module_number + 1)])
        repetition_index = f'{module_number}_{second_module_number}'

    reads_a1: int | str
    reads_a2: int | str
    indels_rel: float | str
    indels_rel1: float | str
    indels_rel2: float | str
    mismatches_rel: float | str
    mismatches_rel1: float | str
    mismatches_rel2: float | str
    # get info about errors and number of reads from quality annotations if provided
    reads_a1 = reads_a2 = '---'
    indels_rel = mismatches_rel = '---'
    indels_rel1 = mismatches_rel1 = '---'
    indels_rel2 = mismatches_rel2 = '---'
    if qual_annot is not None:
        # get info about number of reads
        a1 = int(predicted[0]) if isinstance(predicted[0], int) else None
        a2 = int(predicted[1]) if isinstance(predicted[1], int) else None
        reads_a1 = 0 if a1 is None else len([a for a in qual_annot if a.module_repetitions[module_number] == a1])
        reads_a2 = 0 if a2 is None else len([a for a in qual_annot if a.module_repetitions[module_number] == a2])

        # get info about errors
        errors = [a.get_module_errors(module_number) for a in qual_annot]
        errors_a1 = [a.get_module_errors(module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a1]
        errors_a2 = [a.get_module_errors(module_number) for a in qual_annot
                     if a.module_repetitions[module_number] == a2]
        assert len([l for i, m, l in errors if l == 0]) == 0

        # extract error metrics
        indels_rel, mismatches_rel = errors_per_read(errors, relative=True)
        indels_rel1, mismatches_rel1 = errors_per_read(errors_a1, relative=True)
        indels_rel2, mismatches_rel2 = errors_per_read(errors_a2, relative=True)

    return {
        'motif_name': motif.name, 'motif_nomenclature': motif.nomenclature, 'motif_sequence': motif_seq,
        'chromosome': motif.chrom, 'start': start, 'end': end,
        'allele1': predicted[0], 'allele2': predicted[1],
        'confidence': confidence[0], 'conf_allele1': confidence[1], 'conf_allele2': confidence[2],
        'reads_a1': reads_a1, 'reads_a2': reads_a2,
        'indels': indels_rel, 'indels_a1': indels_rel1, 'indels_a2': indels_rel2,
        'mismatches': mismatches_rel, 'mismatches_a1': mismatches_rel1, 'mismatches_a2': mismatches_rel2,
        'quality_reads': qual_num, 'one_primer_reads': primer_num, 'filtered_reads': filt_num,
        'conf_background': confidence[3] if len(confidence) > 3 else '---',
        'conf_background_all': confidence[4] if len(confidence) > 4 else '---',
        'conf_extended': confidence[5] if len(confidence) > 5 else '---',
        'conf_extended_all': confidence[6] if len(confidence) > 6 else '---',
        'repetition_index': repetition_index
    }


def create_motif(df: pd.DataFrame) -> Motif:
    motif_str = df[MOTIF_COLUMN_NAME].iloc[0]
    name = None if 'name' not in df.columns or df.iloc[0]['name'] in ['None', ''] else df.iloc[0]['name']
    motif_class = Motif(motif_str, name)
    return motif_class


def create_annotations(df: pd.DataFrame, motif: Motif) -> list[Annotation]:
    annotations: list[Annotation] = []
    for _, row in df.iterrows():
        ann = Annotation(
            row['read_id'], row['mate_order'], row['read'], row['reference'],
            row['modules'], row['log_likelihood'], motif
        )
        annotations.append(ann)
    return annotations


def genotype_group(
    motif: Motif, monoallelic: bool,  # this could be included in Motif
    annotations: list[Annotation]  # I would prefer to get dataframe here
) -> list[GenotypeInfo]:
    result_genotypes = []

    postfilter = PostFilter()
    read_distribution = np.bincount([len(ann.read_seq) for ann in annotations], minlength=100)
    for curr_module_num, _, _ in motif.get_repeating_modules():

        anns_spanning, rest = postfilter.get_filtered(annotations, curr_module_num, both_primers=True)
        anns_flanking, anns_filtered = postfilter.get_filtered(rest, curr_module_num, both_primers=False)
        del rest

        model = Inference(read_distribution, None)
        lh_array, predicted, confidence = model.genotype(anns_spanning, anns_flanking, curr_module_num, monoallelic)
        result_genotypes.append(
            (curr_module_num, anns_spanning, anns_flanking, anns_filtered, predicted, confidence, lh_array, model)
        )

    return result_genotypes


def phase_group(motif: Motif, annotations: list[Annotation]) -> list[None | tuple]:
    result_phasing: list[None | tuple] = [None]

    postfilter = PostFilter()
    repeating_modules = motif.get_repeating_modules()
    for i, (curr_module_num, _, _) in enumerate(repeating_modules[1:], start=1):
        prev_module_num = repeating_modules[i - 1][0]
        mod_nums = [prev_module_num, curr_module_num]

        anns_2good, _filtered = postfilter.get_filtered_list(annotations, mod_nums, both_primers=[True, True])
        _left_good, _left_bad = postfilter.get_filtered_list(_filtered, mod_nums, both_primers=[False, True])
        _right_good, anns_0good = postfilter.get_filtered_list(_left_bad, mod_nums, both_primers=[True, False])
        anns_1good = _left_good + _right_good
        del _filtered, _left_good, _left_bad, _right_good

        phasing, supp_reads = phase(anns_2good, prev_module_num, curr_module_num)

        result_phasing.append(
            (curr_module_num, anns_2good, anns_1good, anns_0good, phasing, supp_reads, prev_module_num)
        )

    return result_phasing


def phase(
    annotations: list[Annotation], module_number1: int, module_number2: int
) -> tuple[tuple[str, str], tuple[str, str, str]]:
    """
    Infer phasing based on the Annotations.
    :param annotations: list(Annotation) - good (blue) annotations
    :param module_number1: int - index of a repetition
    :param module_number2: int - index of the second repetition
    :return: tuple - predicted symbols and confidences
    """
    # resolve trivial case
    if len(annotations) == 0:
        return ('-|-', '-|-'), ('-/0', '-/0', '-/0')

    # gather module repetitions from annotations and count them
    repetitions = Counter([
        (ann.module_repetitions[module_number1], ann.module_repetitions[module_number2]) for ann in annotations
    ])

    # pick the highest two
    most_common = repetitions.most_common(2)
    rep1, cnt1 = most_common[0]
    rep2, cnt2 = most_common[1] if len(most_common) >= 2 else (('-', '-'), 0)

    # output phasing with number of supported reads
    phasing = (f'{rep1[0]}|{rep1[1]}', f'{rep2[0]}|{rep2[1]}')
    supported_reads = (f'{cnt1 + cnt2}/{len(annotations)}', f'{cnt1}/{len(annotations)}', f'{cnt2}/{len(annotations)}')
    return phasing, supported_reads


def generate_groups(input_stream: TextIO, chunk_size: int = 1000000) -> Iterator[pd.DataFrame]:
    """
    Generate sub-parts of the input table according to "column_name".
    Able to process even large files.
    :param input_stream: TextIO - input stream
    :param chunk_size: int - chunk size for table processing
    :return: Iterator[pd.DataFrame] - sub-parts of the input table
    """
    column_name = MOTIF_COLUMN_NAME
    # initialize reading
    current_group_data = pd.DataFrame()

    # read the output of remaSTR into annotations
    for chunk in pd.read_csv(input_stream, sep='\t', chunksize=chunk_size, iterator=True, quoting=csv.QUOTE_NONE):

        # identify the unique groups in the chunk
        unique_groups = chunk[column_name].unique()

        # got through the groups
        for group in unique_groups:

            # filter rows for the current group from the chunk
            group_data = chunk[chunk[column_name] == group]

            # if this group is a continuation of the previous group from the last chunk
            if not current_group_data.empty and current_group_data[column_name].iloc[0] == group:
                current_group_data = pd.concat([current_group_data, group_data])
                continue  # Move to the next group in the current chunk

            # if there's data in current_group_data, process and empty it
            if not current_group_data.empty:
                yield current_group_data
                current_group_data = pd.DataFrame()  # reset

            # check if the group is at end the chunk (probably continues into next one)
            if group == unique_groups[-1]:
                current_group_data = group_data
            else:
                # process the group as normally
                yield group_data

    # process the last group
    if not current_group_data.empty:
        yield current_group_data


def normalize_ref_alt(ref: str, alt: str) -> tuple[str, str]:
    suffix = 0
    it = zip(reversed(ref), reversed(alt))
    for _ in range(min(len(ref), len(alt)) - 1):
        (a, b) = next(it)
        if a == b:
            suffix += 1

    return (ref[:len(ref) - suffix], alt[:len(alt) - suffix])


def make_vcf_line(
    chrom: str, pos: str, unit: str, ref_copies: str, alt_copies: str, genotype: str,
    lines: list[str]
):
    if ref_copies == alt_copies:
        return  # they are the same, there is no variant
    if "|" in alt_copies:
        return  # skip over phased variants, TODO: later

    ref_seq = unit * int(ref_copies)
    if alt_copies == "B":
        info = f"REF={ref_copies};RU={unit};BG;END={pos}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq, "<BG>", ".", "PASS", info, "GT", genotype
        ))
    elif alt_copies == "E":
        info = f"REF={ref_copies};RU={unit};EXP;END={pos}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq[0], "<EXP>", ".", "PASS", info, "GT", genotype
        ))
    else:
        alt_seq = unit * int(alt_copies)

        svlen = len(alt_seq) - len(ref_seq)
        svtype = "INS" if svlen > 0 else "DEL"
        (ref_seq, alt_seq) = normalize_ref_alt(ref_seq, alt_seq)

        info = f"REF={ref_copies};RU={unit};SVLEN={svlen};SVTYPE={svtype}"
        lines.append("{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\n".format(
            chrom, pos, ".", ref_seq, alt_seq, ".", "PASS", info, "GT", genotype
        ))


def write_vcf(df: pd.DataFrame, out: str) -> None:
    lines = []
    lines.append('##fileformat=VCFv4.1\n')
    lines.append('##ALT=<ID=BG,Description="Background">\n')
    lines.append('##ALT=<ID=EXP,Description="Expansion of unknown (large) size">\n')
    lines.append('##FILTER=<ID=PASS,Description="All filters passed">\n')
    lines.append('##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">\n')
    lines.append('##INFO=<ID=END,Number=1,Type=Integer,Description="End position of the variant">\n')
    lines.append('##INFO=<ID=BG,Number=0,Type=Flag,Description="Background variant">\n')
    lines.append('##INFO=<ID=EXP,Number=0,Type=Flag,Description="Expansion variant">\n')
    lines.append('##INFO=<ID=REF,Number=1,Type=Integer,Description="Reference copy number">\n')
    lines.append('##INFO=<ID=RU,Number=1,Type=String,Description="Repeat unit in ref orientation">\n')
    lines.append('##INFO=<ID=SVLEN,Number=1,Type=Integer,Description="Alt length - Ref length">\n')
    lines.append('##INFO=<ID=SVTYPE,Number=1,Type=String,Description="Type of structural variant">\n')
    lines.append('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tsample\n')

    records: list[str] = []
    for _, row in df.iterrows():
        m1 = re.match(r"([A-Z]+)\[([0-9]+)\]", row["motif_sequence"])
        if m1 is None:
            print(f"{row['motif_sequence']} returned None")
            continue
        unit, copies = m1.groups()
        allele1 = str(row["allele1"])
        allele2 = str(row["allele2"])

        if allele1 == allele2:
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele1"]), "1/1", records)
        else:
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele1"]), "1/.", records)
            make_vcf_line(row["chromosome"], row["start"], unit, copies, str(row["allele2"]), "./1", records)

    records.sort(key=chr_and_pos)
    with open(f"{out}/variants.vcf", "w") as f:
        f.writelines(lines + records)


def chr_and_pos(line: str) -> tuple[int, int]:
    m1 = re.match(r"(chr[0-9XY]+)\t([0-9]+)\t.*", line)
    if m1 is None:
        raise ValueError(f"got {line}")
    chrom, pos = m1.groups()
    chrom2: int = {
        "chr1":  1,  "chr2":  2,  "chr3":  3,  "chr4":  4,  "chr5": 5,   "chr6": 6,
        "chr7":  7,  "chr8":  8,  "chr9":  9,  "chr10": 10, "chr11": 11, "chr12": 12,
        "chr13": 13, "chr14": 14, "chr15": 15, "chr16": 16, "chr17": 17, "chr18": 18,
        "chr19": 19, "chr20": 20, "chr21": 21, "chr22": 22, "chrX": 23,  "chrY": 24
    }[chrom]
    return (chrom2, int(pos))


class PostFilter:
    """
    Class that encapsulates post-filtering.
    """

    def __init__(self):
        self.min_flank_len = MIN_FLANK_LEN
        self.min_rep_len = MIN_REP_LEN
        self.min_rep_cnt = MIN_REP_CNT
        self.max_rel_error = MAX_REL_ERROR
        self.max_abs_error = MAX_ABS_ERROR

    def quality_annotation(self, ann: Annotation, module_number: int, both_primers: bool = True) -> bool:
        """
        Is this annotation good?
        :param ann: Annotation - annotation to be evaluated
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: bool - quality annotation?
        """
        is_right = ann.is_annotated_right()

        primers = ann.primers(module_number)
        has_primers = primers == 2 if both_primers else primers >= 1

        has_less_errors = (
            ann.has_less_errors(self.max_rel_error, relative=True)
            and ann.has_less_errors(self.max_abs_error, relative=False)
        )

        left_flank = sum(ann.module_bases[module_number + 1:]) >= self.min_flank_len
        right_flank = sum(ann.module_bases[:module_number]) >= self.min_flank_len
        has_flanks = left_flank and right_flank if both_primers else left_flank or right_flank

        has_repetitions = (
            ann.module_bases[module_number] >= self.min_rep_len
            and ann.module_repetitions[module_number] >= self.min_rep_cnt
        )

        _seq, reps = ann.motif.modules[module_number]

        return is_right and has_primers and has_less_errors and has_flanks and (has_repetitions or reps == 1)

    def get_filtered_list(
        self, annotations: list[Annotation], module_number: list[int], both_primers: list[bool] | None = None
    ) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations (list of modules).
        :param annotations: list(Annotation) - annotations
        :param module_number: list(int) - module numbers
        :param both_primers: list(bool) or None - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # adjust input if needed
        if both_primers is None:
            both_primers = [True] * len(module_number)
        assert len(both_primers) == len(module_number)

        # filter annotations
        quality_annotations = [
            an for an in annotations
            if all((
                self.quality_annotation(an, mn, both_primers=bp) for mn, bp in zip(module_number, both_primers)
            ))
        ]
        filtered_annotations = [an for an in annotations if an not in quality_annotations]

        return quality_annotations, filtered_annotations

    # TODO: is this just a specialized version of the previous method? Do we need this?
    def get_filtered(
        self, annotations: list[Annotation], module_number: int, both_primers: bool = True
    ) -> tuple[list[Annotation], list[Annotation]]:
        """
        Get filtered annotations.
        :param annotations: list(Annotation) - annotations
        :param module_number: int - module number
        :param both_primers: bool - do we require both primers to be present
        :return: list(Annotation), list(Annotation) - quality annotations, non-quality annotations
        """
        # pick quality annotations
        quality_annotations = []
        filtered_annotations = []
        for an in annotations:
            if self.quality_annotation(an, module_number, both_primers):
                quality_annotations.append(an)
            else:
                filtered_annotations.append(an)

        return quality_annotations, filtered_annotations


class ChromEnum(enum.Enum):
    X = 'X'
    Y = 'Y'
    NORM = 'NORM'


def chrom_from_string(chrom_str: str) -> ChromEnum:
    """
    Converts a string to a ChromEnum object.
    :param chrom_str: str - the string to convert to a ChromEnum object
    :return ChromEnum - enum object representing the chromosome
    """
    return (
        ChromEnum.X if chrom_str in ['chrX', 'NC_000023'] else
        ChromEnum.Y if chrom_str in ['chrY', 'NC_000024'] else
        ChromEnum.NORM
    )


def move_right(alignment: str, start: int, end: int) -> str:
    """
    Shift first part of the alignment to the right.
    :param alignment: str - alignment of the read
    :param start: int - start idx for shift
    :param end: int - one after end idx for a shift
    :return: str - alignment, where first part is shifted to right
    """
    align_part = alignment[start:end]
    # find last empty:
    idx = 0
    for idx in reversed(range(len(align_part))):
        if align_part[idx] != '_':
            break
    idx += 1

    # return shifted alignment
    return alignment[:start] + ('_' * (len(align_part) - idx)) + align_part[:idx] + alignment[end:]


def get_range(states, symbol: str) -> tuple[int, int]:
    """
    Get range of a module
    :param symbol: str - symbol for state to get range for
    :return: int, int - start and (one after) end range coordinates
    """
    try:
        first_idx = states.index(symbol)
        last_idx = len(states) - states[-1::-1].index(symbol) - 1
        return first_idx, last_idx + 1
    except ValueError:
        return -1, -1


def get_left_flank(states) -> int:
    """
    Get range of a left flank.
    :return: int - (one after) nd of the left flank before module '0'
    """
    for i, state in enumerate(states):
        if state != '-':
            return i
    return -1


def select_annotation(annotations, allele, allele2, index_rep, index_rep2):
    if allele is None:
        return annotations
    if allele2 is None or index_rep2 is None:
        return [a for a in annotations if a.module_repetitions[index_rep] == allele]

    return [a for a in annotations if a.module_repetitions[index_rep] == allele and a.module_repetitions[index_rep2] == allele2]


def setup_alignments(annotations: list[Annotation]) -> tuple[list[str], list[str]]:
    alignments = [''] * len(annotations)  # alignment strings
    align_inds = np.zeros(len(annotations), dtype=int)  # indices of annotations that were processed
    states = []  # has numbers of states in the final multiple alignment

    while True:
        # get minimal state:
        min_comp = (True, 'Z', -1)
        total_done = 0
        for i, (annot, ai) in enumerate(zip(annotations, align_inds)):
            if ai >= len(annot.states):
                total_done += 1
                continue
            state = annot.states[ai]
            comparator = (state != 'I', state, i)
            min_comp = min(comparator, min_comp)

        # if we have done every state, end:
        if total_done >= len(alignments):
            break

        states.append(min_comp[1])

        # now print all states, that are minimal:
        for i, (annot, ai) in enumerate(zip(annotations, align_inds)):
            if ai >= len(annot.states):
                alignments[i] += '_'  # put ends of each alignment to be of same length
                continue
            if annot.states[ai] == min_comp[1]:
                alignments[i] += annot.read_seq[ai]
                align_inds[i] += 1
            else:
                alignments[i] += '_'

    return alignments, states


def write_alignment(
    annotations: list[Annotation],
    index_rep: int, allele: int | None = None, index_rep2: int | None = None, allele2: int | None = None,
    cutoff_after: int | None = None, right_align: bool = False
) -> str:
    """
    Creates a multi-alignment of all annotations into output text file
    :param annotations: list(Annotation) - annotated reads
    :param index_rep: int - index of repetition module of a motif
    :param allele: int/None - which allele to print only, if None print all of them
    :param index_rep2: int - index of second repetition module of a motif
    :param allele2: int/None - which allele2 to print only, if None print all of them
    :param zip_it: bool - whether to gzip the resulting file
    :param right_align: bool - whether we deal with right alignment file
    :param cutoff_after: int - how many bases to keep outside the annotated motif (None for keep all)
    """
    annotations = select_annotation(annotations, allele, allele2, index_rep, index_rep2)

    # apply cutoff
    if cutoff_after is not None:
        annotations = [a.get_shortened_annotation(cutoff_after) for a in annotations]

    # setup alignments
    alignments, states = setup_alignments(annotations)

    # sort according to motif count:
    left_flank = np.array([-(ann.module_bases[0] + ann.left_flank_len) for ann in annotations])
    left_flank_exist = np.array([-(ann.module_repetitions[0]) if not right_align else 0 for ann in annotations])
    if index_rep2 is not None:
        # sorting first with 1st allele then with second
        reps1 = np.array([-ann.module_bases[index_rep] for ann in annotations])
        reps2 = np.array([-ann.module_bases[index_rep2] for ann in annotations])
        # sort by existence of left flank, first allele, second, left flank len.
        sort_inds = np.lexsort((left_flank, reps2, reps1, left_flank_exist))
    else:
        reps = np.array([-ann.module_bases[index_rep] for ann in annotations])
        sort_inds = np.lexsort((left_flank, reps, left_flank_exist))
    annotations = np.array(annotations)[sort_inds]
    alignments = list(np.array(alignments)[sort_inds])

    # for every alignment, shift the left flank right
    end = get_left_flank(states)
    if end != -1:
        for i in range(len(alignments)):
            alignments[i] = move_right(alignments[i], 0, end)

    # for every alignment, shift the first module right
    start0, end0 = get_range(states, '0')
    if start0 != -1:
        for i in range(len(alignments)):
            alignments[i] = move_right(alignments[i], start0, end0)

    # in addition, those that have only '_' in state '0' (missing left flank), shift right also '1' state
    start1, end1 = get_range(states, '1')
    first_zero_idx = len(alignments)
    for i in range(len(alignments)):
        if start1 != -1 and (start0 == -1 or alignments[i][start0:end0].count('_') == end0 - start0 or right_align):
            first_zero_idx = min(first_zero_idx, i)
            alignments[i] = move_right(alignments[i], start1, end1)

    # add empty line if we have some alignments without left flank
    annot_names = [annot.read_id for annot in annotations]
    if first_zero_idx != len(alignments) and not right_align:
        alignments = alignments[:first_zero_idx] + ['_' * len(alignments[0])] + alignments[first_zero_idx:]
        annot_names = annot_names[:first_zero_idx] + ['empty_line'] + annot_names[first_zero_idx:]

    string_tmp = []
    for annot_name, align in zip(annot_names, alignments):
        string_tmp.append(f">{annot_name}\n{align}\n")
    string = "".join(string_tmp)

    return string


def sorted_repetitions(annotations: list[Annotation]) -> list[tuple[tuple[int, ...], int]]:
    """
    Aggregate same repetition counts for annotations and sort them according to quantity of repetitions of each module
    :param annotations: Annotated reads
    :return: list of (repetitions, count), sorted by repetitions
    """
    count_dict = Counter(tuple(annot.module_repetitions) for annot in annotations)
    return sorted(count_dict.items(), key=lambda k: k[0])


def write_histogram_image2d(
    deduplicated: list[Annotation], index_rep: int, index_rep2: int, seq: str, seq2: str
) -> Hist2DGraph | None:
    if deduplicated is None or len(deduplicated) == 0:
        return None

    dedup_reps: list[tuple[tuple[bool, int], tuple[bool, int]]] = []
    for x in deduplicated:
        r_1 = x.get_str_repetitions(index_rep)
        r_2 = x.get_str_repetitions(index_rep2)
        if r_1 is not None and r_2 is not None:
            dedup_reps.append((r_1, r_2))

    if len(dedup_reps) == 0:
        return None

    # assign maximals
    xm = max(r for (_, r), _ in dedup_reps)
    ym = max(r for _, (_, r) in dedup_reps)
    max_ticks = max(ym, xm) + 2
    xm = max(MAX_REPETITIONS, xm)
    ym = max(MAX_REPETITIONS, ym)

    # create data containers
    data = np.zeros((xm + 1, ym + 1), dtype=int)
    data_primer = np.zeros((xm + 1, ym + 1), dtype=int)
    for ((c1, r1), (c2, r2)) in dedup_reps:
        if c1 and c2:
            data[r1, r2] += 1
        if c1 and not c2:
            data_primer[r1, r2:] += 1
        if not c1 and c2:
            data_primer[r1:, r2] += 1

    str1 = 'STR %d [%s]' % (index_rep + 1, seq.split('-')[-1])
    str2 = 'STR %d [%s]' % (index_rep2 + 1, seq2.split('-')[-1])

    def parse_labels(num, num_primer):
        if num == 0 and num_primer == 0:
            return ''
        if num == 0 and num_primer != 0:
            return '0/%s' % str(num_primer)
        if num != 0 and num_primer == 0:
            return '%s/0' % str(num)
        return '%s/%s' % (str(num), str(num_primer))

    z_partial = data_primer[:max_ticks, :max_ticks]
    z_full = data[:max_ticks, :max_ticks]
    text = [[parse_labels(z_full[i, j], z_partial[i, j]) for j in range(z_full.shape[1])] for i in range(z_full.shape[0])]

    z_partial_out: list[list[int]] = z_partial.tolist()
    z_full_out: list[list[int]] = z_full.tolist()
    return (z_partial_out, z_full_out, text, str1, str2)


def write_histogram_image(
    annotations: list[Annotation], filt_annot: list[Annotation], index_rep: int
) -> HistReadCounts:
    """
    Stores quantity of different combinations of module repetitions, generates separate graph image for each module
    :param out_prefix: Output file prefix
    :param annotations: Annotated reads.
    :param filt_annot: Annotated reads (filtered)
    :param index_rep: int - index of repetition module of a motif
    """
    repetitions = sorted_repetitions(annotations)
    repetitions_filt = sorted_repetitions(filt_annot)

    spanning_counts = [(r[index_rep], c) for r, c in repetitions]
    filtered_counts = [(r[index_rep], c) for r, c in repetitions_filt]
    inread_counts: list[tuple] = []

    xm = max(
        [r for r, c in spanning_counts]
        + [r for r, c in filtered_counts]
        + [r for r, c in inread_counts]
        + [MAX_REPETITIONS]
    )

    # set data
    spanning = [0] * (xm + 1)
    for r, c in spanning_counts:
        spanning[r] += c

    flanking = spanning.copy()
    for r, c in filtered_counts:
        flanking[r] += c

    inread = flanking.copy()
    for r, c in inread_counts:
        inread[r] += c

    # plot_histogram_image_plotly(out_prefix, spanning, flanking, inread)

    only_flanking = [df - d for df, d in zip(flanking, spanning)]
    only_inread = [di - df for di, df in zip(inread, flanking)]
    return spanning, only_flanking, only_inread


def combine_distribs(deletes, inserts):
    """
    Combine insert and delete models/distributions
    :param deletes: ndarray - delete distribution
    :param inserts: ndarray - insert distribution
    :return: ndarray - combined array of the same length
    """
    # how much to fill?
    to_fill = sum(deletes == 0.0) + 1
    while to_fill < len(inserts) and inserts[to_fill] > 0.0001:
        to_fill += 1

    # create the end array
    end_distr = np.zeros_like(deletes, dtype=float)

    # fill it!
    for i, a in enumerate(inserts[:to_fill]):
        end_distr[i:] += (deletes * a)[:len(deletes) - i]

    return end_distr


def const_rate(_n, p1=0.0, _p2=1.0, _p3=1.0):
    return p1


def linear_rate(n, p1=0.0, p2=1.0, _p3=1.0):
    return p1 + p2 * n


def quadratic_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * n + p3 * n * n


def exp_rate(n, p1=0.0, p2=1.0, p3=1.0):
    return p1 + p2 * math.exp(p3 * n)


def clip(value, minimal, maximal):
    """
    Clips value to range <minimal, maximal>
    :param value: ? - value
    :param minimal: ? - minimal value
    :param maximal: ? - maximal value
    :return: ? - clipped value
    """
    return min(max(minimal, value), maximal)


def model_full(rng, model_params, n, rate_func=linear_rate):
    """
    Create binomial model for both deletes and inserts of STRs
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param n: int - target allele number
    :param rate_func: function - rate function for deletes
    :return: ndarray - combined distribution
    """
    p1, p2, p3, q = model_params
    deletes = binom.pmf(np.arange(rng), n, clip(1 - rate_func(n, p1, p2, p3), 0.0, 1.0))
    inserts = binom.pmf(np.arange(rng), n, q)
    return combine_distribs(deletes, inserts)


def model_template(rng, model_params, rate_func=linear_rate):
    """
    Partial function for model creation.
    :param rng: int - max_range of distribution
    :param model_params: 4-tuple - parameters for inserts and deletes
    :param rate_func: function - rate function for deletes
    :return: partial function with only 1 parameter - n - target allele number
    """
    return functools.partial(model_full, rng, model_params, rate_func=rate_func)


def generate_models(
    min_rep: int, max_rep: int, multiple_backgrounds: bool = True
) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :param multiple_backgrounds: bool - whether to generate all background states
    :return: generator of allele pairs (numbers or 'E' or 'B')
    """
    for model_index1 in range(min_rep, max_rep):
        for model_index2 in range(model_index1, max_rep):
            yield model_index1, model_index2
        yield model_index1, 'E'
        if multiple_backgrounds:
            yield 'B', model_index1

    yield 'B', 'B'
    yield 'E', 'E'


def generate_models_one_allele(min_rep: int, max_rep: int) -> Iterator[tuple[int | str, int | str]]:
    """
    Generate all pairs of alleles (models for generation of reads).
    :param min_rep: int - minimal number of repetitions
    :param max_rep: int - maximal number of repetitions
    :return: generator of allele pairs (numbers or 'E' or 'B'), 'X' for non-existing allele
    """
    for model_index1 in range(min_rep, max_rep):
        yield model_index1, 'X'

    yield 'B', 'X'
    yield 'E', 'X'


class Inference:
    """ Class for inference of alleles. """
    MIN_REPETITIONS = 1
    # default parameters for inference (miSeq default)
    DEFAULT_MODEL_PARAMS = (0.00716322, 0.000105087, 0.0210812, 0.0001648)
    DEFAULT_FIT_FUNCTION = 'linear'

    # TODO: type the members
    def __init__(
        self, read_distribution, params_file,
        str_rep=MIN_REP_CNT, minl_primer1=MIN_FLANK_LEN, minl_primer2=MIN_FLANK_LEN, minl_str=MIN_REP_LEN,
        p_bckg_closed=None, p_bckg_open=None, p_expanded=None
    ):
        """
        Initialization of the Inference class + setup of all models and their probabilities.
        :param read_distribution: ndarray(int) - read distribution
        :param params_file: str - filename of parameters, None for defaults
        :param str_rep: int - length of the STR
        :param minl_primer1: int - minimal length of the left primer
        :param minl_primer2: int - minimal length of the right primer
        :param minl_str: int - minimal length of the STR
        :param p_bckg_closed: float - probability of the background model for closed observation
        :param p_bckg_open: float - probability of the background model for open observation
        :param p_expanded: float - probability of the expanded model (if None it is equal to other models)
        """
        # assign variables
        self.str_rep = str_rep
        self.minl_primer1 = minl_primer1
        self.minl_primer2 = minl_primer2
        self.minl_str = minl_str
        self.read_distribution = read_distribution
        self.sum_reads = np.sum(read_distribution)
        self.params_file = params_file
        self.p_expanded = p_expanded
        self.p_bckg_closed = p_bckg_closed
        self.p_bckg_open = p_bckg_open

    def construct_models(self, min_rep, max_rep, e_model):
        """
        Construct all models needed for current inference.
        :param min_rep: int - minimal allele to model
        :param max_rep: int - maximal allele to model
        :param e_model: int - model for expanded alleles
        :return: None
        """
        # extract params
        model_params = self.DEFAULT_MODEL_PARAMS
        rate_func_str = self.DEFAULT_FIT_FUNCTION
        str_to_func = {'linear': linear_rate, 'const': const_rate, 'exponential': exp_rate, 'square': quadratic_rate}
        rate_func = const_rate
        if rate_func_str in str_to_func.keys():
            rate_func = str_to_func[rate_func_str]

        # save min_rep and max_rep
        self.min_rep = min_rep
        self.max_rep = max_rep  # non-inclusive
        self.max_with_e = e_model + 1  # non-inclusive

        # get models
        mt = model_template(self.max_with_e, model_params, rate_func)
        self.background_model = np.concatenate([
            np.zeros(self.min_rep, dtype=float),
            np.ones(self.max_with_e - self.min_rep, dtype=float) / float(self.max_with_e - self.min_rep)
        ])
        self.expanded_model = mt(self.max_with_e - 1)
        self.allele_models = {i: mt(i) for i in range(min_rep, max_rep)}
        self.models = {'E': self.expanded_model, 'B': self.background_model}
        self.models.update(self.allele_models)

        # get model likelihoods
        open_to_closed = 10.0

        l_others = 1.0
        l_bckg_open = 0.01
        l_exp = 1.01

        l_bckg_model_open = 1.0

        if self.p_expanded is None:
            self.p_expanded = l_exp
        if self.p_bckg_open is None and self.p_bckg_closed is None:
            self.p_bckg_open = l_bckg_open
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_closed is None:
            self.p_bckg_closed = self.p_bckg_open / open_to_closed
        if self.p_bckg_open is None:
            self.p_bckg_open = self.p_bckg_closed * open_to_closed

        self.model_probabilities = {'E': self.p_expanded, 'B': l_bckg_model_open}
        self.model_probabilities.update({i: l_others for i in self.allele_models.keys()})

    def likelihood_rl(self, rl):
        """
        Likelihood of a read with this length.
        :param rl: int - read length
        :return: float - likelihood of a read this long
        """
        return self.read_distribution[rl] / float(self.sum_reads)

    @staticmethod
    def likelihood_model(model, g):
        """
        Likelihood of a generated allele al from a model of
        :param model: ndarray - model that we evaluate
        :param g: int - observed read count
        :return: float - likelihood of a read coming from this model
        """
        return model[g]

    def likelihood_coverage(self, true_length, rl, closed=True):
        """
        Likelihood of generating a read with this length and this allele.
        :param true_length: int - true number of repetitions of an STR
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of a read being generated with this attributes
        """
        whole_inside_str = max(0, true_length * self.str_rep + self.minl_primer1 + self.minl_primer2 - rl + 1)
        # closed_overlapping = max(0, rl - self.minl_primer1 - self.minl_primer2 - true_length * self.str_rep + 1)
        open_overlapping = max(0, rl + true_length * self.str_rep - 2 * self.minl_str + 1)

        assert open_overlapping > whole_inside_str, '%d open %d whole inside %d %d %d' % (
            open_overlapping, whole_inside_str, true_length, rl, self.minl_str)

        return 1.0 / float(open_overlapping - whole_inside_str)

    def likelihood_read_allele(self, model, observed, rl, closed=True):
        """
        Likelihood of generation of read with observed allele count and rl.
        :param model: ndarray - model for the allele
        :param observed: int - observed allele count
        :param rl: int - read length
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return:
        """
        if closed:
            return (self.likelihood_rl(rl)
                    * self.likelihood_model(model, observed)
                    * self.likelihood_coverage(observed, rl, True))
        number_of_options = 0
        partial_likelihood = 0
        for true_length in itertools.chain(range(observed, self.max_rep), [self.max_with_e - 1]):
            partial_likelihood += (self.likelihood_model(model, true_length)
                                   * self.likelihood_coverage(true_length, rl, False))
            number_of_options += 1

        return self.likelihood_rl(rl) * partial_likelihood / float(number_of_options)

    @functools.lru_cache()
    def likelihood_read(
        self, observed: int, rl: int, model_index1: int, model_index2: int | None = None, closed: bool = True
    ) -> float:
        """
        Compute likelihood of generation of a read from either of those models.
        :param observed: int - observed allele count
        :param rl: int - read length
        :param model_index1: char/int - model index for left allele
        :param model_index2: char/int - model index for right allele or None if mono-allelic
        :param closed: bool - if the read is closed - i.e. both primers are there
        :return: float - likelihood of this read generation
        """
        # TODO: tuto podla mna nemoze byt len tak +, chyba tam korelacia modelov, ale v ramci zjednodusenia asi ok
        allele1_likelihood = (
            self.model_probabilities[model_index1] * self.likelihood_read_allele(self.models[model_index1], observed, rl, closed)
        )
        allele2_likelihood = 0.0 if model_index2 is None else (
            self.model_probabilities[model_index2] * self.likelihood_read_allele(self.models[model_index2], observed, rl, closed)
        )

        p_bckg = self.p_bckg_closed if closed else self.p_bckg_open
        bckgrnd_likelihood = p_bckg * self.likelihood_read_allele(self.models['B'], observed, rl, closed)

        assert not np.isnan(allele2_likelihood)
        assert not np.isnan(allele1_likelihood)
        assert not np.isnan(bckgrnd_likelihood)

        # "tuto" refers to the next line
        return allele1_likelihood + allele2_likelihood + bckgrnd_likelihood

    def infer(
        self, annotations: list[Annotation], filt_annotations: list[Annotation],
        index_rep: int, monoallelic: bool = False
    ) -> dict[tuple[int | str, int | str], float]:
        """
        Does all the inference,
        computes for which 2 combination of alleles are these annotations and parameters the best.
        argmax_{G1, G2} P(G1, G2 | AL, COV, RL)
            ~ P(AL, COV, RL | G1, G2) * P(G1, G2)
            = prod_{read_i} P(al_i, cov_i, rl_i | G1, G2) * P(G1, G2)
            = independent G1 G2
            = prod_{read_i} P(al_i, cov_i, rl_i | G1) * P(al_i, cov_i, rl_i | G2) * P(G1) * P(G2)
            {here G1, G2 is from possible alleles, background, and expanded, priors are from params}

         P(al_i, cov_i, rl_i | G1) - 2 options:
             1. closed evidence (al_i = X), we know X;
             2. open evidence (al_i >= X), cl_i == True if i is closed

         1.: P(al_i, cov_i, rl_i, cl_i | G1)
            = P(rl_i from read distrib.) * p(allele is al_i | G1) * P(read generated closed evidence | rl_i, al_i)
         2.: P(rl_i is from r.distr.) * P(allele is >= al_i | G1) * P(read generated open evidence | rl_i, al_i)

        :param annotations: list(Annotation) - closed annotated reads (both primers set)
        :param filt_annotations: list(Annotation) - open annotated reads (only one primer set)
        :param index_rep: int - index of a repetition
        :param verbose: bool - print more stuff?
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: dict(tuple(int, int):float) - directory of model indices to their likelihood
        """
        # generate closed observed and read_length arrays
        observed_annots = np.array([ann.module_repetitions[index_rep] for ann in annotations])
        rl_annots = np.array([len(ann.read_seq) for ann in annotations])
        closed_annots = np.ones_like(observed_annots, dtype=bool)

        # generate open observed and read_length arrays
        observed_fa = np.array([ann.module_repetitions[index_rep] for ann in filt_annotations])
        rl_fa = np.array([len(ann.read_seq) for ann in filt_annotations])
        closed_fa = np.zeros_like(observed_fa, dtype=bool)

        # join them and keep the information if they are open or closed
        observed_arr = np.concatenate((observed_annots, observed_fa)).astype(int)
        rl_arr = np.concatenate((rl_annots, rl_fa)).astype(int)
        closed_arr = np.concatenate([closed_annots, closed_fa]).astype(bool)

        # generate the boundaries:
        overhead = 3
        if len(observed_annots) == 0:
            max_rep = max(observed_fa) + overhead  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, max(observed_fa) - overhead)  # inclusive
        else:
            max_rep = max(observed_annots) + overhead + 1  # non-inclusive
            min_rep = max(self.MIN_REPETITIONS, min(observed_annots) - overhead)  # inclusive

        # expanded allele
        e_allele = max_rep
        if len(observed_fa) > 0:
            e_allele = max(max_rep, max(observed_fa) + 1)

        # generate all the models
        self.construct_models(min_rep, max_rep, e_allele)

        # go through every model and evaluate:
        evaluated_models = {}
        if monoallelic:
            models = generate_models_one_allele(min_rep, max_rep)
        else:
            models = generate_models(min_rep, max_rep, multiple_backgrounds=True)
        for m1, m2 in models:
            evaluated_models[(m1, m2)] = 0.0
            # go through every read
            for obs, rl, closed in zip(observed_arr, rl_arr, closed_arr):
                lh = self.likelihood_read(obs, rl, m1, None if m2 == 'X' else m2, closed=closed)
                # TODO weighted sum according to the closeness/openness of reads?
                evaluated_models[(m1, m2)] += np.log(lh)

        return evaluated_models

    def predict(self, lh_dict: dict[tuple[int | str, int | str], float]) -> tuple[np.ndarray, tuple[int, int]]:
        # convert to a numpy array:
        lh_array = np.zeros((self.max_rep, self.max_rep + 1))
        for (k1, k2), v in lh_dict.items():
            if k2 == 'X':  # if we have mono-allelic
                k2 = k1
            # B is the smallest, E is the largest!
            if k2 == 'B' or k1 == 'E' or (isinstance(k1, int) and isinstance(k2, int) and k2 < k1):
                k1, k2 = k2, k1
            if k1 == 'B':
                k1 = 0
            if k2 == 'B':
                k2 = 0
            if k1 == 'E':  # only if k2 is 'E' too.
                k1 = 0
            if k2 == 'E':
                k2 = self.max_rep
            lh_array[k1, k2] = v

        # get minimal and maximal likelihood
        ind_good = (lh_array < 0.0) & (lh_array > -1e10) & (lh_array != np.nan)
        if len(lh_array[ind_good]) == 0:
            return lh_array, (0, 0)
        lh_array[~ind_good] = np.NINF
        best = sorted(np.unravel_index(np.argmax(lh_array), lh_array.shape))
        prediction = (int(best[0]), int(best[1]))

        # output best option
        return lh_array, prediction

    def convert_to_sym(self, best: tuple[int, int], monoallelic: bool) -> tuple[int | str, int | str]:
        """
        Convert numeric alleles to their symbolic representations.
        :param best: (int, int) - numeric representation of alleles
        :param monoallelic: bool - if this is monoallelic version
        :return: (int|str, int|str) - symbolic representation of alleles
        """
        # convert it to symbols
        if best[0] == 0 and best[1] == self.max_rep:
            best_sym = ('E', 'E')
        else:
            def fn1(x):
                return 'E' if x == self.max_rep else 'B' if x == 0 else x
            best_sym = tuple(map(fn1, best))

        # if mono-allelic return 'X' as second allele symbol
        if monoallelic:
            best_sym = (best_sym[0], 'X')

        return best_sym

    def get_confidence(
        self, lh_array: np.ndarray, predicted: tuple[int, int], monoallelic: bool = False
    ) -> Confidences:
        """
        Get confidence of a prediction.
        :param lh_array: 2D-ndarray - log likelihoods of the prediction
        :param predicted: tuple(int, int) - predicted alleles
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: tuple[float, float, float | str, float, float, float, float] - prediction confidence of
        all, first, and second allele(s), background and expanded states
        """
        # get confidence
        lh_corr_array = lh_array - np.max(lh_array)
        lh_sum = np.sum(np.exp(lh_corr_array))
        confidence: float = np.exp(lh_corr_array[predicted[0], predicted[1]]) / lh_sum
        confidence1: float
        confidence2: float
        if predicted[0] == predicted[1]:  # same alleles - we compute the probability per allele
            confidence1 = np.sum(np.exp(lh_corr_array[predicted[0], :])) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        elif predicted[1] == lh_corr_array.shape[0]:  # expanded allele - expanded is only on one side of the array
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = np.sum(np.exp(lh_corr_array[:, predicted[1]])) / lh_sum
        else:  # normal behavior - different alleles , no expanded, compute all likelihoods of the alleles
            confidence1 = (
                np.sum(np.exp(lh_corr_array[predicted[0], :]))
                + np.sum(np.exp(lh_corr_array[:, predicted[0]]))
                - np.exp(lh_corr_array[predicted[0], predicted[0]])
            ) / lh_sum
            confidence2 = (
                np.sum(np.exp(lh_corr_array[:, predicted[1]]))
                + np.sum(np.exp(lh_corr_array[predicted[1], :]))
                - np.exp(lh_corr_array[predicted[1], predicted[1]])
            ) / lh_sum

        confidence_back: float = np.exp(lh_corr_array[0, 0]) / lh_sum
        confidence_back_all: float = np.sum(np.exp(lh_corr_array[0, :])) / lh_sum
        confidence_exp: float = np.exp(lh_corr_array[0, self.max_rep]) / lh_sum
        confidence_exp_all: float = np.sum(np.exp(lh_corr_array[:, self.max_rep])) / lh_sum

        if monoallelic:
            # confidence2 = '---'  # TODO: fix this
            confidence2 = np.nan

        return (
            confidence, confidence1, confidence2,
            confidence_back, confidence_back_all,
            confidence_exp, confidence_exp_all
        )

    def genotype(
        self, spanning: list[Annotation], partial: list[Annotation], index_rep: int, monoallelic: bool = False
    ) -> tuple[np.ndarray | None, tuple[str | int, str | int], Confidences]:
        """
        Genotype based on all annotations - infer likelihoods
        :param spanning: list(Annotation) - good (blue) annotations
        :param partial: list(Annotation) - (grey) annotations with one primer
        :param index_rep: int - index of a repetition
        :param monoallelic: bool - do we have a mono-allelic motif (i.e. chrX/chrY and male sample?)
        :return: predicted symbols and confidences
        """
        # if we do not have any good annotations, then quit
        if len(spanning) == 0 and len(partial) == 0:
            return None, ('B', 'B'), (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)

        # infer likelihoods
        lh_dict = self.infer(spanning, partial, index_rep, monoallelic)
        lh_array, predicted = self.predict(lh_dict)

        # adjust for no spanning reads (should output Background)
        if len(spanning) == 0:
            predicted = (0, 0)

        # convert numbers to symbols
        predicted_sym = self.convert_to_sym(predicted, monoallelic)

        # get confidence of our prediction
        confidence = self.get_confidence(lh_array, predicted, monoallelic)

        # return predicted and confidence
        return lh_array, predicted_sym, confidence


def draw_pcolor(
    model: Inference, lh_array: np.ndarray, name: str, lognorm: bool = True
) -> ProbHeatmap:
    ind_good = (lh_array < 0.0) & (lh_array > -1e10) & (lh_array != np.nan)
    z_min, z_max = min(lh_array[ind_good]), max(lh_array[ind_good])
    max_str = len(lh_array)
    if lognorm:
        lh_view = -np.log(-lh_array)
        z_min = -np.log(-z_min)
        z_max = -np.log(-z_max)
    else:
        lh_view = lh_array.copy()

    # background (B, i) - copy it below min_rep
    lh_view[model.min_rep - 1, :] = lh_view[0, :]

    lh_copy = lh_view.copy()
    lh_copy[-1, model.min_rep] = lh_copy[0, 0]
    lh_copy[-1, model.min_rep + 1] = lh_copy[0, model.max_rep]

    title = '%s likelihood of options (%s)' % ('Loglog' if lognorm else 'Log', name)
    # print(lh_copy.shape, lognorm, title, max_str)
    heatmap_data = save_pcolor_plotly_file(model, lh_copy, lognorm, title, max_str)
    return heatmap_data


def save_pcolor_plotly_file(
    model: Inference, lh_copy: np.ndarray, lognorm: bool, title: str, max_str: int, start_ticks: int = 5, step_ticks: int = 5
) -> ProbHeatmap:
    text = [['' for _ in range(max_str - model.min_rep + 1)] for _ in range(max_str - model.min_rep + 1)]
    text[-1][0] = 'B'
    text[-1][1] = 'E'

    hovertext = []
    for j in ['B'] + list(range(model.min_rep, max_str)):
        inner = [f'{j}/{i}' for i in list(range(model.min_rep, max_str)) + ['E']]
        hovertext.append(inner)

    hovertext[0][-1] = 'E/E'
    hovertext[-1][0] = 'B'
    hovertext[-1][1] = 'E'

    z: list[list[float]] = lh_copy[model.min_rep - 1:, model.min_rep:].tolist()
    hovertext2: list[list[str]] = hovertext
    y_tickvals: list[int] = list(range(start_ticks - model.min_rep + 1, max_str - model.min_rep + 1, step_ticks)) + [0]
    y_ticktext: list[int | str] = list(range(start_ticks, max_str, step_ticks)) + ['B']
    x_tickvals: list[int] = list(range(start_ticks - model.min_rep, max_str - model.min_rep, step_ticks)) + [int(max_str - model.min_rep)]
    x_ticktext: list[int | str] = list(range(start_ticks, max_str, step_ticks)) + ['E(>%d)' % (model.max_with_e - 2)]
    x_pos: float = max_str - model.min_rep - 0.5

    return (z, hovertext2, y_tickvals, y_ticktext, x_tickvals, x_ticktext, x_pos)


def highlight_subpart(seq: str, highlight: int | list[int]) -> tuple[str, str]:
    """
    Highlights subpart of a motif sequence
    :param seq: str - motif sequence
    :param highlight: int/list(int) - part ot highlight
    :return: str, str - motif sequence with highlighted subpart, highlighted subpart
    """
    if highlight is None:
        return seq, ''

    str_part = []
    highlight1 = np.array(highlight)
    split = [f'{s}]' for s in seq.split(']') if s != '']
    for h in highlight1:
        str_part.append(split[h])
        split[h] = f'<b><u>{split[h]}</u></b>'
    return ''.join(split), ''.join(str_part)


def float_to_str(c: float | str, percents: bool = False, decimals: int = 1) -> str:
    """
    Convert float confidence to string.
    :param c: float/str - confidence
    :param percents: bool - whether to output as a percents or not
    :param decimals: int - how many decimals to round to
    :return: str - converted to string
    """
    if isinstance(c, float):
        return f'{c * 100: .{decimals}f}%' if percents else f'{c: .{decimals}f}'
    return c


def generate_row(sequence: str, result: dict, postfilter: PostFilter) -> tuple:
    """
    Generate rows of a summary table in html report.
    :param sequence: str - motif sequence
    :param result: pd.Series - result row to convert to table
    :param postfilter: PostFilter - postfilter dict from config
    :return: str - html string with rows of the summary table
    """
    highlight = list(map(int, str(result['repetition_index']).split('_')))
    sequence, _subpart = highlight_subpart(sequence, highlight)

    # shorten sequence:
    keep = 10
    first = sequence.find(',')
    last = sequence.rfind(',')
    smaller_seq = sequence if first == -1 else '...' + sequence[first - keep:last + keep + 1] + '...'

    # errors:
    errors = f'{postfilter.max_rel_error * 100:.0f}%'
    if postfilter.max_abs_error is not None:
        errors += f' (abs={postfilter.max_abs_error})'

    # fill templates:
    updated_result = {
        'conf_allele1': float_to_str(result['conf_allele1'], percents=True),
        'conf_allele2': float_to_str(result['conf_allele2'], percents=True),
        'confidence': float_to_str(result['confidence'], percents=True),
        'motif_nomenclature': smaller_seq,
        'indels': float_to_str(result['indels'], decimals=2),
        'mismatches': float_to_str(result['mismatches'], decimals=2),
        'indels_a1': float_to_str(result['indels_a1'], decimals=2),
        'mismatches_a1': float_to_str(result['mismatches_a1'], decimals=2),
        'indels_a2': float_to_str(result['indels_a2'], decimals=2),
        'mismatches_a2': float_to_str(result['mismatches_a2'], decimals=2)
    }
    # return ROW_STRING.format(**{**result, **updated_result})
    motif_name = result['motif_name']
    motif_nomenclature = updated_result['motif_nomenclature']
    allele1 = result['allele1']
    conf_allele1 = updated_result['conf_allele1']
    reads_a1 = result['reads_a1']
    indels_a1 = updated_result['indels_a1']
    mismatches_a1 = updated_result['mismatches_a1']

    allele2 = result['allele2']
    conf_allele2 = updated_result['conf_allele2']
    reads_a2 = result['reads_a2']
    indels_a2 = updated_result['indels_a2']
    mismatches_a2 = updated_result['mismatches_a2']

    confidence = updated_result['confidence']
    indels = updated_result['indels']
    mismatches = updated_result['mismatches']
    quality_reads = result['quality_reads']
    one_primer_reads = result['one_primer_reads']

    row_tuple = (
        motif_name, motif_nomenclature,
        allele1, conf_allele1, reads_a1, indels_a1, mismatches_a1,
        allele2, conf_allele2, reads_a2, indels_a2, mismatches_a2,
        confidence, indels, mismatches, quality_reads, one_primer_reads
    )

    return row_tuple
    # return (result, updated_result)


def generate_motifb64(seq: str, row: dict) -> tuple:
    highlight = list(map(int, str(row['repetition_index']).split('_')))
    sequence, _ = highlight_subpart(seq, highlight)
    motif_name = row['motif_name']
    motif_name_part1 = f'{motif_name.replace("/", "_")}'
    motif_name_part2 = f'{",".join(map(str, highlight)) if highlight is not None else "mot"}'
    motif_name_long = f'{motif_name_part1}_{motif_name_part2}'
    motif_clean = re.sub(r'[^\w_]', '', motif_name_long)
    motif_id = motif_clean.rsplit('_', 1)[0]
    motif_clean_id = motif_id if highlight == [1] else motif_clean  # trick to solve static html

    a1 = row['allele1']
    a2 = row['allele2']
    conf_total = float_to_str(row['confidence'], percents=True)
    conf_a1 = float_to_str(row['conf_allele1'], percents=True)
    conf_a2 = float_to_str(row['conf_allele2'], percents=True)
    if (a1 == 'B' and a2 == 'B') or (a1 == 0 and a2 == 0):
        result = f'BG {conf_total}'
    else:
        result = f'{str(a1):2s} ({conf_a1}) {str(a2):2s} ({conf_a2}) total {conf_total}'

    alignment = f"{motif_name}/alignments.html"

    return (motif_clean_id, motif_id, motif_name, result, alignment, sequence)


Confidences: TypeAlias = tuple[float, float, float, float, float, float, float]
GenotypeInfo: TypeAlias = tuple[
    int, list[Annotation], list[Annotation], list[Annotation], tuple[str | int, str | int],
    Confidences, np.ndarray[Any, Any] | None, Inference
]
Hist2DGraph: TypeAlias = tuple[list[list[int]], list[list[int]], list[list[str]], str, str]
HistReadCounts: TypeAlias = tuple[list[int], list[int], list[int]]
ProbHeatmap: TypeAlias = tuple[list[list[float]], list[list[str]], list[int], list[int | str], list[int], list[int | str], float]
GraphData: TypeAlias = tuple[HistReadCounts | None, ProbHeatmap | None, Hist2DGraph | None]


# def test1() -> None:
#     print("kjsk")
def load_arguments_fake() -> Namespace:
    return Namespace(
        output_dir='dante_out',
        input_tsv=open("../../cache/HG002.GRCh38.remastr_output.tsv", "r"),
        male=False,

        verbose=True,
        nomenclatures=5,
        cutoff_alignments=20,
    )


# %%
if __name__ == '__main__':
    main()
    # test1()
